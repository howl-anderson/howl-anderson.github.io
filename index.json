[{"content":"TL;DR OpenAI\u0026rsquo;s ChatGPT provides the range and meaning of various parameters in its official documentation (https://platform.openai.com/docs/api-reference/chat/create). We will discuss ChatGPT\u0026rsquo;s generation process and how these parameters implement its generation effects.\nChatGPT\u0026rsquo;s Decoding Process We assume minGPT (equivalent to GPT-2) and ChatGPT have the same decoding process: https://github.com/karpathy/minGPT/blob/master/mingpt/model.py#LL283C12-L283C12.\nThe overall process can be summarized as the following steps:\nExpand the user\u0026rsquo;s request from 1 to a batch size of num_samples Perform model inference to obtain logits Perform temperature mapping: logits = logits / temperature [Optional] Perform topk processing: logits = topk_func(logits, top_k) Map logits to probabilities: probs = softmax(logits) Whether to sample: Sample: idx_next = multinomial_sample(probs, num_samples=1) Don\u0026rsquo;t sample: idx_next = topk_func(probs, k=1) Repeat the above process max_new_tokens times Decoding Parameters of ChatGPT temperature The official definition of the temperature parameter is:\ntemperature number Optional Defaults to 1\nWhat sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.\nWe generally recommend altering this or top_p but not both.\nThis corresponds to step 3 of the decoding process.\nBelow we will demonstrate its effect with example data combined with the model decoding process (to simplify the logic, we will not perform topk processing):\nAssume the vocabulary size of a model is 2. At some point, the model output is logits = [0.8, 0.2]. If no temperature mapping is performed (equivalent to setting temperature to 1, which is also the default value): Probability conversion: probs = softmax(logits) = [0.65, 0.35] If temperature is set to 1.8, then logits = logits / temperature = [0.44, 0.11], next step is probability conversion: probs = softmax(logits) = [0.58, 0.42] If temperature is set to 0.2, then logits = logits / temperature = [4, 1], next step is probability conversion: probs = softmax(logits) = [0.95, 0.05] Summary: From the above data, it can be seen that the larger the temperature, the smaller the difference in probability between different tokens with different logits after mapping, and the more likely it is to be randomly selected by the subsequent sample section.\nIt is worth noting that the temperature range of the GPT model is from 0 (inclusive) to 2 (inclusive). However, when temperature=0, it is impossible to be used as a divisor in numerical calculations, so ChatGPT must have adopted some tricks or transformations to solve this problem.\nWe draw a graph to demonstrate the performance of different temperatures on different logits:\n# importing package import matplotlib.pyplot as plt import numpy as np import math # x axis index and values data = list(enumerate(zip(np.arange(0.1, 0.6, 0.1), np.arange(0.9, 0.4, -0.1)))) # colors for each temperature, from low to high temperature, from yellow to dark red # reference: https://colorbrewer2.org/#type=sequential\u0026amp;scheme=YlOrRd\u0026amp;n=5 colors = [\u0026#34;#ffffb2\u0026#34;, \u0026#34;#fecc5c\u0026#34;, \u0026#34;#fd8d3c\u0026#34;, \u0026#34;#f03b20\u0026#34;, \u0026#34;#bd0026\u0026#34;] for t_idx, temperature in enumerate(np.arange(0.4, 1.6 + 0.0001, 0.3)): # each line for each temperature # get x and y values x = [] y = [] for x_idx, (a, b) in data: logits = np.array([a, b]) probs = softmax(logits / temperature) x.append(x_idx) y.append(probs[1] / probs[0]) # max prob / min prob # plot circle_color = colors[t_idx] if math.isclose(temperature, 1.0): # plot the line for temperature 1.0 with black circles plt.scatter(x, y, label=f\u0026#34;{temperature:.1f}\u0026#34;, facecolors=\u0026#34;black\u0026#34;, edgecolors=\u0026#34;black\u0026#34;) else: # other lines with colorful lines plt.scatter(x, y, label=f\u0026#34;{temperature:.1f}\u0026#34;, facecolors=circle_color, edgecolors=\u0026#34;gray\u0026#34;) plt.legend() # set x and y axis plt.xlabel(\u0026#39;logits\u0026#39;) plt.xticks([x for x, _ in data], [f\u0026#34;[{a:.1f}, {b:.1f}]\u0026#34; for _, (a, b) in data]) plt.ylabel(\u0026#39;ratio of max/min prob\u0026#39;) plt.show() The following graph will be output:\nIn the above graph, the x-axis represents logits (of two categories), and the y-axis represents the ratio of the maximum probability to the minimum probability, which can be used to measure the size of the difference.\nWhen temperature is not introduced, the probability ratio and logits are strictly related, and the value of logits can be mapped to probability values through the softmax function. In the above graph, when temperature=0, it is equivalent to the case without introducing temperature, and is represented by a hollow circle ◯ in the graph.\nBy observation, it can be seen that no matter which logits we choose, we can see that the larger the temperature, the smaller the difference between probabilities (i.e., the ratio of max prob / min prob), which means that the probability difference is smaller. The opposite is also true. Therefore, it can be concluded that the larger the temperature, the more random the results generated by the model, and the smaller the temperature, the more deterministic the results generated by the model.\ntop_p The official definition of the top_p parameter is:\ntop_p number Optional Defaults to 1\nAn alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.\nWe generally recommend altering this or temperature but not both.\nThis corresponds to step 4 of the decoding process.\nUnlike minGPT, which uses absolute values ​​(top_n) for selection, OpenAI GPT uses percentages (top_p).\nThis part will clean up unqualified tokens (not in top_n or outside top_p ratio) by setting their logit values ​​to float(\u0026lsquo;Inf\u0026rsquo;).\nstop The official definition of the stop parameter is:\nstop string or array Optional Defaults to null\nUp to 4 sequences where the API will stop generating further tokens.\nThere is no corresponding step in MinGPT.\nThe meaning expressed by this part is also clear and unambiguous. It will stop generating when certain defined strings are detected in the output. This feature may have been applied in some software. For example {{gen 'rewrite' stop=\u0026quot;\\\\n-\u0026quot;}} in https://github.com/microsoft/guidance\nn The official definition of the n parameter is:\nn integer Optional Defaults to 1\nHow many chat completion choices to generate for each input message.\nThis corresponds to step 1 of the decoding process.\nSince each text in the batch of size n is sampled independently, different tokens may be selected at the same position, and these variations on the texts expand further as the position continues to extend, eventually generating different texts. Of course, there is also a certain probability of generating completely identical texts.\nmax_tokens The official definition of the max_tokens parameter is:\nmax_tokens integer Optional Defaults to inf\nThe maximum number of tokens to generate in the chat completion.\nThe total length of input tokens and generated tokens is limited by the model\u0026rsquo;s context length.\nThis corresponds to step 7 of the decoding process.\nThis part determines the maximum number of decoding runs. In minGPT, this number of decodings is fixed, and the model will definitely generate max_tokens tokens. But in OpenAI GPT it is not necessarily the case, due to several factors:\nThe setting of the stop parameter, see above for details. Possible special pause tokens. Through actual use of ChatGPT, it can be found that ChatGPT does not mechanically output the specified text length, but will stop by itself after fully answering the question. Experimental code is as follows: import openai openai.api_key = \u0026#34;sk-xxx\u0026#34; completion = openai.ChatCompletion.create( model=\u0026#34;gpt-4\u0026#34;, messages=[{\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Help me output the even numbers between 1 and 10, separated by spaces. Do not output anything other than numbers.\u0026#34;}], temperature=0, max_tokens=100, ) response = completion.choices[0].message[\u0026#34;content\u0026#34;] print(\u0026#34;length: \u0026#34;, len(response)) # will output: length: 10 print(response) # will output: 2 4 6 8 10 presence_penalty The official definition of the presence_penalty parameter is:\nfrequency_penalty number Optional Defaults to 0\nNumber between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model\u0026rsquo;s likelihood to repeat the same line verbatim.\nThere is no corresponding step in MinGPT.\nIt is explained in detail at https://platform.openai.com/docs/api-reference/parameter-details.\nSpecifically, at a certain decoding timestep, the logit value of token j is mu[j], c[j] indicates how many times j has appeared in the currently generated text. The value of c[j] \u0026gt; 0 can only be 1 (j has appeared at least once before) or 0 (has not appeared before). In OpenAI\u0026rsquo;s explanation, they use alpha_presence to refer to presence_penalty, the two are completely the same thing with different symbols. To be consistent with the documentation, the symbols in the documentation are used here. After introducing the presence_penalty mechanism, its value is revised to mu[j] - float(c[j] \u0026gt; 0) * alpha_presence. This means that when alpha_presence is positive, the logit of token j will be reduced because j has been generated in the previous text. The decrease in logit also means a decrease in the probability of being sampled. Therefore, by providing a positive presence_penalty, the probability of the model generating repeated tokens will be reduced, in other words, a penalty is imposed. Conversely, if alpha_presence is negative, it will encourage the model to generate repeated tokens.\nAlthough presence_penalty contains the word penalty, since its value range can be both positive and negative, it does not necessarily penalize the repeated appearance of tokens, but can also encourage repetition.\nfrequency_penalty The official definition of the frequency_penalty parameter is:\nfrequency_penalty number Optional Defaults to 0\nNumber between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model\u0026rsquo;s likelihood to repeat the same line verbatim.\nThere is no corresponding step in MinGPT.\nThis parameter is highly similar to presence_penalty. It is also explained in detail at https://platform.openai.com/docs/api-reference/parameter-details.\nSpecifically, the logit value of token j is mu[j]. After applying frequency_penalty, it will be revised to mu[j] -\u0026gt; mu[j] - c[j] * alpha_frequency. Where c[j] is how many times j has appeared in the currently generated text. And alpha_frequency is frequency_penalty. This means that when frequency_penalty is positive, the logit of token j will decrease because j has been generated in the previous text, and the more j has been generated before (i.e. the larger the c[j]), the more severe the penalty. Here we can see the difference between frequency_penalty and presence_penalty: frequency_penalty strengthens the penalty as the number of occurrences of the token increases, while presence_penalty only distinguishes whether it has occurred, which is fully reflected in their name difference: frequency and presence.\nSimilar to presence, frequency_penalty can take both positive and negative values, thereby implementing penalties or rewards for repeating tokens.\nlogit_bias The official definition of the logit_bias parameter is:\nlogit_bias map Optional Defaults to null\nModify the likelihood of specified tokens appearing in the completion.\nAccepts a json object that maps tokens (specified by their token ID in the tokenizer) to an associated bias value from -100 to 100. Mathematically, the bias is added to the logits generated by the model prior to sampling. The exact effect will vary per model, but values between -1 and 1 should decrease or increase likelihood of selection; values like -100 or 100 should result in a ban or exclusive selection of the relevant token.\nThere is no corresponding step in MinGPT.\nThis parameter is used to unconditionally modify the logits of one or more tokens, thereby increasing or decreasing their likelihood of occurrence. Specifically, for variable token j, its logit value is mu[j]. After using logit_bias, its value will be modified to: mu[j] -\u0026gt; mu[j] + logit_bias[j].\n","permalink":"https://blog.xiaoquankong.ai/posts/the-decoding-process-of-chatgpt-and-the-various-parameters-in-it/","summary":"TL;DR OpenAI\u0026rsquo;s ChatGPT provides the range and meaning of various parameters in its official documentation (https://platform.openai.com/docs/api-reference/chat/create). We will discuss ChatGPT\u0026rsquo;s generation process and how these parameters implement its generation effects.\nChatGPT\u0026rsquo;s Decoding Process We assume minGPT (equivalent to GPT-2) and ChatGPT have the same decoding process: https://github.com/karpathy/minGPT/blob/master/mingpt/model.py#LL283C12-L283C12.\nThe overall process can be summarized as the following steps:\nExpand the user\u0026rsquo;s request from 1 to a batch size of num_samples Perform model inference to obtain logits Perform temperature mapping: logits = logits / temperature [Optional] Perform topk processing: logits = topk_func(logits, top_k) Map logits to probabilities: probs = softmax(logits) Whether to sample: Sample: idx_next = multinomial_sample(probs, num_samples=1) Don\u0026rsquo;t sample: idx_next = topk_func(probs, k=1) Repeat the above process max_new_tokens times Decoding Parameters of ChatGPT temperature The official definition of the temperature parameter is:","title":"The decoding process of ChatGPT and the various parameters in it"},{"content":"TL;DR using relative path as metadata_path to projector will cause TensorBoard cannot find metadata. The correct way is use FQPN (fully-qualified path name, aka absolute path)\nHow it happened Follow code copied from TensorFlow official website https://www.tensorflow.org/versions/r1.3/programmers_guide/embedding, which is also the official approach to use TensorFlow\u0026rsquo;s embedding projector. This is also most people actually do in their code.\nfrom tensorflow.contrib.tensorboard.plugins import projector # Create randomly initialized embedding weights which will be trained. vocabulary_size = 10000 embedding_size = 200 embedding_var = tf.get_variable(\u0026#39;word_embedding\u0026#39;, [vocabulary_size, embedding_size]) # Format: tensorflow/tensorboard/plugins/projector/projector_config.proto config = projector.ProjectorConfig() # You can add multiple embeddings. Here we add only one. embedding = config.embeddings.add() embedding.tensor_name = embedding_var.name # Link this tensor to its metadata file (e.g. labels). embedding.metadata_path = os.path.join(LOG_DIR, \u0026#39;metadata.tsv\u0026#39;) # Use the same LOG_DIR where you stored your checkpoint. summary_writer = tf.summary.FileWriter(LOG_DIR) # The next line writes a projector_config.pbtxt in the LOG_DIR. TensorBoard will # read this file during startup. projector.visualize_embeddings(summary_writer, config) Above code binding tensor embedding_var to it\u0026rsquo;s metadata file location by metadata_path, then the config information will write to disk by summary_writer.\nThe problem is the path of metadata file assign to embedding.metadata_path is relative path. If current path is log directory (i.e: LOG_DIR in source code), program can find the metadata file. But people usually don\u0026rsquo;t start TensorBoard in log directory, most people like me usually use TensorBoard --logdir ./log to start TensorBoard, in such condition, TensorBoard read the config and get the location of metadata file, but TensorBoard can not find the file by this location info, this is because location is correct only if your current work path is log directory, which most likely not true. Then TensorBoard cannot find the metadata file, loading process will never be done.\nPS Someone also notice this issue and give same solution\n","permalink":"https://blog.xiaoquankong.ai/posts/solution-for-tensorboard-embedding-blocked-when-loading-metadata/","summary":"\u003cp\u003e\u003cstrong\u003eTL;DR\u003c/strong\u003e using relative path as \u003ccode\u003emetadata_path\u003c/code\u003e to \u003ccode\u003eprojector\u003c/code\u003e will cause TensorBoard cannot find metadata. The correct way is use FQPN (fully-qualified path name, aka absolute path)\u003c/p\u003e","title":"Solution for TensorBoard embedding blocked when loading metadata"},{"content":"TL;DR This article show how Whisper work and some Linux programming tricks it used.\nWhat\u0026rsquo;s the Whisper Many people familiar with the famous metric monitor system: graphite\nHere are some introduce from official website of graphite:\nGraphite is an enterprise-scale monitoring tool that runs well on cheap hardware.\nGraphite does two things:\nStore numeric time-series data Render graphs of this data on demand Whisper is one of the core parts of graphite. There are two components of \u0026ldquo;Store numeric time-series data\u0026rdquo;: One for receive the data from network which is the job of Carbon, another for write the data into physical disk (like a database) which is the job of Whisper.\nOfficially, Whisper is a time-serial database or library which design for graphite project, but still very university as a time-serial database.\nImportant:\nFollow code / structure analysis based on whisper==0.9.10, different version may has different result.\nOverview the structure of the Whisper database Summary Whisper is a single binary file database, each file have three parts: Header / Archives / Data. The reference implement (aka Whisper in Graphite) was wrote in Python with manipulate binary file with struct library.\nHeader Header of Whisper used to record meta-information such as aggregationType / maxRetention / xff / archiveCount\naggregationType Data type: long int (aka \u0026lsquo;L\u0026rsquo; in struct format). aggregationType control how the higher precision aggregate to lower precision.\naggregationTypeToMethod = dict({ 1: \u0026#39;average\u0026#39;, 2: \u0026#39;sum\u0026#39;, 3: \u0026#39;last\u0026#39;, 4: \u0026#39;max\u0026#39;, 5: \u0026#39;min\u0026#39;, 6: \u0026#39;avg_zero\u0026#39; }) maxRetention Data type: long int. maxRetention show the max retention this database can hold. The unit of this field is second.\nxff xff is the abbreviation for xFilesFactor. Data type: Float (aka \u0026rsquo;l\u0026rsquo; in struct format). When higher precision data aggregate to lower precision, if the proportion of valid data lower than this value, the result of aggregation will set to None.\narchiveCount Data type: long int. archiveCount are used to count the number of archive.\nArchives summary Archives are stored the meta-information of Data or Data is a part of Archives. Different archive means different precision and retention for data store.\nArchives field have several strict limitation:\nAt least has one archive Archive\u0026rsquo;s precision must strictly monotonically after ordered Precision of lower archive must be a multiple of higher\u0026rsquo;s (not equal) Archive\u0026rsquo;s retention must strictly monotonically too, the lower precision the longer retention Archive of higher precision must have enough point to consolidate archive of lower precision Here comes an example for the last limitation: Higher: 1s/20 Lower: 60s/1\nThis example fits first four limitation, but not the last one.\nThose limitations are checked by:\nif not archiveList: raise InvalidConfiguration(\u0026#34;You must specify at least one archive configuration!\u0026#34;) archiveList.sort(key=lambda a: a[0]) # Sort by precision (secondsPerPoint) for i, archive in enumerate(archiveList): if i == len(archiveList) - 1: break nextArchive = archiveList[i + 1] if not archive[0] \u0026lt; nextArchive[0]: raise InvalidConfiguration(\u0026#34;A Whisper database may not be configured having \u0026#34; \u0026#34;two archives with the same precision (archive%d: %s, archive%d: %s)\u0026#34; % (i, archive, i + 1, nextArchive)) if nextArchive[0] % archive[0] != 0: raise InvalidConfiguration(\u0026#34;Higher precision archives\u0026#39; precision \u0026#34; \u0026#34;must evenly divide all lower precision archives\u0026#39; precision \u0026#34; \u0026#34;(archive%d: %s, archive%d: %s)\u0026#34; % (i, archive[0], i + 1, nextArchive[0])) retention = archive[0] * archive[1] nextRetention = nextArchive[0] * nextArchive[1] if not nextRetention \u0026gt; retention: raise InvalidConfiguration(\u0026#34;Lower precision archives must cover \u0026#34; \u0026#34;larger time intervals than higher precision archives \u0026#34; \u0026#34;(archive%d: %s seconds, archive%d: %s seconds)\u0026#34; % (i, retention, i + 1, nextRetention)) archivePoints = archive[1] pointsPerConsolidation = nextArchive[0] // archive[0] if not archivePoints \u0026gt;= pointsPerConsolidation: raise InvalidConfiguration(\u0026#34;Each archive must have at least enough points \u0026#34; \u0026#34;to consolidate to the next archive (archive%d consolidates %d of \u0026#34; \u0026#34;archive%d\u0026#39;s points but it has only %d total points)\u0026#34; % (i + 1, pointsPerConsolidation, i, archivePoints)) structure Archives have three fields: offset / secondsPerPoint / points\noffset Data type: long int. offset indicate the offset from the very begin of file.\nsecondsPerPoint Data type: long int. secondsPerPoint is the precision of archive. Obviously the max precision is one-second per data point.\npoints Data type: long int. points indicate the capacity of this archive.\nderivative Few derivative attribution, not exists in the fields but can be compute form those fields\nretention retention = points * secondsPerPoint\nsize Size of the Data field in physical occupation.\nsize = points * pointSize\npointSize will covered in the Data part, it is a fix size in Whisper.\nData summary Data part are the actual data store part. Those parts are compose by Point which contain time and value information.\nPoint each point have two parts: Interval, data\nInterval Data type: Long int, UNIX timestamp\nData Data type: Double (aka \u0026rsquo;d\u0026rsquo; in struct format), store the metric value\nConclusion ASCII Art Chart +-------------------------------------------------------------------------+ |AT|MR|xff|AC|offset|SPP|points| ... |Interval|data| ... | +-------------------------------------------------------------------------+ | | Archive One | ... | Point One | ... | +-------------------------------------------------------------------------+ | Header | Archives | Data | +-------------------------------------------------------------------------+ | Whisper file | +-------------------------------------------------------------------------+ AT: aggregationType MR: maxRetention AC: archiveCount SPP: secondsPerPoint Create process pre-requires There are two acquire pre-requires:\npath : the path of the database on disk archiveList : List of all the archive, can not modified after create Two optional pre-requires:\nxFilesFactor = None aggregationMethod = None check archiveList # Validate archive configurations... validateArchiveList(archiveList) Trap on the implementation Function validateArchiveList seems only check the validation of archiveList but actually also change the archiveList by execute order operation archiveList.sort(key=lambda a: a[0]). This is not a good design, if people don\u0026rsquo;t look at the implementation of validateArchiveList, their will never know archiveList already sorted. And because fallow code depend on the order of archiveList, if you don\u0026rsquo;t know this, you will don\u0026rsquo;t understand how the code works.\nAlso see summary section of Archives part\nWrite Header aggregationType = struct.pack(longFormat, aggregationMethodToType.get(aggregationMethod, 1)) oldest = max([secondsPerPoint * points for secondsPerPoint, points in archiveList]) maxRetention = struct.pack(longFormat, oldest) xFilesFactor = struct.pack(floatFormat, float(xFilesFactor)) archiveCount = struct.pack(longFormat, len(archiveList)) packedMetadata = aggregationType + maxRetention + xFilesFactor + archiveCount fh.write(packedMetadata) Write ArchiveList The key point of this process is the offset computing.\nheaderSize = metadataSize + (archiveInfoSize * len(archiveList)) archiveOffsetPointer = headerSize for secondsPerPoint, points in archiveList: archiveInfo = struct.pack(archiveInfoFormat, archiveOffsetPointer, secondsPerPoint, points) fh.write(archiveInfo) archiveOffsetPointer += (points * pointSize) Write Data Because the database is just create, so there are no real data, the process will write some fake data: \\x00.\nif CAN_FALLOCATE and useFallocate: remaining = archiveOffsetPointer - headerSize fallocate(fh, headerSize, remaining) elif sparse: fh.seek(archiveOffsetPointer - 1) fh.write(b\u0026#39;\\x00\u0026#39;) else: remaining = archiveOffsetPointer - headerSize chunksize = 16384 zeroes = b\u0026#39;\\x00\u0026#39; * chunksize while remaining \u0026gt; chunksize: fh.write(zeroes) remaining -= chunksize fh.write(zeroes[:remaining]) IO optimization As you see, Whisper use several method to optimize the IO operation\nFallocate This is a Linux-specific system call. The function prototype is int fallocate(int fd, int mode, off_t offset, off_t len);. fallocate() allows the caller to directly manipulate the allocated disk space for the file referred to by fd for the byte range starting at offset and continuing for len bytes.\nFor more information, please see man fallocate\nSparse File File is not really allocate to the disk, only record some metadata to represent the length of file. Advantage of sparse file is very fast allocate speed at create time, disadvantage is when really write the data to the file for first, the disk space are allocate, this may lead to disk fragment. This will lead slower write and read speed.\nFor more information, please see Sparse file on Wikipedia\nWrite by chunk If your data is several times bigger than disk sector, this will be the most effective. Modern computer has 4096 bytes (4KB) size disk sector. 4 times of 4k is 16384, so write by this number of data is effective too.\nFor more information, please see Disk sector on Wikipedia\nQuery process preprocess time range According to the max retention, shrink the query time range:\nif now is None: now = int(time.time()) if untilTime is None: untilTime = now fromTime = int(fromTime) untilTime = int(untilTime) # Here we try and be flexible and return as much data as we can. # If the range of data is from too far in the past or fully in the future, we # return nothing if fromTime \u0026gt; untilTime: raise InvalidTimeInterval(\u0026#34;Invalid time interval: from time \u0026#39;%s\u0026#39; is after until time \u0026#39;%s\u0026#39;\u0026#34; % (fromTime, untilTime)) oldestTime = now - header[\u0026#39;maxRetention\u0026#39;] # Range is in the future if fromTime \u0026gt; now: return None # Range is beyond retention if untilTime \u0026lt; oldestTime: return None # Range requested is partially beyond retention, adjust if fromTime \u0026lt; oldestTime: fromTime = oldestTime # Range is partially in the future, adjust if untilTime \u0026gt; now: untilTime = now Find the archive Whisper will try to find the most precise archive:\ndiff = now - fromTime for archive in header[\u0026#39;archives\u0026#39;]: if archive[\u0026#39;retention\u0026#39;] \u0026gt;= diff: break Justify the data point Arrange the time to time interval:\nfromInterval = int(fromTime - (fromTime % archive[\u0026#39;secondsPerPoint\u0026#39;])) + archive[\u0026#39;secondsPerPoint\u0026#39;] untilInterval = int(untilTime - (untilTime % archive[\u0026#39;secondsPerPoint\u0026#39;])) + archive[\u0026#39;secondsPerPoint\u0026#39;] Summary, Whisper always find the next interval near the query time\nSpecial, if the start interval same with the end interval, always contain next interval:\nif fromInterval == untilInterval: # Zero-length time range: always include the next point untilInterval += archive[\u0026#39;secondsPerPoint\u0026#39;] Compute offset # Determine fromOffset timeDistance = fromInterval - baseInterval pointDistance = timeDistance // archive[\u0026#39;secondsPerPoint\u0026#39;] byteDistance = pointDistance * pointSize fromOffset = archive[\u0026#39;offset\u0026#39;] + (byteDistance % archive[\u0026#39;size\u0026#39;]) # Determine untilOffset timeDistance = untilInterval - baseInterval pointDistance = timeDistance // archive[\u0026#39;secondsPerPoint\u0026#39;] byteDistance = pointDistance * pointSize untilOffset = archive[\u0026#39;offset\u0026#39;] + (byteDistance % archive[\u0026#39;size\u0026#39;]) Trick on % operation in Python Whisper use % to archive the wrap effect:\nbyteDistance % archive[\u0026#39;size\u0026#39;] equal to\nif byteDistance \u0026gt;= 0: return byteDistance else: return archive[\u0026#39;size\u0026#39;] + byteDistance The trick or feature of % operation is:\nprint(-3 % 5) # output 2 print(3 % 5) # output 3 Read data # Now we unpack the series data we just read (anything faster than unpack?) byteOrder, pointTypes = pointFormat[0], pointFormat[1:] points = len(seriesString) // pointSize seriesFormat = byteOrder + (pointTypes * points) unpackedSeries = struct.unpack(seriesFormat, seriesString) # And finally we construct a list of values (optimize this!) valueList = [None] * points # Pre-allocate entire list for speed currentInterval = fromInterval step = archive[\u0026#39;secondsPerPoint\u0026#39;] for i in xrange(0, len(unpackedSeries), 2): pointTime = unpackedSeries[i] if pointTime == currentInterval: pointValue = unpackedSeries[i+1] valueList[i//2] = pointValue # In-place reassignment is faster than append() currentInterval += step timeInfo = (fromInterval, untilInterval, step) return (timeInfo, valueList) Some key design in the process is that when Whisper read the data it will check weather the interval read from disk equal to expected timestamp. If not equaled, this means although here has a data point but the this not valid, it\u0026rsquo;s outdate data, then Whisper will use None as data value. This is very important this checker make Whisper can discrete write the disk and the result is always correct.\nWrite / Update data process Although Whisper have two update interface: update() and update_many(), but the latter interface support write multiply data point at one call. In the Carbon application, it almost use update_many(), which have better IO performance. Fallow text will cover update_many() but don\u0026rsquo;t worry about the update(), their are almost the same thing.\npre-require path : Database file location points : a list of point ((timestamp, value) pair Order the points Order the points by timestamp in the descending order, after that the newer point will at the head of list.\npoints = [(int(t), float(v)) for (t, v) in points] points.sort(key=lambda p: p[0], reverse=True) # Order points by timestamp, newest first Group the data point by retention for point in points: age = now - point[0] while currentArchive[\u0026#39;retention\u0026#39;] \u0026lt; age: # We can\u0026#39;t fit any more points in this archive if currentPoints: # Commit all the points we\u0026#39;ve found that it can fit currentPoints.reverse() # Put points in chronological order __archive_update_many(fh, header, currentArchive, currentPoints) currentPoints = [] try: currentArchive = next(archives) except StopIteration: currentArchive = None break if not currentArchive: break # Drop remaining points that don\u0026#39;t fit in the database currentPoints.append(point) if currentArchive and currentPoints: # Don\u0026#39;t forget to commit after we\u0026#39;ve checked all the archives currentPoints.reverse() __archive_update_many(fh, header, currentArchive, currentPoints) It\u0026rsquo;s not easy to make clear what\u0026rsquo;s operation do: data will write from higher precision archive to lower precision one. If the data is belong to this archive, it will write by this archive, if out the retention, leave the data to next (lower precision) archive.\nThis is important that, before all the data pass to writer function __archive_update_many, their all do the order reverse by currentPoints.reverse()\nArrange the data point step = archive[\u0026#39;secondsPerPoint\u0026#39;] alignedPoints = [(timestamp - (timestamp % step), value) for (timestamp, value) in points] Group the data by timestamp gap # Create a packed string for each contiguous sequence of points packedStrings = [] previousInterval = None currentString = b\u0026#34;\u0026#34; lenAlignedPoints = len(alignedPoints) for i in xrange(0, lenAlignedPoints): # Take last point in run of points with duplicate intervals if i + 1 \u0026lt; lenAlignedPoints and alignedPoints[i][0] == alignedPoints[i + 1][0]: continue (interval, value) = alignedPoints[i] # if the point is the start point or the it\u0026#39;s continue point from previous if (not previousInterval) or (interval == previousInterval + step): currentString += struct.pack(pointFormat, interval, value) previousInterval = interval else: # if the timestamp continue is breaked numberOfPoints = len(currentString) // pointSize startInterval = previousInterval - (step * (numberOfPoints - 1)) packedStrings.append((startInterval, currentString)) currentString = struct.pack(pointFormat, interval, value) previousInterval = interval if currentString: numberOfPoints = len(currentString) // pointSize startInterval = previousInterval - (step * (numberOfPoints - 1)) packedStrings.append((startInterval, currentString)) Important:\nTake last point in run of points with duplicate intervals\nThis is a very important feature.\nWrite data # Read base point and determine where our writes will start fh.seek(archive[\u0026#39;offset\u0026#39;]) packedBasePoint = fh.read(pointSize) (baseInterval, baseValue) = struct.unpack(pointFormat, packedBasePoint) if baseInterval == 0: # This file\u0026#39;s first update baseInterval = packedStrings[0][0] # Use our first string as the base, so we start at the start # Write all of our packed strings in locations determined by the baseInterval for (interval, packedString) in packedStrings: timeDistance = interval - baseInterval pointDistance = timeDistance // step byteDistance = pointDistance * pointSize myOffset = archive[\u0026#39;offset\u0026#39;] + (byteDistance % archive[\u0026#39;size\u0026#39;]) fh.seek(myOffset) archiveEnd = archive[\u0026#39;offset\u0026#39;] + archive[\u0026#39;size\u0026#39;] bytesBeyond = (myOffset + len(packedString)) - archiveEnd if bytesBeyond \u0026gt; 0: fh.write(packedString[:-bytesBeyond]) assert fh.tell() == archiveEnd, \u0026#34;archiveEnd=%d fh.tell=%d bytesBeyond=%d len(packedString)=%d\u0026#34; % ( archiveEnd, fh.tell(), bytesBeyond, len(packedString)) fh.seek(archive[\u0026#39;offset\u0026#39;]) fh.write( packedString[-bytesBeyond:]) # Safe because it can\u0026#39;t exceed the archive (retention checking logic above) else: fh.write(packedString) Note here: Write function have warp feature.\nAggregator All the archive will try to aggregate to the next archive\n# Now we propagate the updates to lower-precision archives higher = archive lowerArchives = [arc for arc in header[\u0026#39;archives\u0026#39;] if arc[\u0026#39;secondsPerPoint\u0026#39;] \u0026gt; archive[\u0026#39;secondsPerPoint\u0026#39;]] for lower in lowerArchives: fit = lambda i: i - (i % lower[\u0026#39;secondsPerPoint\u0026#39;]) lowerIntervals = [fit(p[0]) for p in alignedPoints] uniqueLowerIntervals = set(lowerIntervals) propagateFurther = False for interval in uniqueLowerIntervals: if __propagate(fh, header, interval, higher, lower): propagateFurther = True if not propagateFurther: break higher = lower Aggregation method on single point def __propagate(fh, header, timestamp, higher, lower): aggregationMethod = header[\u0026#39;aggregationMethod\u0026#39;] xff = header[\u0026#39;xFilesFactor\u0026#39;] lowerIntervalStart = timestamp - (timestamp % lower[\u0026#39;secondsPerPoint\u0026#39;]) lowerIntervalEnd = lowerIntervalStart + lower[\u0026#39;secondsPerPoint\u0026#39;] fh.seek(higher[\u0026#39;offset\u0026#39;]) packedPoint = fh.read(pointSize) (higherBaseInterval, higherBaseValue) = struct.unpack(pointFormat, packedPoint) if higherBaseInterval == 0: higherFirstOffset = higher[\u0026#39;offset\u0026#39;] else: timeDistance = lowerIntervalStart - higherBaseInterval pointDistance = timeDistance // higher[\u0026#39;secondsPerPoint\u0026#39;] byteDistance = pointDistance * pointSize higherFirstOffset = higher[\u0026#39;offset\u0026#39;] + (byteDistance % higher[\u0026#39;size\u0026#39;]) higherPoints = lower[\u0026#39;secondsPerPoint\u0026#39;] // higher[\u0026#39;secondsPerPoint\u0026#39;] higherSize = higherPoints * pointSize relativeFirstOffset = higherFirstOffset - higher[\u0026#39;offset\u0026#39;] relativeLastOffset = (relativeFirstOffset + higherSize) % higher[\u0026#39;size\u0026#39;] higherLastOffset = relativeLastOffset + higher[\u0026#39;offset\u0026#39;] fh.seek(higherFirstOffset) if higherFirstOffset \u0026lt; higherLastOffset: # We don\u0026#39;t wrap the archive seriesString = fh.read(higherLastOffset - higherFirstOffset) else: # We do wrap the archive higherEnd = higher[\u0026#39;offset\u0026#39;] + higher[\u0026#39;size\u0026#39;] seriesString = fh.read(higherEnd - higherFirstOffset) fh.seek(higher[\u0026#39;offset\u0026#39;]) seriesString += fh.read(higherLastOffset - higher[\u0026#39;offset\u0026#39;]) # Now we unpack the series data we just read byteOrder, pointTypes = pointFormat[0], pointFormat[1:] points = len(seriesString) // pointSize seriesFormat = byteOrder + (pointTypes * points) unpackedSeries = struct.unpack(seriesFormat, seriesString) # And finally we construct a list of values neighborValues = [None] * points currentInterval = lowerIntervalStart step = higher[\u0026#39;secondsPerPoint\u0026#39;] for i in xrange(0, len(unpackedSeries), 2): pointTime = unpackedSeries[i] if pointTime == currentInterval: neighborValues[i // 2] = unpackedSeries[i + 1] currentInterval += step # Propagate aggregateValue to propagate from neighborValues if we have enough known points knownValues = [v for v in neighborValues if v is not None] if not knownValues: return False knownPercent = float(len(knownValues)) / float(len(neighborValues)) if knownPercent \u0026gt;= xff: # We have enough data to propagate a value! aggregateValue = aggregate(aggregationMethod, knownValues, neighborValues) myPackedPoint = struct.pack(pointFormat, lowerIntervalStart, aggregateValue) fh.seek(lower[\u0026#39;offset\u0026#39;]) packedPoint = fh.read(pointSize) (lowerBaseInterval, lowerBaseValue) = struct.unpack(pointFormat, packedPoint) if lowerBaseInterval == 0: # First propagated update to this lower archive fh.seek(lower[\u0026#39;offset\u0026#39;]) fh.write(myPackedPoint) else: # Not our first propagated update to this lower archive timeDistance = lowerIntervalStart - lowerBaseInterval pointDistance = timeDistance // lower[\u0026#39;secondsPerPoint\u0026#39;] byteDistance = pointDistance * pointSize lowerOffset = lower[\u0026#39;offset\u0026#39;] + (byteDistance % lower[\u0026#39;size\u0026#39;]) fh.seek(lowerOffset) fh.write(myPackedPoint) return True else: return False If the xff (too much invalid point below the threhold) lead the aggregation failed, the next level aggregation will be cancled.\nMISC IO operation Whisper\u0026rsquo;s author do some effect on the IO optimise. When open the file, Whisper use fadvise to tell OS optimise the IO operation.\nif CAN_FADVISE and FADVISE_RANDOM: posix_fadvise(fh.fileno(), 0, 0, POSIX_FADV_RANDOM) Let\u0026rsquo;s see how manual say about fadvise:\nAllows an application to to tell the kernel how it expects to use a file handle, so that the kernel can choose appropriate read-ahead and caching techniques for access to the corresponding file.\nfadvise have several options:\nFADV_NORMAL ：No special treatment FADV_RANDOM : Expect page references in random order FADV_SEQUENTIAL : Expect page references in sequential order FADV_WILLNEED : Expect access in the near future FADV_DONTNEED : Do not expect access in the near future. Subsequent access of pages in this range will succeed, but will result either in reloading of the memory contents from the underlying mapped file or zero-fill-in-demand pages for mappings without an underlying file FADV_NOREUSE : Access data only once For more information, please see man fadvise\n","permalink":"https://blog.xiaoquankong.ai/posts/introduce-to-the-implement-of-whisper/","summary":"\u003cp\u003e\u003cstrong\u003eTL;DR\u003c/strong\u003e This article show how Whisper work and some Linux programming tricks it used.\u003c/p\u003e","title":"Introduce to the implement of Whisper: the time-serial database"}]