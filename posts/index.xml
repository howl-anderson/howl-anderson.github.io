<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Posts on Xiaoquan Kong&#39;s Blog</title>
    <link>https://blog.xiaoquankong.ai/posts/</link>
    <description>Recent content in Posts on Xiaoquan Kong&#39;s Blog</description>
    <image>
      <title>Xiaoquan Kong&#39;s Blog</title>
      <url>https://blog.xiaoquankong.ai/papermod-cover.png</url>
      <link>https://blog.xiaoquankong.ai/papermod-cover.png</link>
    </image>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Mon, 07 Aug 2023 22:10:08 +0800</lastBuildDate><atom:link href="https://blog.xiaoquankong.ai/posts/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>The decoding process of ChatGPT and the various parameters in it</title>
      <link>https://blog.xiaoquankong.ai/posts/the-decoding-process-of-chatgpt-and-the-various-parameters-in-it/</link>
      <pubDate>Mon, 07 Aug 2023 22:10:08 +0800</pubDate>
      
      <guid>https://blog.xiaoquankong.ai/posts/the-decoding-process-of-chatgpt-and-the-various-parameters-in-it/</guid>
      <description>TL;DR OpenAI&amp;rsquo;s ChatGPT provides the range and meaning of various parameters in its official documentation (https://platform.openai.com/docs/api-reference/chat/create). We will discuss ChatGPT&amp;rsquo;s generation process and how these parameters implement its generation effects.
ChatGPT&amp;rsquo;s Decoding Process We assume minGPT (equivalent to GPT-2) and ChatGPT have the same decoding process: https://github.com/karpathy/minGPT/blob/master/mingpt/model.py#LL283C12-L283C12.
The overall process can be summarized as the following steps:
Expand the user&amp;rsquo;s request from 1 to a batch size of num_samples Perform model inference to obtain logits Perform temperature mapping: logits = logits / temperature [Optional] Perform topk processing: logits = topk_func(logits, top_k) Map logits to probabilities: probs = softmax(logits) Whether to sample: Sample: idx_next = multinomial_sample(probs, num_samples=1) Don&amp;rsquo;t sample: idx_next = topk_func(probs, k=1) Repeat the above process max_new_tokens times Decoding Parameters of ChatGPT temperature The official definition of the temperature parameter is:</description>
    </item>
    
    <item>
      <title>Solution for TensorBoard embedding blocked when loading metadata</title>
      <link>https://blog.xiaoquankong.ai/posts/solution-for-tensorboard-embedding-blocked-when-loading-metadata/</link>
      <pubDate>Thu, 17 Aug 2017 14:50:24 +0000</pubDate>
      
      <guid>https://blog.xiaoquankong.ai/posts/solution-for-tensorboard-embedding-blocked-when-loading-metadata/</guid>
      <description>&lt;p&gt;&lt;strong&gt;TL;DR&lt;/strong&gt; using relative path as &lt;code&gt;metadata_path&lt;/code&gt; to &lt;code&gt;projector&lt;/code&gt; will cause TensorBoard cannot find metadata. The correct way is use FQPN (fully-qualified path name, aka absolute path)&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Introduce to the implement of Whisper: the time-serial database</title>
      <link>https://blog.xiaoquankong.ai/posts/introduce-to-the-implement-of-whisper/</link>
      <pubDate>Wed, 05 Oct 2016 18:49:49 +0000</pubDate>
      
      <guid>https://blog.xiaoquankong.ai/posts/introduce-to-the-implement-of-whisper/</guid>
      <description>&lt;p&gt;&lt;strong&gt;TL;DR&lt;/strong&gt; This article show how Whisper work and some Linux programming tricks it used.&lt;/p&gt;</description>
    </item>
    
  </channel>
</rss>
