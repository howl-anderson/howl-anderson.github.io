<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>The decoding process of ChatGPT and the various parameters in it | Xiaoquan Kong&#39;s Blog</title>
<meta name="keywords" content="">
<meta name="description" content="TL;DR OpenAI&rsquo;s ChatGPT provides the range and meaning of various parameters in its official documentation (https://platform.openai.com/docs/api-reference/chat/create). We will discuss ChatGPT&rsquo;s generation process and how these parameters implement its generation effects.
ChatGPT&rsquo;s Decoding Process We assume minGPT (equivalent to GPT-2) and ChatGPT have the same decoding process: https://github.com/karpathy/minGPT/blob/master/mingpt/model.py#LL283C12-L283C12.
The overall process can be summarized as the following steps:
Expand the user&rsquo;s request from 1 to a batch size of num_samples Perform model inference to obtain logits Perform temperature mapping: logits = logits / temperature [Optional] Perform topk processing: logits = topk_func(logits, top_k) Map logits to probabilities: probs = softmax(logits) Whether to sample: Sample: idx_next = multinomial_sample(probs, num_samples=1) Don&rsquo;t sample: idx_next = topk_func(probs, k=1) Repeat the above process max_new_tokens times Decoding Parameters of ChatGPT temperature The official definition of the temperature parameter is:">
<meta name="author" content="Xiaoquan Kong">
<link rel="canonical" href="https://blog.xiaoquankong.ai/posts/the-decoding-process-of-chatgpt-and-the-various-parameters-in-it/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.ddf8a1131e5ee910ad62ceef67eaeda962dffdd057098a9b91e635fd160b369b.css" integrity="sha256-3fihEx5e6RCtYs7vZ&#43;rtqWLf/dBXCYqbkeY1/RYLNps=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://blog.xiaoquankong.ai/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://blog.xiaoquankong.ai/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://blog.xiaoquankong.ai/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://blog.xiaoquankong.ai/apple-touch-icon.png">
<link rel="mask-icon" href="https://blog.xiaoquankong.ai/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://blog.xiaoquankong.ai/posts/the-decoding-process-of-chatgpt-and-the-various-parameters-in-it/">
<link rel="alternate" hreflang="zh" href="https://blog.xiaoquankong.ai/zh/posts/the-decoding-process-of-chatgpt-and-the-various-parameters-in-it/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --hljs-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.css" integrity="sha384-vKruj+a13U8yHIkAyGgK1J3ArTLzrFGBbBc0tDp4ad/EyewESeXE/Iv67Aj8gKZ0" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.js" integrity="sha384-PwRUT/YqbnEjkZO0zZxNqcxACrXe+j766U2amXcgMg5457rve2Y7I6ZJSm2A0mS4" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/contrib/auto-render.min.js" integrity="sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05" crossorigin="anonymous"
    onload="renderMathInElement(document.body);"></script>

<script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement(document.body, {
            delimiters: [
                {left: "$$", right: "$$", display: true},
                {left: "$", right: "$", display: false}
            ]
        });
    });
</script>



<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
	(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
	m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
	})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
	ga('create', 'UA-105150423-2', 'auto');
	
	ga('send', 'pageview');
}
</script><meta property="og:title" content="The decoding process of ChatGPT and the various parameters in it" />
<meta property="og:description" content="TL;DR OpenAI&rsquo;s ChatGPT provides the range and meaning of various parameters in its official documentation (https://platform.openai.com/docs/api-reference/chat/create). We will discuss ChatGPT&rsquo;s generation process and how these parameters implement its generation effects.
ChatGPT&rsquo;s Decoding Process We assume minGPT (equivalent to GPT-2) and ChatGPT have the same decoding process: https://github.com/karpathy/minGPT/blob/master/mingpt/model.py#LL283C12-L283C12.
The overall process can be summarized as the following steps:
Expand the user&rsquo;s request from 1 to a batch size of num_samples Perform model inference to obtain logits Perform temperature mapping: logits = logits / temperature [Optional] Perform topk processing: logits = topk_func(logits, top_k) Map logits to probabilities: probs = softmax(logits) Whether to sample: Sample: idx_next = multinomial_sample(probs, num_samples=1) Don&rsquo;t sample: idx_next = topk_func(probs, k=1) Repeat the above process max_new_tokens times Decoding Parameters of ChatGPT temperature The official definition of the temperature parameter is:" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://blog.xiaoquankong.ai/posts/the-decoding-process-of-chatgpt-and-the-various-parameters-in-it/" /><meta property="og:image" content="https://blog.xiaoquankong.ai/papermod-cover.png"/><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2023-08-07T22:10:08+08:00" />
<meta property="article:modified_time" content="2023-08-07T22:10:08+08:00" />

<meta name="twitter:card" content="summary_large_image"/>
<meta name="twitter:image" content="https://blog.xiaoquankong.ai/papermod-cover.png"/>

<meta name="twitter:title" content="The decoding process of ChatGPT and the various parameters in it"/>
<meta name="twitter:description" content="TL;DR OpenAI&rsquo;s ChatGPT provides the range and meaning of various parameters in its official documentation (https://platform.openai.com/docs/api-reference/chat/create). We will discuss ChatGPT&rsquo;s generation process and how these parameters implement its generation effects.
ChatGPT&rsquo;s Decoding Process We assume minGPT (equivalent to GPT-2) and ChatGPT have the same decoding process: https://github.com/karpathy/minGPT/blob/master/mingpt/model.py#LL283C12-L283C12.
The overall process can be summarized as the following steps:
Expand the user&rsquo;s request from 1 to a batch size of num_samples Perform model inference to obtain logits Perform temperature mapping: logits = logits / temperature [Optional] Perform topk processing: logits = topk_func(logits, top_k) Map logits to probabilities: probs = softmax(logits) Whether to sample: Sample: idx_next = multinomial_sample(probs, num_samples=1) Don&rsquo;t sample: idx_next = topk_func(probs, k=1) Repeat the above process max_new_tokens times Decoding Parameters of ChatGPT temperature The official definition of the temperature parameter is:"/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://blog.xiaoquankong.ai/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "The decoding process of ChatGPT and the various parameters in it",
      "item": "https://blog.xiaoquankong.ai/posts/the-decoding-process-of-chatgpt-and-the-various-parameters-in-it/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "The decoding process of ChatGPT and the various parameters in it",
  "name": "The decoding process of ChatGPT and the various parameters in it",
  "description": "TL;DR OpenAI\u0026rsquo;s ChatGPT provides the range and meaning of various parameters in its official documentation (https://platform.openai.com/docs/api-reference/chat/create). We will discuss ChatGPT\u0026rsquo;s generation process and how these parameters implement its generation effects.\nChatGPT\u0026rsquo;s Decoding Process We assume minGPT (equivalent to GPT-2) and ChatGPT have the same decoding process: https://github.com/karpathy/minGPT/blob/master/mingpt/model.py#LL283C12-L283C12.\nThe overall process can be summarized as the following steps:\nExpand the user\u0026rsquo;s request from 1 to a batch size of num_samples Perform model inference to obtain logits Perform temperature mapping: logits = logits / temperature [Optional] Perform topk processing: logits = topk_func(logits, top_k) Map logits to probabilities: probs = softmax(logits) Whether to sample: Sample: idx_next = multinomial_sample(probs, num_samples=1) Don\u0026rsquo;t sample: idx_next = topk_func(probs, k=1) Repeat the above process max_new_tokens times Decoding Parameters of ChatGPT temperature The official definition of the temperature parameter is:",
  "keywords": [
    
  ],
  "articleBody": "TL;DR OpenAI’s ChatGPT provides the range and meaning of various parameters in its official documentation (https://platform.openai.com/docs/api-reference/chat/create). We will discuss ChatGPT’s generation process and how these parameters implement its generation effects.\nChatGPT’s Decoding Process We assume minGPT (equivalent to GPT-2) and ChatGPT have the same decoding process: https://github.com/karpathy/minGPT/blob/master/mingpt/model.py#LL283C12-L283C12.\nThe overall process can be summarized as the following steps:\nExpand the user’s request from 1 to a batch size of num_samples Perform model inference to obtain logits Perform temperature mapping: logits = logits / temperature [Optional] Perform topk processing: logits = topk_func(logits, top_k) Map logits to probabilities: probs = softmax(logits) Whether to sample: Sample: idx_next = multinomial_sample(probs, num_samples=1) Don’t sample: idx_next = topk_func(probs, k=1) Repeat the above process max_new_tokens times Decoding Parameters of ChatGPT temperature The official definition of the temperature parameter is:\ntemperature number Optional Defaults to 1\nWhat sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.\nWe generally recommend altering this or top_p but not both.\nThis corresponds to step 3 of the decoding process.\nBelow we will demonstrate its effect with example data combined with the model decoding process (to simplify the logic, we will not perform topk processing):\nAssume the vocabulary size of a model is 2. At some point, the model output is logits = [0.8, 0.2]. If no temperature mapping is performed (equivalent to setting temperature to 1, which is also the default value): Probability conversion: probs = softmax(logits) = [0.65, 0.35] If temperature is set to 1.8, then logits = logits / temperature = [0.44, 0.11], next step is probability conversion: probs = softmax(logits) = [0.58, 0.42] If temperature is set to 0.2, then logits = logits / temperature = [4, 1], next step is probability conversion: probs = softmax(logits) = [0.95, 0.05] Summary: From the above data, it can be seen that the larger the temperature, the smaller the difference in probability between different tokens with different logits after mapping, and the more likely it is to be randomly selected by the subsequent sample section.\nIt is worth noting that the temperature range of the GPT model is from 0 (inclusive) to 2 (inclusive). However, when temperature=0, it is impossible to be used as a divisor in numerical calculations, so ChatGPT must have adopted some tricks or transformations to solve this problem.\nWe draw a graph to demonstrate the performance of different temperatures on different logits:\n# importing package import matplotlib.pyplot as plt import numpy as np import math # x axis index and values data = list(enumerate(zip(np.arange(0.1, 0.6, 0.1), np.arange(0.9, 0.4, -0.1)))) # colors for each temperature, from low to high temperature, from yellow to dark red # reference: https://colorbrewer2.org/#type=sequential\u0026scheme=YlOrRd\u0026n=5 colors = [\"#ffffb2\", \"#fecc5c\", \"#fd8d3c\", \"#f03b20\", \"#bd0026\"] for t_idx, temperature in enumerate(np.arange(0.4, 1.6 + 0.0001, 0.3)): # each line for each temperature # get x and y values x = [] y = [] for x_idx, (a, b) in data: logits = np.array([a, b]) probs = softmax(logits / temperature) x.append(x_idx) y.append(probs[1] / probs[0]) # max prob / min prob # plot circle_color = colors[t_idx] if math.isclose(temperature, 1.0): # plot the line for temperature 1.0 with black circles plt.scatter(x, y, label=f\"{temperature:.1f}\", facecolors=\"black\", edgecolors=\"black\") else: # other lines with colorful lines plt.scatter(x, y, label=f\"{temperature:.1f}\", facecolors=circle_color, edgecolors=\"gray\") plt.legend() # set x and y axis plt.xlabel('logits') plt.xticks([x for x, _ in data], [f\"[{a:.1f}, {b:.1f}]\" for _, (a, b) in data]) plt.ylabel('ratio of max/min prob') plt.show() The following graph will be output:\nIn the above graph, the x-axis represents logits (of two categories), and the y-axis represents the ratio of the maximum probability to the minimum probability, which can be used to measure the size of the difference.\nWhen temperature is not introduced, the probability ratio and logits are strictly related, and the value of logits can be mapped to probability values through the softmax function. In the above graph, when temperature=0, it is equivalent to the case without introducing temperature, and is represented by a hollow circle ◯ in the graph.\nBy observation, it can be seen that no matter which logits we choose, we can see that the larger the temperature, the smaller the difference between probabilities (i.e., the ratio of max prob / min prob), which means that the probability difference is smaller. The opposite is also true. Therefore, it can be concluded that the larger the temperature, the more random the results generated by the model, and the smaller the temperature, the more deterministic the results generated by the model.\ntop_p The official definition of the top_p parameter is:\ntop_p number Optional Defaults to 1\nAn alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.\nWe generally recommend altering this or temperature but not both.\nThis corresponds to step 4 of the decoding process.\nUnlike minGPT, which uses absolute values ​​(top_n) for selection, OpenAI GPT uses percentages (top_p).\nThis part will clean up unqualified tokens (not in top_n or outside top_p ratio) by setting their logit values ​​to float(‘Inf’).\nstop The official definition of the stop parameter is:\nstop string or array Optional Defaults to null\nUp to 4 sequences where the API will stop generating further tokens.\nThere is no corresponding step in MinGPT.\nThe meaning expressed by this part is also clear and unambiguous. It will stop generating when certain defined strings are detected in the output. This feature may have been applied in some software. For example {{gen 'rewrite' stop=\"\\\\n-\"}} in https://github.com/microsoft/guidance\nn The official definition of the n parameter is:\nn integer Optional Defaults to 1\nHow many chat completion choices to generate for each input message.\nThis corresponds to step 1 of the decoding process.\nSince each text in the batch of size n is sampled independently, different tokens may be selected at the same position, and these variations on the texts expand further as the position continues to extend, eventually generating different texts. Of course, there is also a certain probability of generating completely identical texts.\nmax_tokens The official definition of the max_tokens parameter is:\nmax_tokens integer Optional Defaults to inf\nThe maximum number of tokens to generate in the chat completion.\nThe total length of input tokens and generated tokens is limited by the model’s context length.\nThis corresponds to step 7 of the decoding process.\nThis part determines the maximum number of decoding runs. In minGPT, this number of decodings is fixed, and the model will definitely generate max_tokens tokens. But in OpenAI GPT it is not necessarily the case, due to several factors:\nThe setting of the stop parameter, see above for details. Possible special pause tokens. Through actual use of ChatGPT, it can be found that ChatGPT does not mechanically output the specified text length, but will stop by itself after fully answering the question. Experimental code is as follows: import openai openai.api_key = \"sk-xxx\" completion = openai.ChatCompletion.create( model=\"gpt-4\", messages=[{\"role\": \"user\", \"content\": \"Help me output the even numbers between 1 and 10, separated by spaces. Do not output anything other than numbers.\"}], temperature=0, max_tokens=100, ) response = completion.choices[0].message[\"content\"] print(\"length: \", len(response)) # will output: length: 10 print(response) # will output: 2 4 6 8 10 presence_penalty The official definition of the presence_penalty parameter is:\nfrequency_penalty number Optional Defaults to 0\nNumber between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model’s likelihood to repeat the same line verbatim.\nThere is no corresponding step in MinGPT.\nIt is explained in detail at https://platform.openai.com/docs/api-reference/parameter-details.\nSpecifically, at a certain decoding timestep, the logit value of token j is mu[j], c[j] indicates how many times j has appeared in the currently generated text. The value of c[j] \u003e 0 can only be 1 (j has appeared at least once before) or 0 (has not appeared before). In OpenAI’s explanation, they use alpha_presence to refer to presence_penalty, the two are completely the same thing with different symbols. To be consistent with the documentation, the symbols in the documentation are used here. After introducing the presence_penalty mechanism, its value is revised to mu[j] - float(c[j] \u003e 0) * alpha_presence. This means that when alpha_presence is positive, the logit of token j will be reduced because j has been generated in the previous text. The decrease in logit also means a decrease in the probability of being sampled. Therefore, by providing a positive presence_penalty, the probability of the model generating repeated tokens will be reduced, in other words, a penalty is imposed. Conversely, if alpha_presence is negative, it will encourage the model to generate repeated tokens.\nAlthough presence_penalty contains the word penalty, since its value range can be both positive and negative, it does not necessarily penalize the repeated appearance of tokens, but can also encourage repetition.\nfrequency_penalty The official definition of the frequency_penalty parameter is:\nfrequency_penalty number Optional Defaults to 0\nNumber between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model’s likelihood to repeat the same line verbatim.\nThere is no corresponding step in MinGPT.\nThis parameter is highly similar to presence_penalty. It is also explained in detail at https://platform.openai.com/docs/api-reference/parameter-details.\nSpecifically, the logit value of token j is mu[j]. After applying frequency_penalty, it will be revised to mu[j] -\u003e mu[j] - c[j] * alpha_frequency. Where c[j] is how many times j has appeared in the currently generated text. And alpha_frequency is frequency_penalty. This means that when frequency_penalty is positive, the logit of token j will decrease because j has been generated in the previous text, and the more j has been generated before (i.e. the larger the c[j]), the more severe the penalty. Here we can see the difference between frequency_penalty and presence_penalty: frequency_penalty strengthens the penalty as the number of occurrences of the token increases, while presence_penalty only distinguishes whether it has occurred, which is fully reflected in their name difference: frequency and presence.\nSimilar to presence, frequency_penalty can take both positive and negative values, thereby implementing penalties or rewards for repeating tokens.\nlogit_bias The official definition of the logit_bias parameter is:\nlogit_bias map Optional Defaults to null\nModify the likelihood of specified tokens appearing in the completion.\nAccepts a json object that maps tokens (specified by their token ID in the tokenizer) to an associated bias value from -100 to 100. Mathematically, the bias is added to the logits generated by the model prior to sampling. The exact effect will vary per model, but values between -1 and 1 should decrease or increase likelihood of selection; values like -100 or 100 should result in a ban or exclusive selection of the relevant token.\nThere is no corresponding step in MinGPT.\nThis parameter is used to unconditionally modify the logits of one or more tokens, thereby increasing or decreasing their likelihood of occurrence. Specifically, for variable token j, its logit value is mu[j]. After using logit_bias, its value will be modified to: mu[j] -\u003e mu[j] + logit_bias[j].\n",
  "wordCount" : "1851",
  "inLanguage": "en",
  "datePublished": "2023-08-07T22:10:08+08:00",
  "dateModified": "2023-08-07T22:10:08+08:00",
  "author":{
    "@type": "Person",
    "name": "Xiaoquan Kong"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://blog.xiaoquankong.ai/posts/the-decoding-process-of-chatgpt-and-the-various-parameters-in-it/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Xiaoquan Kong's Blog",
    "logo": {
      "@type": "ImageObject",
      "url": "https://blog.xiaoquankong.ai/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://blog.xiaoquankong.ai/" accesskey="h" title="Xiaoquan Kong&#39;s Blog (Alt + H)">Xiaoquan Kong&#39;s Blog</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
                <ul class="lang-switch"><li>|</li>
                    <li>
                        <a href="https://blog.xiaoquankong.ai/zh/" title="中文"
                            aria-label="中文">中文</a>
                    </li>
                </ul>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://blog.xiaoquankong.ai/archives" title="Archive">
                    <span>Archive</span>
                </a>
            </li>
            <li>
                <a href="https://blog.xiaoquankong.ai/categories" title="Categories">
                    <span>Categories</span>
                </a>
            </li>
            <li>
                <a href="https://blog.xiaoquankong.ai/tags/" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
            <li>
                <a href="https://www.xiaoquankong.ai" title="About Me">
                    <span>About Me</span>&nbsp;
                    <svg fill="none" shape-rendering="geometricPrecision" stroke="currentColor" stroke-linecap="round"
                        stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12">
                        <path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"></path>
                        <path d="M15 3h6v6"></path>
                        <path d="M10 14L21 3"></path>
                    </svg>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="https://blog.xiaoquankong.ai/">Home</a>&nbsp;»&nbsp;<a href="https://blog.xiaoquankong.ai/posts/">Posts</a></div>
    <h1 class="post-title">
      The decoding process of ChatGPT and the various parameters in it
    </h1>
    <div class="post-meta"><span title='2023-08-07 22:10:08 +0800 CST'>August 7, 2023</span>&nbsp;·&nbsp;9 min&nbsp;·&nbsp;Xiaoquan Kong&nbsp;|&nbsp;Translations:
<ul class="i18n_list">
    <li>
        <a href="https://blog.xiaoquankong.ai/zh/posts/the-decoding-process-of-chatgpt-and-the-various-parameters-in-it/">中文</a>
    </li>
</ul>

</div>
  </header> <div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#chatgpts-decoding-process" aria-label="ChatGPT&amp;rsquo;s Decoding Process">ChatGPT&rsquo;s Decoding Process</a></li>
                <li>
                    <a href="#decoding-parameters-of-chatgpt" aria-label="Decoding Parameters of ChatGPT">Decoding Parameters of ChatGPT</a><ul>
                        
                <li>
                    <a href="#temperature" aria-label="temperature">temperature</a></li>
                <li>
                    <a href="#top_p" aria-label="top_p">top_p</a></li>
                <li>
                    <a href="#stop" aria-label="stop">stop</a></li>
                <li>
                    <a href="#n" aria-label="n">n</a></li>
                <li>
                    <a href="#max_tokens" aria-label="max_tokens">max_tokens</a></li>
                <li>
                    <a href="#presence_penalty" aria-label="presence_penalty">presence_penalty</a></li>
                <li>
                    <a href="#frequency_penalty" aria-label="frequency_penalty">frequency_penalty</a></li>
                <li>
                    <a href="#logit_bias" aria-label="logit_bias">logit_bias</a>
                </li>
            </ul>
            </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><p><strong>TL;DR</strong> OpenAI&rsquo;s ChatGPT provides the range and meaning of various parameters in its official documentation (<a href="https://platform.openai.com/docs/api-reference/chat/create">https://platform.openai.com/docs/api-reference/chat/create</a>). We will discuss ChatGPT&rsquo;s generation process and how these parameters implement its generation effects.</p>
<h2 id="chatgpts-decoding-process">ChatGPT&rsquo;s Decoding Process<a hidden class="anchor" aria-hidden="true" href="#chatgpts-decoding-process">#</a></h2>
<p>We assume minGPT (equivalent to GPT-2) and ChatGPT have the same decoding process: <a href="https://github.com/karpathy/minGPT/blob/master/mingpt/model.py#LL283C12-L283C12">https://github.com/karpathy/minGPT/blob/master/mingpt/model.py#LL283C12-L283C12</a>.</p>
<p>The overall process can be summarized as the following steps:</p>
<ol>
<li>Expand the user&rsquo;s request from 1 to a batch size of num_samples</li>
<li>Perform model inference to obtain logits</li>
<li>Perform temperature mapping: logits = logits / temperature</li>
<li>[Optional] Perform topk processing: logits = topk_func(logits, top_k)</li>
<li>Map logits to probabilities: probs = softmax(logits)</li>
<li>Whether to sample:
<ol>
<li>Sample: idx_next = multinomial_sample(probs, num_samples=1)</li>
<li>Don&rsquo;t sample: idx_next = topk_func(probs, k=1)</li>
</ol>
</li>
<li>Repeat the above process max_new_tokens times</li>
</ol>
<h2 id="decoding-parameters-of-chatgpt">Decoding Parameters of ChatGPT<a hidden class="anchor" aria-hidden="true" href="#decoding-parameters-of-chatgpt">#</a></h2>
<h3 id="temperature">temperature<a hidden class="anchor" aria-hidden="true" href="#temperature">#</a></h3>
<p>The official definition of the temperature parameter is:</p>
<blockquote>
<p>temperature number Optional Defaults to 1</p>
<p>What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.</p>
<p>We generally recommend altering this or <strong><code>top_p</code></strong> but not both.</p>
</blockquote>
<p>This corresponds to step 3 of the decoding process.</p>
<p>Below we will demonstrate its effect with example data combined with the model decoding process (to simplify the logic, we will not perform topk processing):</p>
<ol>
<li>Assume the vocabulary size of a model is 2. At some point, the model output is logits = [0.8, 0.2].</li>
<li>If no temperature mapping is performed (equivalent to setting temperature to 1, which is also the default value): Probability conversion: probs = softmax(logits) = [0.65, 0.35]</li>
<li>If temperature is set to 1.8, then logits = logits / temperature = [0.44, 0.11], next step is probability conversion: probs = softmax(logits) = [0.58, 0.42]</li>
<li>If temperature is set to 0.2, then logits = logits / temperature = [4, 1], next step is probability conversion: probs = softmax(logits) = [0.95, 0.05]</li>
</ol>
<p>Summary: From the above data, it can be seen that the larger the temperature, the smaller the difference in probability between different tokens with different logits after mapping, and the more likely it is to be randomly selected by the subsequent sample section.</p>
<p>It is worth noting that the temperature range of the GPT model is from 0 (inclusive) to 2 (inclusive). However, when temperature=0, it is impossible to be used as a divisor in numerical calculations, so ChatGPT must have adopted some tricks or transformations to solve this problem.</p>
<p>We draw a graph to demonstrate the performance of different temperatures on different logits:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># importing package</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">math</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># x axis index and values</span>
</span></span><span class="line"><span class="cl"><span class="n">data</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">enumerate</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mf">0.9</span><span class="p">,</span> <span class="mf">0.4</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1</span><span class="p">))))</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># colors for each temperature, from low to high temperature, from yellow to dark red</span>
</span></span><span class="line"><span class="cl"><span class="c1"># reference: https://colorbrewer2.org/#type=sequential&amp;scheme=YlOrRd&amp;n=5</span>
</span></span><span class="line"><span class="cl"><span class="n">colors</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&#34;#ffffb2&#34;</span><span class="p">,</span> <span class="s2">&#34;#fecc5c&#34;</span><span class="p">,</span> <span class="s2">&#34;#fd8d3c&#34;</span><span class="p">,</span> <span class="s2">&#34;#f03b20&#34;</span><span class="p">,</span> <span class="s2">&#34;#bd0026&#34;</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">for</span> <span class="n">t_idx</span><span class="p">,</span> <span class="n">temperature</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mf">0.4</span><span class="p">,</span> <span class="mf">1.6</span> <span class="o">+</span> <span class="mf">0.0001</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">)):</span>
</span></span><span class="line"><span class="cl">    <span class="c1"># each line for each temperature</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># get x and y values</span>
</span></span><span class="line"><span class="cl">    <span class="n">x</span> <span class="o">=</span> <span class="p">[]</span>
</span></span><span class="line"><span class="cl">    <span class="n">y</span> <span class="o">=</span> <span class="p">[]</span>
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">x_idx</span><span class="p">,</span> <span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span> <span class="ow">in</span> <span class="n">data</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="n">logits</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">])</span>
</span></span><span class="line"><span class="cl">        <span class="n">probs</span> <span class="o">=</span> <span class="n">softmax</span><span class="p">(</span><span class="n">logits</span> <span class="o">/</span> <span class="n">temperature</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">x</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">x_idx</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">y</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">probs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">/</span> <span class="n">probs</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="c1"># max prob / min prob</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    <span class="c1"># plot</span>
</span></span><span class="line"><span class="cl">    <span class="n">circle_color</span> <span class="o">=</span> <span class="n">colors</span><span class="p">[</span><span class="n">t_idx</span><span class="p">]</span>
</span></span><span class="line"><span class="cl">    <span class="k">if</span> <span class="n">math</span><span class="o">.</span><span class="n">isclose</span><span class="p">(</span><span class="n">temperature</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># plot the line for temperature 1.0 with black circles</span>
</span></span><span class="line"><span class="cl">        <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s2">&#34;</span><span class="si">{</span><span class="n">temperature</span><span class="si">:</span><span class="s2">.1f</span><span class="si">}</span><span class="s2">&#34;</span><span class="p">,</span> <span class="n">facecolors</span><span class="o">=</span><span class="s2">&#34;black&#34;</span><span class="p">,</span> <span class="n">edgecolors</span><span class="o">=</span><span class="s2">&#34;black&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">    <span class="k">else</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">        <span class="c1"># other lines with colorful lines</span>
</span></span><span class="line"><span class="cl">        <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s2">&#34;</span><span class="si">{</span><span class="n">temperature</span><span class="si">:</span><span class="s2">.1f</span><span class="si">}</span><span class="s2">&#34;</span><span class="p">,</span> <span class="n">facecolors</span><span class="o">=</span><span class="n">circle_color</span><span class="p">,</span> <span class="n">edgecolors</span><span class="o">=</span><span class="s2">&#34;gray&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># set x and y axis</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;logits&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">([</span><span class="n">x</span> <span class="k">for</span> <span class="n">x</span><span class="p">,</span> <span class="n">_</span> <span class="ow">in</span> <span class="n">data</span><span class="p">],</span>
</span></span><span class="line"><span class="cl">           <span class="p">[</span><span class="sa">f</span><span class="s2">&#34;[</span><span class="si">{</span><span class="n">a</span><span class="si">:</span><span class="s2">.1f</span><span class="si">}</span><span class="s2">, </span><span class="si">{</span><span class="n">b</span><span class="si">:</span><span class="s2">.1f</span><span class="si">}</span><span class="s2">]&#34;</span> <span class="k">for</span> <span class="n">_</span><span class="p">,</span> <span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span> <span class="ow">in</span> <span class="n">data</span><span class="p">])</span>
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;ratio of max/min prob&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</span></span></code></pre></div><p>The following graph will be output:</p>
<p><img loading="lazy" src="output.png" alt="output.png"  />
</p>
<p>In the above graph, the x-axis represents logits (of two categories), and the y-axis represents the ratio of the maximum probability to the minimum probability, which can be used to measure the size of the difference.</p>
<p>When temperature is not introduced, the probability ratio and logits are strictly related, and the value of logits can be mapped to probability values through the softmax function. In the above graph, when temperature=0, it is equivalent to the case without introducing temperature, and is represented by a hollow circle ◯ in the graph.</p>
<p>By observation, it can be seen that no matter which logits we choose, we can see that the larger the temperature, the smaller the difference between probabilities (i.e., the ratio of max prob / min prob), which means that the probability difference is smaller. The opposite is also true. Therefore, it can be concluded that the larger the temperature, the more random the results generated by the model, and the smaller the temperature, the more deterministic the results generated by the model.</p>
<h3 id="top_p">top_p<a hidden class="anchor" aria-hidden="true" href="#top_p">#</a></h3>
<p>The official definition of the top_p parameter is:</p>
<blockquote>
<p>top_p number Optional Defaults to 1</p>
<p>An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.</p>
<p>We generally recommend altering this or <strong><code>temperature</code></strong> but not both.</p>
</blockquote>
<p>This corresponds to step 4 of the decoding process.</p>
<p>Unlike minGPT, which uses absolute values ​​(top_n) for selection, OpenAI GPT uses percentages (top_p).</p>
<p>This part will clean up unqualified tokens (not in top_n or outside top_p ratio) by setting their logit values ​​to float(&lsquo;Inf&rsquo;).</p>
<h3 id="stop">stop<a hidden class="anchor" aria-hidden="true" href="#stop">#</a></h3>
<p>The official definition of the stop parameter is:</p>
<blockquote>
<p>stop string or array Optional Defaults to null</p>
<p>Up to 4 sequences where the API will stop generating further tokens.</p>
</blockquote>
<p>There is no corresponding step in MinGPT.</p>
<p>The meaning expressed by this part is also clear and unambiguous. It will stop generating when certain defined strings are detected in the output. This feature may have been applied in some software. For example <code>{{gen 'rewrite' stop=&quot;\\n-&quot;}}</code> in <a href="https://github.com/microsoft/guidance">https://github.com/microsoft/guidance</a></p>
<h3 id="n">n<a hidden class="anchor" aria-hidden="true" href="#n">#</a></h3>
<p>The official definition of the n parameter is:</p>
<blockquote>
<p>n integer Optional Defaults to 1</p>
<p>How many chat completion choices to generate for each input message.</p>
</blockquote>
<p>This corresponds to step 1 of the decoding process.</p>
<p>Since each text in the batch of size n is sampled independently, different tokens may be selected at the same position, and these variations on the texts expand further as the position continues to extend, eventually generating different texts. Of course, there is also a certain probability of generating completely identical texts.</p>
<h3 id="max_tokens">max_tokens<a hidden class="anchor" aria-hidden="true" href="#max_tokens">#</a></h3>
<p>The official definition of the max_tokens parameter is:</p>
<blockquote>
<p>max_tokens integer Optional Defaults to inf</p>
<p>The maximum number of <strong><a href="https://platform.openai.com/tokenizer">tokens</a></strong> to generate in the chat completion.</p>
<p>The total length of input tokens and generated tokens is limited by the model&rsquo;s context length.</p>
</blockquote>
<p>This corresponds to step 7 of the decoding process.</p>
<p>This part determines the maximum number of decoding runs. In minGPT, this number of decodings is fixed, and the model will definitely generate max_tokens tokens. But in OpenAI GPT it is not necessarily the case, due to several factors:</p>
<ul>
<li>The setting of the stop parameter, see above for details.</li>
<li>Possible special pause tokens. Through actual use of ChatGPT, it can be found that ChatGPT does not mechanically output the specified text length, but will stop by itself after fully answering the question.</li>
<li>Experimental code is as follows:</li>
</ul>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">openai</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">openai</span><span class="o">.</span><span class="n">api_key</span> <span class="o">=</span> <span class="s2">&#34;sk-xxx&#34;</span> 
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">completion</span> <span class="o">=</span> <span class="n">openai</span><span class="o">.</span><span class="n">ChatCompletion</span><span class="o">.</span><span class="n">create</span><span class="p">(</span>
</span></span><span class="line"><span class="cl">    <span class="n">model</span><span class="o">=</span><span class="s2">&#34;gpt-4&#34;</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="n">messages</span><span class="o">=</span><span class="p">[{</span><span class="s2">&#34;role&#34;</span><span class="p">:</span> <span class="s2">&#34;user&#34;</span><span class="p">,</span> <span class="s2">&#34;content&#34;</span><span class="p">:</span> <span class="s2">&#34;Help me output the even numbers between 1 and 10, separated by spaces. Do not output anything other than numbers.&#34;</span><span class="p">}],</span> 
</span></span><span class="line"><span class="cl">    <span class="n">temperature</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">    <span class="n">max_tokens</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
</span></span><span class="line"><span class="cl"><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="n">response</span> <span class="o">=</span> <span class="n">completion</span><span class="o">.</span><span class="n">choices</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">message</span><span class="p">[</span><span class="s2">&#34;content&#34;</span><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s2">&#34;length: &#34;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">response</span><span class="p">))</span> <span class="c1"># will output: length: 10  </span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="n">response</span><span class="p">)</span> <span class="c1"># will output: 2 4 6 8 10</span>
</span></span></code></pre></div><h3 id="presence_penalty">presence_penalty<a hidden class="anchor" aria-hidden="true" href="#presence_penalty">#</a></h3>
<p>The official definition of the presence_penalty parameter is:</p>
<blockquote>
<p>frequency_penalty number Optional Defaults to 0</p>
<p>Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model&rsquo;s likelihood to repeat the same line verbatim.</p>
</blockquote>
<p>There is no corresponding step in MinGPT.</p>
<p>It is explained in detail at <a href="https://platform.openai.com/docs/api-reference/parameter-details">https://platform.openai.com/docs/api-reference/parameter-details</a>.</p>
<p>Specifically, at a certain decoding timestep, the logit value of token <code>j</code> is <code>mu[j]</code>, c[j] indicates how many times j has appeared in the currently generated text. The value of <code>c[j] &gt; 0</code> can only be 1 (j has appeared at least once before) or 0 (has not appeared before). In OpenAI&rsquo;s explanation, they use <code>alpha_presence</code> to refer to presence_penalty, the two are completely the same thing with different symbols. To be consistent with the documentation, the symbols in the documentation are used here. After introducing the presence_penalty mechanism, its value is revised to <code>mu[j] - float(c[j] &gt; 0) * alpha_presence</code>. This means that when <code>alpha_presence</code> is positive, the logit of token j will be reduced because j has been generated in the previous text. The decrease in logit also means a decrease in the probability of being sampled. Therefore, by providing a positive presence_penalty, the probability of the model generating repeated tokens will be reduced, in other words, a penalty is imposed. Conversely, if <code>alpha_presence</code> is negative, it will encourage the model to generate repeated tokens.</p>
<p>Although presence_penalty contains the word penalty, since its value range can be both positive and negative, it does not necessarily penalize the repeated appearance of tokens, but can also encourage repetition.</p>
<h3 id="frequency_penalty">frequency_penalty<a hidden class="anchor" aria-hidden="true" href="#frequency_penalty">#</a></h3>
<p>The official definition of the frequency_penalty parameter is:</p>
<blockquote>
<p>frequency_penalty number Optional Defaults to 0</p>
<p>Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model&rsquo;s likelihood to repeat the same line verbatim.</p>
</blockquote>
<p>There is no corresponding step in MinGPT.</p>
<p>This parameter is highly similar to presence_penalty. It is also explained in detail at <a href="https://platform.openai.com/docs/api-reference/parameter-details">https://platform.openai.com/docs/api-reference/parameter-details</a>.</p>
<p>Specifically, the logit value of token <code>j</code> is <code>mu[j]</code>. After applying frequency_penalty, it will be revised to <code>mu[j] -&gt; mu[j] - c[j] * alpha_frequency</code>. Where <code>c[j]</code> is how many times j has appeared in the currently generated text. And <code>alpha_frequency</code> is frequency_penalty. This means that when frequency_penalty is positive, the logit of token j will decrease because j has been generated in the previous text, and the more j has been generated before (i.e. the larger the <code>c[j]</code>), the more severe the penalty. Here we can see the difference between frequency_penalty and presence_penalty: frequency_penalty strengthens the penalty as the number of occurrences of the token increases, while presence_penalty only distinguishes whether it has occurred, which is fully reflected in their name difference: frequency and presence.</p>
<p>Similar to presence, frequency_penalty can take both positive and negative values, thereby implementing penalties or rewards for repeating tokens.</p>
<h3 id="logit_bias">logit_bias<a hidden class="anchor" aria-hidden="true" href="#logit_bias">#</a></h3>
<p>The official definition of the logit_bias parameter is:</p>
<blockquote>
<p>logit_bias map Optional Defaults to null</p>
<p>Modify the likelihood of specified tokens appearing in the completion.</p>
<p>Accepts a json object that maps tokens (specified by their token ID in the tokenizer) to an associated bias value from -100 to 100. Mathematically, the bias is added to the logits generated by the model prior to sampling. The exact effect will vary per model, but values between -1 and 1 should decrease or increase likelihood of selection; values like -100 or 100 should result in a ban or exclusive selection of the relevant token.</p>
</blockquote>
<p>There is no corresponding step in MinGPT.</p>
<p>This parameter is used to unconditionally modify the logits of one or more tokens, thereby increasing or decreasing their likelihood of occurrence. Specifically, for variable token <code>j</code>, its logit value is <code>mu[j]</code>. After using logit_bias, its value will be modified to: <code>mu[j] -&gt; mu[j] + logit_bias[j]</code>.</p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
    </ul>
<nav class="paginav">
  <a class="next" href="https://blog.xiaoquankong.ai/posts/solution-for-tensorboard-embedding-blocked-when-loading-metadata/">
    <span class="title">Next »</span>
    <br>
    <span>Solution for TensorBoard embedding blocked when loading metadata</span>
  </a>
</nav>


<div class="share-buttons">
    <a target="_blank" rel="noopener noreferrer" aria-label="share The decoding process of ChatGPT and the various parameters in it on twitter"
        href="https://twitter.com/intent/tweet/?text=The%20decoding%20process%20of%20ChatGPT%20and%20the%20various%20parameters%20in%20it&amp;url=https%3a%2f%2fblog.xiaoquankong.ai%2fposts%2fthe-decoding-process-of-chatgpt-and-the-various-parameters-in-it%2f&amp;hashtags=">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-253.927,424.544c135.939,0 210.268,-112.643 210.268,-210.268c0,-3.218 0,-6.437 -0.153,-9.502c14.406,-10.421 26.973,-23.448 36.935,-38.314c-13.18,5.824 -27.433,9.809 -42.452,11.648c15.326,-9.196 26.973,-23.602 32.49,-40.92c-14.252,8.429 -30.038,14.56 -46.896,17.931c-13.487,-14.406 -32.644,-23.295 -53.946,-23.295c-40.767,0 -73.87,33.104 -73.87,73.87c0,5.824 0.613,11.494 1.992,16.858c-61.456,-3.065 -115.862,-32.49 -152.337,-77.241c-6.284,10.881 -9.962,23.601 -9.962,37.088c0,25.594 13.027,48.276 32.95,61.456c-12.107,-0.307 -23.448,-3.678 -33.41,-9.196l0,0.92c0,35.862 25.441,65.594 59.311,72.49c-6.13,1.686 -12.72,2.606 -19.464,2.606c-4.751,0 -9.348,-0.46 -13.946,-1.38c9.349,29.426 36.628,50.728 68.965,51.341c-25.287,19.771 -57.164,31.571 -91.8,31.571c-5.977,0 -11.801,-0.306 -17.625,-1.073c32.337,21.15 71.264,33.41 112.95,33.41Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share The decoding process of ChatGPT and the various parameters in it on linkedin"
        href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fblog.xiaoquankong.ai%2fposts%2fthe-decoding-process-of-chatgpt-and-the-various-parameters-in-it%2f&amp;title=The%20decoding%20process%20of%20ChatGPT%20and%20the%20various%20parameters%20in%20it&amp;summary=The%20decoding%20process%20of%20ChatGPT%20and%20the%20various%20parameters%20in%20it&amp;source=https%3a%2f%2fblog.xiaoquankong.ai%2fposts%2fthe-decoding-process-of-chatgpt-and-the-various-parameters-in-it%2f">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-288.985,423.278l0,-225.717l-75.04,0l0,225.717l75.04,0Zm270.539,0l0,-129.439c0,-69.333 -37.018,-101.586 -86.381,-101.586c-39.804,0 -57.634,21.891 -67.617,37.266l0,-31.958l-75.021,0c0.995,21.181 0,225.717 0,225.717l75.02,0l0,-126.056c0,-6.748 0.486,-13.492 2.474,-18.315c5.414,-13.475 17.767,-27.434 38.494,-27.434c27.135,0 38.007,20.707 38.007,51.037l0,120.768l75.024,0Zm-307.552,-334.556c-25.674,0 -42.448,16.879 -42.448,39.002c0,21.658 16.264,39.002 41.455,39.002l0.484,0c26.165,0 42.452,-17.344 42.452,-39.002c-0.485,-22.092 -16.241,-38.954 -41.943,-39.002Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share The decoding process of ChatGPT and the various parameters in it on reddit"
        href="https://reddit.com/submit?url=https%3a%2f%2fblog.xiaoquankong.ai%2fposts%2fthe-decoding-process-of-chatgpt-and-the-various-parameters-in-it%2f&title=The%20decoding%20process%20of%20ChatGPT%20and%20the%20various%20parameters%20in%20it">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-3.446,265.638c0,-22.964 -18.616,-41.58 -41.58,-41.58c-11.211,0 -21.361,4.457 -28.841,11.666c-28.424,-20.508 -67.586,-33.757 -111.204,-35.278l18.941,-89.121l61.884,13.157c0.756,15.734 13.642,28.29 29.56,28.29c16.407,0 29.706,-13.299 29.706,-29.701c0,-16.403 -13.299,-29.702 -29.706,-29.702c-11.666,0 -21.657,6.792 -26.515,16.578l-69.105,-14.69c-1.922,-0.418 -3.939,-0.042 -5.585,1.036c-1.658,1.073 -2.811,2.761 -3.224,4.686l-21.152,99.438c-44.258,1.228 -84.046,14.494 -112.837,35.232c-7.468,-7.164 -17.589,-11.591 -28.757,-11.591c-22.965,0 -41.585,18.616 -41.585,41.58c0,16.896 10.095,31.41 24.568,37.918c-0.639,4.135 -0.99,8.328 -0.99,12.576c0,63.977 74.469,115.836 166.33,115.836c91.861,0 166.334,-51.859 166.334,-115.836c0,-4.218 -0.347,-8.387 -0.977,-12.493c14.564,-6.47 24.735,-21.034 24.735,-38.001Zm-119.474,108.193c-20.27,20.241 -59.115,21.816 -70.534,21.816c-11.428,0 -50.277,-1.575 -70.522,-21.82c-3.007,-3.008 -3.007,-7.882 0,-10.889c3.003,-2.999 7.882,-3.003 10.885,0c12.777,12.781 40.11,17.317 59.637,17.317c19.522,0 46.86,-4.536 59.657,-17.321c3.016,-2.999 7.886,-2.995 10.885,0.008c3.008,3.011 3.003,7.882 -0.008,10.889Zm-5.23,-48.781c-16.373,0 -29.701,-13.324 -29.701,-29.698c0,-16.381 13.328,-29.714 29.701,-29.714c16.378,0 29.706,13.333 29.706,29.714c0,16.374 -13.328,29.698 -29.706,29.698Zm-160.386,-29.702c0,-16.381 13.328,-29.71 29.714,-29.71c16.369,0 29.689,13.329 29.689,29.71c0,16.373 -13.32,29.693 -29.689,29.693c-16.386,0 -29.714,-13.32 -29.714,-29.693Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share The decoding process of ChatGPT and the various parameters in it on facebook"
        href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fblog.xiaoquankong.ai%2fposts%2fthe-decoding-process-of-chatgpt-and-the-various-parameters-in-it%2f">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-106.468,0l0,-192.915l66.6,0l12.672,-82.621l-79.272,0l0,-53.617c0,-22.603 11.073,-44.636 46.58,-44.636l36.042,0l0,-70.34c0,0 -32.71,-5.582 -63.982,-5.582c-65.288,0 -107.96,39.569 -107.96,111.204l0,62.971l-72.573,0l0,82.621l72.573,0l0,192.915l-191.104,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share The decoding process of ChatGPT and the various parameters in it on whatsapp"
        href="https://api.whatsapp.com/send?text=The%20decoding%20process%20of%20ChatGPT%20and%20the%20various%20parameters%20in%20it%20-%20https%3a%2f%2fblog.xiaoquankong.ai%2fposts%2fthe-decoding-process-of-chatgpt-and-the-various-parameters-in-it%2f">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-58.673,127.703c-33.842,-33.881 -78.847,-52.548 -126.798,-52.568c-98.799,0 -179.21,80.405 -179.249,179.234c-0.013,31.593 8.241,62.428 23.927,89.612l-25.429,92.884l95.021,-24.925c26.181,14.28 55.659,21.807 85.658,21.816l0.074,0c98.789,0 179.206,-80.413 179.247,-179.243c0.018,-47.895 -18.61,-92.93 -52.451,-126.81Zm-126.797,275.782l-0.06,0c-26.734,-0.01 -52.954,-7.193 -75.828,-20.767l-5.441,-3.229l-56.386,14.792l15.05,-54.977l-3.542,-5.637c-14.913,-23.72 -22.791,-51.136 -22.779,-79.287c0.033,-82.142 66.867,-148.971 149.046,-148.971c39.793,0.014 77.199,15.531 105.329,43.692c28.128,28.16 43.609,65.592 43.594,105.4c-0.034,82.149 -66.866,148.983 -148.983,148.984Zm81.721,-111.581c-4.479,-2.242 -26.499,-13.075 -30.604,-14.571c-4.105,-1.495 -7.091,-2.241 -10.077,2.241c-2.986,4.483 -11.569,14.572 -14.182,17.562c-2.612,2.988 -5.225,3.364 -9.703,1.12c-4.479,-2.241 -18.91,-6.97 -36.017,-22.23c-13.314,-11.876 -22.304,-26.542 -24.916,-31.026c-2.612,-4.484 -0.279,-6.908 1.963,-9.14c2.016,-2.007 4.48,-5.232 6.719,-7.847c2.24,-2.615 2.986,-4.484 4.479,-7.472c1.493,-2.99 0.747,-5.604 -0.374,-7.846c-1.119,-2.241 -10.077,-24.288 -13.809,-33.256c-3.635,-8.733 -7.327,-7.55 -10.077,-7.688c-2.609,-0.13 -5.598,-0.158 -8.583,-0.158c-2.986,0 -7.839,1.121 -11.944,5.604c-4.105,4.484 -15.675,15.32 -15.675,37.364c0,22.046 16.048,43.342 18.287,46.332c2.24,2.99 31.582,48.227 76.511,67.627c10.685,4.615 19.028,7.371 25.533,9.434c10.728,3.41 20.492,2.929 28.209,1.775c8.605,-1.285 26.499,-10.833 30.231,-21.295c3.732,-10.464 3.732,-19.431 2.612,-21.298c-1.119,-1.869 -4.105,-2.99 -8.583,-5.232Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share The decoding process of ChatGPT and the various parameters in it on telegram"
        href="https://telegram.me/share/url?text=The%20decoding%20process%20of%20ChatGPT%20and%20the%20various%20parameters%20in%20it&amp;url=https%3a%2f%2fblog.xiaoquankong.ai%2fposts%2fthe-decoding-process-of-chatgpt-and-the-various-parameters-in-it%2f">
        <svg version="1.1" xml:space="preserve" viewBox="2 2 28 28" height="30px" width="30px" fill="currentColor">
            <path
                d="M26.49,29.86H5.5a3.37,3.37,0,0,1-2.47-1,3.35,3.35,0,0,1-1-2.47V5.48A3.36,3.36,0,0,1,3,3,3.37,3.37,0,0,1,5.5,2h21A3.38,3.38,0,0,1,29,3a3.36,3.36,0,0,1,1,2.46V26.37a3.35,3.35,0,0,1-1,2.47A3.38,3.38,0,0,1,26.49,29.86Zm-5.38-6.71a.79.79,0,0,0,.85-.66L24.73,9.24a.55.55,0,0,0-.18-.46.62.62,0,0,0-.41-.17q-.08,0-16.53,6.11a.59.59,0,0,0-.41.59.57.57,0,0,0,.43.52l4,1.24,1.61,4.83a.62.62,0,0,0,.63.43.56.56,0,0,0,.4-.17L16.54,20l4.09,3A.9.9,0,0,0,21.11,23.15ZM13.8,20.71l-1.21-4q8.72-5.55,8.78-5.55c.15,0,.23,0,.23.16a.18.18,0,0,1,0,.06s-2.51,2.3-7.52,6.8Z" />
        </svg>
    </a>
</div>
  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2023 <a href="https://blog.xiaoquankong.ai/">Xiaoquan Kong&#39;s Blog</a></span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
