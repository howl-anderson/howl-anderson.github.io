[{"content":"TL:DR OpenAI 的 ChatGPT 在其官方文档（https://platform.openai.com/docs/api-reference/chat/create）中给出了各种参数的范围和含义。我们将讨论 ChatGPT 的生成过程和这些参数是如何实现其生成的效果的。\nChatGPT 的解码过程 我们假设 minGPT （等同于 GPT-2） 和 ChatGPT 拥有一样的解码过程：https://github.com/karpathy/minGPT/blob/master/mingpt/model.py#LL283C12-L283C12 。\n总体过程可以概括为以下几个步骤：\n将用户的请求，从 1 个扩充成 num_samples 大小的 batch 进行模型推理，得到 logits 进行 temperature 映射：logits = logits / temperature [可选] 进行 topk 处理：logits = topk_func(logits, top_k) logits 到 概率的转换：probs = softmax(logits) 是否 sample： 进行 sample：idx_next = multinomial_sample(probs, num_samples=1) 不进行 sample：idx_next = topk_func(probs, k=1) 重复上述过程 max_new_tokens 次 ChatGPT 的解码参数 temperature temperature 参数的官方定义如下：\ntemperature number Optional Defaults to 1\nWhat sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.\nWe generally recommend altering this or top_p but not both.\n这一部分对应着解码过程的步骤 3.\n下面我们将结合模型解码过程，使用数据示例来演示其效果（为了简化逻辑过程，我们不进行 topk 处理）：\n假设某个模型的 vocabulary 的大小为 2，在某个时刻，模型的输出为 logits = [0.8, 0.2]。 如果不进行 temperature 映射（等价于将 temperature 设置为 1， 也就是默认值）： 概率转换：probs = softmax(logits) = [0.65, 0.35] 如果 temperature 设置为 1.8，那么 logits = logits / temperature = [0.44, 0.11], 下一步进行概率转换：probs = softmax(logits) = [0.58, 0.42] 如果 temperature 设置为 0.2，那么 logits = logits / temperature = [4, 1], 下一步进行概率转换：probs = softmax(logits) = [0.95, 0.05] 总结：从上面的数据可以看出，temperature 越大，logits 数值不同的 token 经过映射后其概率差异越小，从而在后续的 sample 部分被 sample 的概率差异越小，也就是说，temperature 越大，模型生成的结果越随机。反之亦然。\n值得注意的是，GPT 模型的 temperature 取值范围是 0（包含） 到 2（包含）。但是 temperature=0，这个在数值上是无法作为被除数的，ChatGPT 必然采用了某种 trick 或者变换以解决这个问题。\n我们画一张图用来演示，不同 temperature 在不同 logits 上的表现：\n# importing package import matplotlib.pyplot as plt import numpy as np import math # x axis index and values data = list(enumerate(zip(np.arange(0.1, 0.6, 0.1), np.arange(0.9, 0.4, -0.1)))) # colors for each temperature, from low to high temperature, from yellow to dark red # reference: https://colorbrewer2.org/#type=sequential\u0026amp;scheme=YlOrRd\u0026amp;n=5 colors = [\u0026#34;#ffffb2\u0026#34;, \u0026#34;#fecc5c\u0026#34;, \u0026#34;#fd8d3c\u0026#34;, \u0026#34;#f03b20\u0026#34;, \u0026#34;#bd0026\u0026#34;] for t_idx, temperature in enumerate(np.arange(0.4, 1.6 + 0.0001, 0.3)): # each line for each temperature # get x and y values x = [] y = [] for x_idx, (a, b) in data: logits = np.array([a, b]) probs = softmax(logits / temperature) x.append(x_idx) y.append(probs[1] / probs[0]) # max prob / min prob # plot circle_color = colors[t_idx] if math.isclose(temperature, 1.0): # plot the line for temperature 1.0 with black circles plt.scatter(x, y, label=f\u0026#34;{temperature:.1f}\u0026#34;, facecolors=\u0026#34;black\u0026#34;, edgecolors=\u0026#34;black\u0026#34;) else: # other lines with colorful lines plt.scatter(x, y, label=f\u0026#34;{temperature:.1f}\u0026#34;, facecolors=circle_color, edgecolors=\u0026#34;gray\u0026#34;) plt.legend() # set x and y axis plt.xlabel(\u0026#39;logits\u0026#39;) plt.xticks([x for x, _ in data], [f\u0026#34;[{a:.1f}, {b:.1f}]\u0026#34; for _, (a, b) in data]) plt.ylabel(\u0026#39;ratio of max/min prob\u0026#39;) plt.show() 将会输出如下的图：\n在上图中，横坐标为 logits（由两个类别构成），纵坐标为 max prob / min prob，也就是概率最大的 token 的概率与概率最小的 token 的概率的比值，这个比值可以用于衡量差异的大小。\n在没有引入 temperature 时，概率的比值和 logits 存在严格相关的，logits 的值通过 softmax 函数可以映射得到概率值。在上图中，temperature = 0 的情况，等价于没有引入 temperature 的情况，在图中使用空心圆圈 ◯ 表示。。\n通过观察，可知无论我们选择哪一个 logits，我们都可以看到：temperature 越大，概率之间的差异（也就是 max prob / min prob 比值）越小，也就是概率差异越小。反之亦然。因此，可以得出结论：temperature 越大，模型生成的结果越随机，temperature 越小，模型生成的结果越确定。\ntop_p top_p 参数的官方定义如下：\ntop_p number Optional Defaults to 1\nAn alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.\nWe generally recommend altering this or temperature but not both.\n这一部分对应着解码过程的步骤 4.\n与 minGPT 不同的是，minGPT 使用绝对值（top_n）来进行选择，而 OpenAI GPT 使用百分比（top_p）。\n这一部分会将不合格（比在 top_n 以内的，或者 top_p 比例以外的）的 token 清理掉，通过将其 logits 值设置为 float(\u0026lsquo;Inf\u0026rsquo;) 来实现。\nstop stop 参数的官方定义如下：\nstop string or array Optional Defaults to null\nUp to 4 sequences where the API will stop generating further tokens.\n这一部分在 MinGPT 中没有对应的步骤。 这一部分所要表达的含义也是清晰明了，在监测到输出中存在某些定义的字符串后，将会停止生成。这一特性可能在有些软件中得到了应用。比如 https://github.com/microsoft/guidance 中的 {{gen 'rewrite' stop=\u0026quot;\\\\n-\u0026quot;}}\nn n 参数的官方定义如下：\nn integer Optional Defaults to 1\nHow many chat completion choices to generate for each input message.\n这一部分对应着解码过程的步骤1。\n由于大小为 n 的 batch 中每一个文本都是独立采样的，因此在同一位置可能会选择不同的 token，这些文本上的变异随着位置的不断延伸而进一步扩大，最终生成了不同的文本。当然了，也有一定概率生成完全一样的文本。\nmax_tokens max_tokens 参数的官方定义如下：\nmax_tokens integer Optional Defaults to inf\nThe maximum number of tokens to generate in the chat completion.\nThe total length of input tokens and generated tokens is limited by the model\u0026rsquo;s context length.\n这一部分对应着解码过程中的步骤7.\n这一部分决定了解码的最高运行次数。在 minGPT 中，这一解码次数是确定的，模型一定会生成 max_tokens 个 token。而在 OpenAI GPT 中则不一定了，有几个因素：\nstop 参数的设置，详情请见上文。 可能的特殊的休止符 token。通过实际使用 ChatGPT，可以发现 ChatGPT 并不会机械的输出指定的文本长度，在充分回答问题后，就会自行停止。 实验代码如下： import openai openai.api_key = \u0026#34;sk-xxx\u0026#34; completion = openai.ChatCompletion.create( model=\u0026#34;gpt-4\u0026#34;, messages=[{\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;你帮我输出1到10之间的偶数，输出时，每个数字之间用一个空格隔开。除了数字，其他的都不要输出。\u0026#34;}], temperature=0, max_tokens=100, ) response = completion.choices[0].message[\u0026#34;content\u0026#34;] print(\u0026#34;length: \u0026#34;, len(response)) # 将会输出：length: 10 print(response) # 将会输出：2 4 6 8 10 presence_penalty presence_penalty 参数的官方定义如下：\nfrequency_penalty number Optional Defaults to 0\nNumber between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model\u0026rsquo;s likelihood to repeat the same line verbatim.\n这一部分在 MinGPT 中没有对应的步骤。\n这一部分的详尽解释在 https://platform.openai.com/docs/api-reference/parameter-details 中有提及。\n具体来说，就是在解码的某个时刻，token j 的 logit 值为 mu[j] ，c[j] 表示在当前已经生成的文本中，出现过多少个 j 这个 token。c[j] \u0026gt; 0 这个表达式的值只能为1（之前 j 出现过至少一次）或者0（没有出现过）。在 OpenAI 的解释中，它使用了 alpha_presence 来指代 presence_penalty，两者完全是同一事物的不同符号而已。为了保持和文档一致，这里都使用文档中的符号。在加入 presence_penalty 机制后，其值修订为 mu[j] - float(c[j] \u0026gt; 0) * alpha_presence 。这就意味着在 alpha_presence 为正的情况下，j 这个 token 的 logit 会因为之前文本中生成过 j 而有所降低。logit 的降低也意味着被 sample 的概率降低。因此通过提供正值 presence_penalty，就会使模型生成重复 token 的概率降低，换言之，进行了惩罚。如果 alpha_presence 为负值，那么同理可得，会对模型生成重复 token 的行为进行奖励。\npresence_penalty 名字中虽然带着 penalty，但由于其取值范围可能是正数也可能是负数，因此并不一定是惩罚 token 的反复出现，也有可能是鼓励反复出现。\nfrequency_penalty frequency_penalty 参数的官方定义如下：\nfrequency_penalty number Optional Defaults to 0\nNumber between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model\u0026rsquo;s likelihood to repeat the same line verbatim.\n这一部分在 MinGPT 中没有对应的步骤。\n这个参数和 presence_penalty 高度相似。同样在 https://platform.openai.com/docs/api-reference/parameter-details 中有详细的解释。\n具体来说，token j 的 logit 值为 mu[j] ，在加入 frequency_penalty 会修订成 mu[j] -\u0026gt; mu[j] - c[j] * alpha_frequency 。其中 c[j] 是当前已经生成的文本中，出现过多少个 j 这个 token。而 alpha_frequency 就是 frequency_penalty 。这就意味着在 frequency_penalty 为正的情况下，j 这个 token 的 logit 会因为之前文本中生成过 j 而有所降低，而且之前生成过的 j 越多（也就是c[j] 数值越大），惩罚越严重。这里可以看出 frequency_penalty 和 presence_penalty 的不同点在于 frequency_penalty 的惩罚会随着 token 出现的次数增加而不断加强，而 presence_penalty 则只会区分是否出现，这样的区别充分体现在了其名字差异上：frequency 和 presence。\n和 presence 类似，frequency_penalty 的取值可正可负，从而实现惩罚或者奖励反复出现的 token。\nlogit_bias logit_bias 参数的官方定义如下：\nlogit_bias map Optional Defaults to null\nModify the likelihood of specified tokens appearing in the completion.\nAccepts a json object that maps tokens (specified by their token ID in the tokenizer) to an associated bias value from -100 to 100. Mathematically, the bias is added to the logits generated by the model prior to sampling. The exact effect will vary per model, but values between -1 and 1 should decrease or increase likelihood of selection; values like -100 or 100 should result in a ban or exclusive selection of the relevant token.\n这一部分在 MinGPT 中没有对应的步骤。\n这一参数用于无条件的修改某个或者多个 token 的 logit，从而增加或者减少其出现的可能性。具体来说，对于变量 token j ，其 logit 值为 mu[j] ，那么在使用 logit_bias 后，其值将会被修改成：mu[j] -\u0026gt; mu[j] + logit_bias[j] .\n","permalink":"https://blog.xiaoquankong.ai/zh/posts/the-decoding-process-of-chatgpt-and-the-various-parameters-in-it/","summary":"TL:DR OpenAI 的 ChatGPT 在其官方文档（https://platform.openai.com/docs/api-reference/chat/create）中给出了各种参数的范围和含义。我们将讨论 ChatGPT 的生成过程和这些参数是如何实现其生成的效果的。\nChatGPT 的解码过程 我们假设 minGPT （等同于 GPT-2） 和 ChatGPT 拥有一样的解码过程：https://github.com/karpathy/minGPT/blob/master/mingpt/model.py#LL283C12-L283C12 。\n总体过程可以概括为以下几个步骤：\n将用户的请求，从 1 个扩充成 num_samples 大小的 batch 进行模型推理，得到 logits 进行 temperature 映射：logits = logits / temperature [可选] 进行 topk 处理：logits = topk_func(logits, top_k) logits 到 概率的转换：probs = softmax(logits) 是否 sample： 进行 sample：idx_next = multinomial_sample(probs, num_samples=1) 不进行 sample：idx_next = topk_func(probs, k=1) 重复上述过程 max_new_tokens 次 ChatGPT 的解码参数 temperature temperature 参数的官方定义如下：\ntemperature number Optional Defaults to 1","title":"ChatGPT 的解码过程和其中的各种参数"},{"content":"本文将介绍如何使用 Rasa NLU 和 Rasa Core 来构建一个简单的带 Web UI 界面的中文天气情况问询机器人(chatbot)。\n源代码地址 https://github.com/howl-anderson/WeatherBot\n功能 这个机器人可以根据你提供的城市（北京、上海等）和日期（明天、后天等），查询出相应的天气预报。\n功能截图 特性 使用 Frame-based 对话管理方案，如果上述两个 Slot (既城市和天气)，有任意一个用户未提供，对话管理系统会负责让你澄清相关 Slot 的值。\n能力范围 受限于天气数据提供方的能力，这个机器人只能查询 中国大陆地区市级城市 三天以内 （今天，明天，后天） 的气象数据，不能查询过去（昨天，前天）等历史数据。 受限于开发时间，这个机器人 不提供 诸如 这个星期五、下个星期一 这种需要计算才能得到日期给定方式。也 不能提供 诸如 绝对日期：三月一号、六一儿童节日 这种日期的查询能力。 因为使用的是免费的天气查询接口，所以 会有配额限制，可能会因为 超出调用次数 ，而在一个小时内不能用。同时网络查询接口可能存在不稳定因素，导致 没有结果返回或者出现异常，尝试多次重新发送请求可解决问题。 在线演示 Demo for 天气预报查询机器人\nRasa NLU Rasa NLU 提供了提取用户意图和词槽的功能。具体原理和使用等不在这里详述，请访问文章 TODO。 这里我们使用的 Rasa NLU 的 pipeline 配置（在项目文件 nlu_model_config.yaml 中）如下:\nlanguage: \u0026#34;zh\u0026#34; pipeline: - name: \u0026#34;nlp_mitie\u0026#34; model: \u0026#34;data/total_word_feature_extractor.dat\u0026#34; - name: \u0026#34;tokenizer_jieba\u0026#34; - name: \u0026#34;ner_mitie\u0026#34; - name: \u0026#34;ner_synonyms\u0026#34; - name: \u0026#34;intent_featurizer_mitie\u0026#34; - name: \u0026#34;intent_classifier_sklearn\u0026#34; 所用的训练数据 （在项目文件 nlu.json 中）如下（内容过长，已做截断）：\n{ \u0026#34;rasa_nlu_data\u0026#34;: { \u0026#34;common_examples\u0026#34;: [ { \u0026#34;intent\u0026#34;: \u0026#34;weather_address_date-time\u0026#34;, \u0026#34;entities\u0026#34;: [ { \u0026#34;start\u0026#34;: 2, \u0026#34;end\u0026#34;: 4, \u0026#34;value\u0026#34;: \u0026#34;上海\u0026#34;, \u0026#34;entity\u0026#34;: \u0026#34;address\u0026#34; }, { \u0026#34;start\u0026#34;: 4, \u0026#34;end\u0026#34;: 6, \u0026#34;value\u0026#34;: \u0026#34;明天\u0026#34;, \u0026#34;entity\u0026#34;: \u0026#34;date-time\u0026#34; } ], \u0026#34;text\u0026#34;: \u0026#34;我要上海明天的天气\u0026#34; }, { \u0026#34;intent\u0026#34;: \u0026#34;weather_address_date-time\u0026#34;, \u0026#34;entities\u0026#34;: [ { \u0026#34;start\u0026#34;: 0, \u0026#34;end\u0026#34;: 2, \u0026#34;value\u0026#34;: \u0026#34;上海\u0026#34;, \u0026#34;entity\u0026#34;: \u0026#34;address\u0026#34; }, { \u0026#34;start\u0026#34;: 2, \u0026#34;end\u0026#34;: 4, \u0026#34;value\u0026#34;: \u0026#34;明天\u0026#34;, \u0026#34;entity\u0026#34;: \u0026#34;date-time\u0026#34; } ], \u0026#34;text\u0026#34;: \u0026#34;上海明天的天气\u0026#34; }, ... } } 训练所用的命令为 (在项目文件 train_NLU.bash 中):\npython -m rasa_nlu.train -c nlu_model_config.yaml -d nlu.json --fixed_model_name current -o models Rasa Core Rasa Core 负责对话管理。具体原理和使用等不在这里详述，请访问文章 TODO。 Rasa core 需要提供一个 domain 文件，里面设定了整个对话机器人的小宇宙，它能知道的所有的意图、词槽和动作。\n本项目所用的 domain (在项目文件 domain.yml 中)，其内容为：\nintents: - greet - goodbye - weather_address_date-time - weather_address - weather_date-time slots: address: type: text date-time: type: text matches: type: unfeaturized entities: - address - date-time actions: - utter_greet - utter_ask_address - utter_ask_date-time - utter_working_on_it - bot.ActionReportWeather - utter_report_weather - utter_goodbye templates: utter_greet: - text: \u0026#34;你好，请说出需要提供天气预测服务的地点和时间\u0026#34; utter_working_on_it: - text: \u0026#34;正在查询中，请稍后 ...\u0026#34; utter_goodbye: - text: \u0026#34;再见！\u0026#34; utter_ask_address: - text: \u0026#34;哪里呢？\u0026#34; utter_ask_date-time: - text: \u0026#34;什么时候？\u0026#34; utter_report_weather: - text: \u0026#34;{matches}\u0026#34; utter_other: - text: \u0026#34;系统不明白您说的话\u0026#34; Rasa Core 还需要通过 故事(story) 的形式让框架学习正确的对话管理样本，格式为 Markdown 格式。本项目中的 Story （在项目文件 stories.md 中）定义如下（内容过长，已做截断）：\n## simple path with greet * greet - utter_greet * weather_address_date-time{\u0026#34;address\u0026#34;: \u0026#34;上海\u0026#34;, \u0026#34;date-time\u0026#34;: \u0026#34;明天\u0026#34;} - utter_working_on_it - action_report_weather - utter_report_weather ## simple path * weather_address_date-time{\u0026#34;address\u0026#34;: \u0026#34;上海\u0026#34;, \u0026#34;date-time\u0026#34;: \u0026#34;明天\u0026#34;} - utter_working_on_it - action_report_weather - utter_report_weather ## address + date-time path with greet * greet - utter_greet * weather_address{\u0026#34;address\u0026#34;: \u0026#34;上海\u0026#34;} - utter_ask_date-time * weather_date-time{\u0026#34;date-time\u0026#34;: \u0026#34;明天\u0026#34;} - utter_working_on_it - action_report_weather - utter_report_weather ... 训练所用的命令为 (在项目文件 train_CORE.bash 中):\npython -m rasa_core.train -s stories.md -d domain.yml -o models/dialogue --epochs 500 依赖 python 版本 python 3\npython 依赖 pip install -r requirements.txt 下载数据和模型 data/total_word_feature_extractor.dat: 从 https://github.com/howl-anderson/MITIE_Chinese_Wikipedia_corpus 下载，解压缩后放置到对应位置 models/default/current：通过运行 train_NLU.bash 生成 models/dialogue：通过运行 train_CORE.bash 生成 功能入口 申请 API key 本项目目前使用 心知天气 提供天气数据，该平台为个人提供免费的 API，但任然需要用户注册并申请 API key 才能使用。用户注册后可以自行找到 我的API密钥。\n启动服务 将如下的 xxx 替换成你的 API key，然后执行即可\nSENIVERSE_KEY=xxx python ./webchat.py 启动成功后，请用浏览器访问 http://localhost:5500 , 你将得到 web 页面，have fun!\n","permalink":"https://blog.xiaoquankong.ai/zh/posts/creating-a-weather-query-bot-using-rasa/","summary":"\u003cp\u003e本文将介绍如何使用 Rasa NLU 和 Rasa Core 来构建一个简单的带 Web UI 界面的中文天气情况问询机器人(chatbot)。\u003c/p\u003e","title":"使用 Rasa 构建天气查询机器人"},{"content":"本文将详细介绍 Rasa NLU 的 pipeline 和 component，介绍其原理和如何使用。\n兼容性说明：\n本文介绍的 Rasa NLU 知识都是基于 v0.13.2, 不同版本之间可能会有差异，不过不用担心，根据官方消息，Rasa NLU 已经进入比较成熟的阶段，后续出现较大结构变动的可能性不大，因此本文介绍的知识在后续版本，甚至 v1.x 依然有效。\nRasa NLU 是一个基于 pipeline 的通用框架。这样可以获得最大的灵活性。\npipeline 定义了各个组件之间数据的前后流动关系，组件之间是存在依赖关系的，任意一个组件的依赖需求没有被满足都将导致 pipeline 出错（Rasa NLU 会在启动的时候检查是否每一个组件的依赖都被满足，如果没有满足，则终止运行并给出相关的提示消息）。具有以下特征：\n组件之间的顺序关系至关重要，比如 NER 组件需要前面的组件提供分词结果才能正常工作，那么前面的组件中有必须有一个分词器。 组件是可以相互替换的，比如同样是提供分词结果，同时有几个 component 可以选择，比如中文的可以选择清华的分词器、北大的分词器的。 有些组件是互斥的，比如：分词器是互斥的，分词结果不能同时由两个组件提供，否则会出现混乱。 有些组件是可以同时使用的，比如：提取文本特征的组件可以同时使用基于规则的和基于文本嵌入向量的。 一个 NLU 应用通常包括 命名实体识别 和 意图识别 两个任务。为了完成这些任务，一个 典型 的 Rasa NLU pipeline 通常具有以下的 pattern:\n初始化类组件：为了加载模型文件，为后续的组件提供框架支持，如初始化 SpaCy 和 MITIE 分词组件：将文本分割成词语序列，为后续的高级 NLP 任务提供基础数据 提取特征：提取词语序列的文本特征，通常采用 Word Embedding 的方式，提取特征的组件可以同时使用，同时搭配的还可能有基于正则表达式的提取特征的方法。 NER 组件：根据前面提供的特征对文本进行命名实体的识别 意图分类：按照语义对文本进行意图的分类，也称意图识别 初始化组件 初始化组件提供加载模型文件为后续组件所用的组件提供初始化。目前只有两个初始化组件：nlp_spacy 和 nlp_mitie，分别对应 SpaCy（https://spacy.io/） 和 MITIE（https://github.com/mit-nlp/MITIE） 框架。\n基于 MITIE 的组件，如： tokenizer_mitie、intent_featurizer_mitie、ner_mitie 和 intent_classifier_mitie 都将依赖 nlp_mitie 提供的对象。\n基于 SpaCy 的组件，如：tokenizer_spacy、intent_featurizer_spacy 和 ner_spacy 都将依赖 nlp_spacy 提供的对象。\n分词组件 什么是分词？ 自然语言处理在通常情况下都需要进行进行分词操作，那么什么是词，为什么要分词呢？\n按照维基百科（https://zh.wikipedia.org/wiki/词）的定义：\n在语言学中，词（英语：word），又称为单词，是能独立运用并含有语义内容或语用内容（即具有表面含义或实际含义）的最小单位。\n很多通用的 NLP 算法、语法语义分析和 End-to-End 应用都是以词作为基本输入单元。在自然语言处理的任务中，把连续的字（英语：character）分隔成更具有语言语义学上意义的词（英语：word）。这个过程就叫做分词（英语：tokenize /segment）。\n举例来说：\n王小明在北京的清华大学读书。\n可以被分词成\n王小明 在 北京 的 清华大学 读书 。\nRasa 分词组件 Rasa 分词组件中，目前直接支持中文的组件是 tokenizer_jieba 使用基于 MIT 开源协议的流行中文分词器 jieba (https://github.com/fxsjy/jieba) 作为底层引擎，经过改造可以支持中文分词的组件是 tokenizer_mitie，暂不支持中文分词但未来会支持中文分词的组件是 tokenizer_spacy。想用其他的分词器？当然没问题，因为 Rasa NLU 采用 pipeline 机制，扩展起来非常容易，你只需要自己实现一个分词组件就可以了，后面的章节我将演示如何自定义自己的中文分词器，本章节将不研究如何实现自己的组件。\n提取特征 无论是命名实体识别还是意图分类，都需要上游的组件提供特征。常见的特征选择为：词向量、Bag-of-words 和 N-grams 等。用户可以选择同时使用任意的上述组件提取特征，这些组件在实现层面上做了合并特性的操作，因此可以任意和和提取特征的组件一起使用。下面逐一介绍各个组件。\n词向量特征 TODO\nBag-of-words TODO\nN-grams TODO\n正则表达式特征 TODO\nNER SpaCy 支持多种 NER 组件：ner_crf 、ner_mitie 、ner_spacy 、ner_duckling 、ner_duckling_http 和 ner_synonyms。\nner_crf 这个组件如其名，使用 CRF 模型来做 ENR, CRF 模型只依赖 tokens 本身，如果想在 feature function 中使用 POS 特性 那么则需要 nlp_spacy 组件提供 spacy_doc 对象来提供 POS 信息。关于 CRF 模型的原理和使用，请移步章节 TODO\nner_mitie 利用 MITIE 模型提供的 language model，只需要 tokens 就可以进行 NER。TODO: 具体原理待研究\nner_spacy 利用 SpaCy 模型自带的 NER 功能，模型的训练需要在 SpaCy 框架下进行，当前 SpaCy 模型不支持用户训练自己的模型，而 SpaCy 官方的模型只支持常见的几种实体，具体情况见官方文档。\nner_duckling 和 ner_duckling_http Duckling 是 Facebook 出品的一款用 Haskell 语言写成的 NER 库，基于规则和模型。Duckling 支持多种实体的提取，如下表（TODO: 标注 Duckling 的版本）：\nDimension Example input Example value output AmountOfMoney \u0026ldquo;42€\u0026rdquo; {\u0026quot;value\u0026quot;:42,\u0026quot;type\u0026quot;:\u0026quot;value\u0026quot;,\u0026quot;unit\u0026quot;:\u0026quot;EUR\u0026quot;} Distance \u0026ldquo;6 miles\u0026rdquo; {\u0026quot;value\u0026quot;:6,\u0026quot;type\u0026quot;:\u0026quot;value\u0026quot;,\u0026quot;unit\u0026quot;:\u0026quot;mile\u0026quot;} Duration \u0026ldquo;3 mins\u0026rdquo; {\u0026quot;value\u0026quot;:3,\u0026quot;minute\u0026quot;:3,\u0026quot;unit\u0026quot;:\u0026quot;minute\u0026quot;,\u0026quot;normalized\u0026quot;:{\u0026quot;value\u0026quot;:180,\u0026quot;unit\u0026quot;:\u0026quot;second\u0026quot;}} Email \u0026ldquo;duckling-team@fb.com\u0026rdquo; {\u0026quot;value\u0026quot;:\u0026quot;duckling-team@fb.com\u0026quot;} Numeral \u0026ldquo;eighty eight\u0026rdquo; {\u0026quot;value\u0026quot;:88,\u0026quot;type\u0026quot;:\u0026quot;value\u0026quot;} Ordinal \u0026ldquo;33rd\u0026rdquo; {\u0026quot;value\u0026quot;:33,\u0026quot;type\u0026quot;:\u0026quot;value\u0026quot;} PhoneNumber \u0026ldquo;+1 (650) 123-4567\u0026rdquo; {\u0026quot;value\u0026quot;:\u0026quot;(+1) 6501234567\u0026quot;} Quantity \u0026ldquo;3 cups of sugar\u0026rdquo; {\u0026quot;value\u0026quot;:3,\u0026quot;type\u0026quot;:\u0026quot;value\u0026quot;,\u0026quot;product\u0026quot;:\u0026quot;sugar\u0026quot;,\u0026quot;unit\u0026quot;:\u0026quot;cup\u0026quot;} Temperature \u0026ldquo;80F\u0026rdquo; {\u0026quot;value\u0026quot;:80,\u0026quot;type\u0026quot;:\u0026quot;value\u0026quot;,\u0026quot;unit\u0026quot;:\u0026quot;fahrenheit\u0026quot;} Time \u0026ldquo;today at 9am\u0026rdquo; {\u0026quot;values\u0026quot;:[{\u0026quot;value\u0026quot;:\u0026quot;2016-12-14T09:00:00.000-08:00\u0026quot;,\u0026quot;grain\u0026quot;:\u0026quot;hour\u0026quot;,\u0026quot;type\u0026quot;:\u0026quot;value\u0026quot;}],\u0026quot;value\u0026quot;:\u0026quot;2016-12-14T09:00:00.000-08:00\u0026quot;,\u0026quot;grain\u0026quot;:\u0026quot;hour\u0026quot;,\u0026quot;type\u0026quot;:\u0026quot;value\u0026quot;} Url \u0026ldquo;https://api.wit.ai/message?q=hi\u0026quot; {\u0026quot;value\u0026quot;:\u0026quot;https://api.wit.ai/message?q=hi\u0026quot;,\u0026quot;domain\u0026quot;:\u0026quot;api.wit.ai\u0026quot;} Volume \u0026ldquo;4 gallons\u0026rdquo; {\u0026quot;value\u0026quot;:4,\u0026quot;type\u0026quot;:\u0026quot;value\u0026quot;,\u0026quot;unit\u0026quot;:\u0026quot;gallon\u0026quot;} TODO: 考虑翻译上表为中文\n这里需要提醒的是 Duckling 对中文的支持并不是很全面，只支持上面诸多实体类型中的几种。\n在 Rssa 中有两种方式去调用 Duckling ，一种是通过 duckling 这个包使用 wrap 的方式访问，另一种是通过 HTTP 访问。上述两种访问方式分别对应 ner_duckling 和 ner_duckling_http 这两个组件。上述两种组件如何起来并不困难，具体请查阅官方文档。\nner_synonyms 正确来说 ner_synonyms 不是一个命名实体的提取组件，更像是一个归一化的组件。ner_synonyms 主要是讲各种同义词（synonyms）映射成为标准词汇，比如将实体 KFC 的值改写成 肯德基，这种归一化的操作为后续业务处理提供便利。\n意图分类 意图识别也称意图分类，Rasa 中的内建组件有 intent_classifier_mitie、intent_classifier_sklearn、intent_classifier_tensorflow_embedding 和 intent_classifier_keyword。\nintent_classifier_mitie TODO\nintent_classifier_sklearn TODO\nintent_classifier_tensorflow_embedding TODO\nintent_classifer_keyword TODO\n结构化输出 Rasa NLU 通过结构化输出组件将结果输出，在Rasa NLU 中结构化输出组件是框架提供的，不属于 Pipeline 的可变动部分，因此也不需要用户去配置（也无法直接配置）。\nTODO：解释输出的结构，包括组件不同的情况下，可能的结果\n配置 Pipeline Rasa NLU 的配置文件使用的是 YAML (YAML Ain\u0026rsquo;t Markup Language) 格式。下面两个是Rasa NLU 配置的文件的样例。\nlanguage: \u0026#34;en\u0026#34; pipeline: - name: \u0026#34;nlp_mitie\u0026#34; model: \u0026#34;data/total_word_feature_extractor.dat\u0026#34; - name: \u0026#34;tokenizer_mitie\u0026#34; - name: \u0026#34;ner_mitie\u0026#34; - name: \u0026#34;ner_synonyms\u0026#34; - name: \u0026#34;intent_entity_featurizer_regex\u0026#34; - name: \u0026#34;intent_classifier_mitie\u0026#34; 大体上 Rasa NLU 的配置文件可以分为两个主要的 Key：language 和 pipeline\nlanguage 用于指定 Rasa NLU 将要处理的语言。因为某些种类的组件，比如分词组件，是对语言敏感的。比如说 jieba 分词就不能正确的处理日文的分词，反之亦然。所有的 Rasa NLU 组件都有一个语言兼容性列表。如果某个组件不支持当前设置的语言，则会在 Pipeline 启动前被框架检测到。另外这种语言信息也可以被其他组件作为配置变量，比如在使用 SpaCy 的时候，默认就会载入和 language 同名的语言模型。如果省略该字段，则默认为 en。\npipeline 是配置文件的核心，pipeline 由列表构成（表现在 YAML 中 就是使用 - 开头），列表的每一个元素都是一个字典(表现在 YAML 中类似于 name: xxx)，这些字典直接对应着 pipeline 的组件。每个组件具体是什么都由字典的 name 键来指定，出现在字典中的其他的键都是对这个组件的配置，运行时将传递给各个组件，具体有什么键和什么意义都由各个组件自行定义。\n在上例中，共有组件 6 个，分别是 nlp_mitie、 tokenizer_mitie、 ner_mitie、 ner_synonyms 、intent_entity_featurizer_regex 和 intent_classifier_mitie。其中 nlp_mitie 组件拥有一个配置项：键（key）为 model，值（value）为 data/total_word_feature_extractor.dat，这个配置项指定了 MITIE 模型文件所在是位置。\n为了最大化的方便用户，Rasa NLU 的配置还可以采用预定义的 pipeline 的方式，如下\npipeline: tensorflow_embedding 直接给 pipeline 赋值一个字符串，这个字符串代表了预定义的 pipeline，在上例中预定义的 pipeline 名为 tensorflow_embedding。\n预定义 Pipeline Rasa NLU 预定义了几个常用的 pipeline\ntensorflow_embedding TODO\nspacy_sklearn TODO\n","permalink":"https://blog.xiaoquankong.ai/zh/posts/pipeline-and-components-in-rasa-nlu/","summary":"\u003cp\u003e本文将详细介绍 Rasa NLU 的 pipeline 和 component，介绍其原理和如何使用。\u003c/p\u003e","title":"Rasa NLU 的 pipeline 和 component"},{"content":"利用 隐马尔科夫模型（HMM） 的解码能力，能从一个观察序列（字符串序列）解码成另一个隐藏状态序列（分词符号序列）。\n解释 将训练数据按照 BMES 标记集合转换成隐藏状态序列。一般情况下使用 BMES 标记体系作为隐藏序列的体系。\nBMES 标记体系 BMES 分别是 Begin / Middle / End / Single 的缩写，分别代表着所标记的字符是一个词语的 开始字符 / 中间字符 / 结尾字符 / 单个字符（也就是说这个词只由一个字符构成）。\n举例来说：\n我 这个词对应的 BMES 标记为 S 我们 这个词对应的 BMES 标记为 BE 老爷爷 这个词对应的 BMES 标记为 BME 风调雨顺 这个词对应的 BMES 标记为 BMME 得到训练数据后，可以统计出 HMM 的两个参数表。得到的 HMM 参数后可以利用 Viterbi 算法解码出隐藏状态序列，再按照隐藏状态序列反向将字符串分割成词语\n示例 以 我们在野生动物园玩 为例, 假设 HMM 解码得到的隐藏状态序列为 BESBMMMES\n则会被分成 我们(BE) / 在(S) / 野生动园(BMMME) / 玩(S)\n优化方案 为了增加处理 OOV 的能力，对于没有出现在训练数据集的字符，每个隐藏状态都有一个非常小的 Emission Probability, 这样可以增加模型的鲁棒性。\n\u0008关联算法 CRF 的基本想法与此算法类似，但性能更加强劲。\n参考文献 Part-of-Speech Tagging in Speech and Language Processing (Draft of August 12, 2018.) ","permalink":"https://blog.xiaoquankong.ai/zh/posts/implementing-a-hmm-based-chinese-tokenizer/","summary":"\u003cp\u003e利用 隐马尔科夫模型（HMM） 的解码能力，能从一个观察序列（字符串序列）解码成另一个隐藏状态序列（分词符号序列）。\u003c/p\u003e","title":"构建中文分词器 - 隐马尔科夫模型"},{"content":"结合 正向最大匹配法 和 反向最大匹配法 的优点，按照一定的规则选择其中表现最优秀的结果作为 双向最大匹配法 的结果。\n解释 从 正向最大匹配法 和 反向最大匹配法 的结果中选择最满足中文分词原则的一个分词结果。\n中文分词原则 分词粒度以大为最佳（最大化平均词语长度） 分词的结果颗粒度（单个词所包含的字符长度）越大越好。\n同样是 北京大学 可以分成 北京 / 大学 或者 北京大学，则后者更优，后者包含的信息更加特定和明确。\n非词典词越少越佳，单字字典词数越少越\u0008佳 非词典词的出现说明出现了 OOV (Out Of Vocabulary) 问题，字典足够大的情况下出现 OOV， 说明分词结果不佳。类似的，分词结果大量出现单个字，也是暗示分词效果不佳。\n\u0008比如 技术和服务 可以被分成 技术 / 和 / 服务 或者 技术 / 和服 / 务，后者中的 务 就是一个 OOV，因为中文中 务 不能单独成词\n最小化词语长度的变化率 同样是 研究生命起源 可以\u0008被分成 研究生 / 命 / 起源 和 研究 / 生命 / 起源，后者 词语长度的变化率 最小，因此是更好的分词结果。\n实现 通过分别实现 正向最大匹配法 和 反向最大匹配法 按照上述原则实现一个判别器，判别最优结果，返回即可。\n参考文献 关于MMSEG分词算法 中文分词基础原则及正向最大匹配法、逆向最大匹配法、双向最大匹配法的分析 ","permalink":"https://blog.xiaoquankong.ai/zh/posts/creating-a-chinese-tokenizer-using-the-maximum-bidirectional-matching-method/","summary":"\u003cp\u003e结合 \u003ccode\u003e正向最大匹配法\u003c/code\u003e 和 \u003ccode\u003e反向最大匹配法\u003c/code\u003e 的优点，按照一定的规则选择其中表现最优秀的结果作为 \u003ccode\u003e双向最大匹配法\u003c/code\u003e 的结果。\u003c/p\u003e","title":"构建中文分词器 - 双向最大匹配法"},{"content":"TL;DR 中文文案排版指南\n前言 排版的重要性 中文文案排版是每一个现代人或多或少都要进行的职业活动。但是文案排版不是一个容易的工作，有很多人一直都在使用错误的方式排版文案。排版错误或者不美观的文案将影响文案的宣传和传播效果，尤其是你的申请表、简历、个人介绍等，影响深远！\n符号约定 为了让空格更容易被识别出来，本文余下内容中将使用☐ (U+2610 BALLOT BOX（方格）) 表示空格\n版权相关 本文中的多数内容都是基于参考文献的衍生和再创造，特此申明！\n空格的使用 「為什麼你們就是不能加個空格呢？」\nby https://github.com/vinta/pangu.js\n中英文之間需要增加空格 正确的做法 排版效果：\n在 LeanCloud 上，数据存储是围绕 AVObject 进行的。\n排版方案：\n在☐LeanCloud☐上，数据存储是围绕☐AVObject☐进行的。\n错误的做法 在LeanCloud上，数据存储是围绕AVObject进行的。\n在 LeanCloud上，数据存储是围绕AVObject 进行的。\n例外情况 例如「豆瓣FM」之类的产品名称等专有名词，按照产品官方定义的格式书写。\n中文与数字之间需要增加空格 正确的做法 今天出去买菜花了 5000 元。\n错误的做法 今天出去买菜花了5000元。\n今天出去买菜花了 5000元。\n数字和单位之间需要增加空格 NOTE： 很典型的错误的！:(\n正确的做法 我家的光纤入屋带宽有 10 Gbps，SSD 一共有 20 TB。\n错误的做法 我家的光纤入屋带宽有 10Gbps，SSD 一共有 20TB。\n例外情况 度／百分比與數字之間不需要增加空格\n温度 正确的做法 今天是 233° 的高溫。\n错误的做法 今天是 233 ° 的高溫。\n百分比 正确的做法 新 MacBook Pro 有 15% 的 CPU 性能提升。\n错误的做法 新 MacBook Pro 有 15 % 的 CPU 性能提升。\n全形（全角）標點與其他字符之間不加空格 正确的做法 剛剛買了一部 iPhone，好開心！\n错误的做法 剛剛買了一部 iPhone ，好開心！\n其他非空格相关的排版问题 常见错误类 中文环境使用半角符号是错误的！ 正确的做法 排版效果：\n嗨！你知道嘛？今天前台的小妹跟我說「喵」了哎！\n排版方案：\n嗨！你知道嘛？今天前台的小妹跟我說「喵」了哎！\nNOTE： 上文文字间没有☐，也就是没有空格。\n错误的做法 排版效果：\n嗨! 你知道嘛? 今天前台的小妹跟我說 \u0026ldquo;喵\u0026rdquo; 了哎!\n排版方案：\n嗨!☐你知道嘛?☐今天前台的小妹跟我說☐\u0026quot;喵\u0026quot;☐了哎!\nNOTE： 这种错误还挺常见的！\n專有名詞使用正確的大小寫 正确的做法 我們的客戶有 GitHub、Foursquare、Microsoft Corporation、Google、Facebook, Inc.。\n错误的做法 我們的客戶有 github、foursquare、microsoft corporation、google、facebook, inc.。\nNOTE： 看完之后，多数人都有想立即把首字母改成大写的冲动，真的！\n不要使用不地道的縮寫 正确的做法 我們需要一位熟悉 JavaScript、HTML5，至少理解一种框架（如 Backbone.js、AngularJS、React 等）的前端開發者。\n错误的做法 我們需要一位熟悉 Js、h5，至少理解一种框架（如 backbone、angular、RJS 等）的 FED。\nNOTE： 错误到简直辣眼睛！千万不要出现在你的简历里，真的拜托了🙏！\n略有争议但我（作者个人）觉得正确的常见错误类 链接之间增加空格 正确的做法 访问我们网站的最新动态，请 点击这里 进行订阅！\n错误的做法 访问我们网站的最新动态，请点击这里进行订阅！\nNOTE： 链接文字和普通文字挤在一起的感觉，让人觉得很业余！\n简体中文使用直角引号（这个还真是挺争议的） 正确的做法 「老师，『有条不紊』的『紊』是什么意思？」\nNOTE： 感觉正体很帅，很正式！\n错误的做法 “老师，‘有条不紊’的‘紊’是什么意思？”\n怎么输入直角引号（i.e.「、」、『、』） 见知乎：如何输入直角引号（「」和『』 ）？\n不常见错误类 既然是不常见的错误，那么本文就不再讨论了，读者还是去看附在文末的参考文献吧，里面什么都有！\n「盤古之白」是什么意思？ 所有的中文字和半形的英文、數字、符號之間应该存在的空白，被漢學家稱為「盤古之白」，因為它劈開了全形字和半形字之間的混沌。\n另有研究顯示，打字的時候不喜歡在中文和英文之間加空格的人，感情路都走得很辛苦，有七成的比例會在 34 歲的時候跟自己不愛的人結婚，而其餘三成的人最後只能把遺產留給自己的貓。畢竟愛情跟書寫都需要適時地留白。\n與大家共勉之。\nby https://github.com/vinta/pangu.js\n工具 仓库 语言 vinta/paranoid-auto-spacing JavaScript huei90/pangu.node Node.js huacnlee/auto-correct Ruby sparanoid/space-lover PHP (WordPress) nauxliu/auto-correct PHP ricoa/copywriting-correct PHP hotoo/pangu.vim Vim sparanoid/grunt-auto-spacing Node.js (Grunt) hjiang/scripts/add-space-between-latin-and-cjk Python 谁在这样做？ 网站 文案 UGC Apple 中国 Yes N/A Apple 香港 Yes N/A Apple 台湾 Yes N/A Microsoft 中国 Yes N/A Microsoft 香港 Yes N/A Microsoft 台湾 Yes N/A LeanCloud Yes N/A 知乎 Yes 部分用户达成 V2EX Yes Yes SegmentFault Yes 部分用户达成 Apple4us Yes N/A 豌豆荚 Yes N/A Ruby China Yes 标题达成 PHPHub Yes 标题达成 少数派 Yes N/A blog.xiaoquankong.ai (本站啦) Yes 作者还在努力推进中 :) 参考文献 中文文案排版指北 ","permalink":"https://blog.xiaoquankong.ai/zh/posts/chinese-document-typesetting-specification-spacing-of-pangu/","summary":"\u003cp\u003e\u003cstrong\u003eTL;DR\u003c/strong\u003e 中文文案排版指南\u003c/p\u003e","title":"「盤古之白」"},{"content":"TL;DR 一些关于 Chinese Spelling Check Task 比较重要的会议和资料的整理和汇总。\n重要的相关会议 ACL ACLCLP ACLCLP 是 Association for Computational Linguistics and Chinese Language Processing 的缩写。\n数据集 Chinese Grammatical Error Diagnosis NLPTEA 2016 Shared Task: http://ir.itc.ntnu.edu.tw/lre/nlptea16cged.htm NLPTEA 2015 Shared Task: http://ir.itc.ntnu.edu.tw/lre/nlptea15cged.htm NLPTEA 2014 Shared Task: http://ir.itc.ntnu.edu.tw/lre/nlptea14cfl.htm\nChinese Spelling Check SIGHAN 2015 Bake-off: http://ir.itc.ntnu.edu.tw/lre/sighan8csc.html CLP 2014 Bake-off: http://ir.itc.ntnu.edu.tw/lre/clp14csc.html SIGHAN 2013 Bake-off: http://ir.itc.ntnu.edu.tw/lre/sighan7csc.html\n","permalink":"https://blog.xiaoquankong.ai/zh/posts/chinese-spelling-check-task-related-materials/","summary":"\u003cp\u003e\u003cstrong\u003eTL;DR\u003c/strong\u003e 一些关于 Chinese Spelling Check Task 比较重要的会议和资料的整理和汇总。\u003c/p\u003e","title":"Chinese Spelling Check Task: 资料汇总"},{"content":" TL;DR 在即将发布（本文章写于 2018-01-06）的 TenserFlow v1.5 中，TensorFlow将会引入一个重要的 User-friendly 特性：Eager Execution. 本文章将展示 Eager Execution 引入的一些新的特性。\n安装 TensorFlow 对应的版本 因为 TensorFlow 正式版（写作时间 2018-01-06，此时的正式版本为1.4.1）中还不包含此功能，因此我们需要安装 TensorFlow nightly build 版本。\npip install tf-nightly # or tf-nightly-gpu if you have GPU 特性探索 Eager execution 在开启这个模式后，TensorFlow 将会立即执行操作，返回结果给 Python，而不需要使用 Session.run(), 例如：\nimport tensorflow as tf import tensorflow.contrib.eager as tfe tfe.enable_eager_execution() x = [[2]] m = tf.matmul(x, x) print(m) 点击这里 launch binder ，在线运行这个例子\n你会得到如下显示：\ntf.Tensor([[4]], shape=(1, 1), dtype=int32) Dynamic models 在不具备动态模型的能力前，TensorFlow 中的每一个 operator 都需要明确定声明和定义。在具备的了动态模型能力之后，TensorFlow 具备了从操作中推导操作数类型的能力，让复杂的动态模型容易实现，例如：\nimport tensorflow as tf import tensorflow.contrib.eager as tfe tfe.enable_eager_execution() a = tf.constant(12) counter = 0 while not tf.equal(a, 1): if tf.equal(a % 2, 0): a = a / 2 else: a = 3 * a + 1 print(a) 点击这里 launch binder ，在线运行这个例子\n如果没有启用 Eager Execution 会显示如下错误：\nUsing a tf.Tensor as a Python bool is not allowed.\n原因是在 while not tf.equal(a, 1) 处，如果没有启动 Eager Execution 那么返回的结果是 tf.Tensor 对象，因为还不知道具体的值所以不能转换成bool类型。\nGradients 得益于 Eager Execution 立即执行的特性，Gradients 也可以立即得到，而不用等到运行时才能知道，例子如下：\nimport tensorflow as tf import tensorflow.contrib.eager as tfe tfe.enable_eager_execution() def square(x): return tf.multiply(x, x) grad = tfe.gradients_function(square) print(square(3.)) # 输出 [9.] print(grad(3.)) # 输出 [6.] 点击这里 launch binder ，在线运行这个例子\n输入的具体情况如下：\ntf.Tensor(9.0, shape=(), dtype=float32) [\u0026lt;tf.Tensor: id=11, shape=(), dtype=float32, numpy=6.0\u0026gt;] Building models 官方推荐应该使用 Python 的 class 来组织模型结构而不是 function。Eager Execution 带有的 tfe.Network 就是设计用来作为模型的父类的，继承这个类之后就支持网络的套嵌，下面这段代码是官方推荐的简易 MNIST 模型的参考：\nclass MNISTModel(tfe.Network): def __init__(self): super(MNISTModel, self).__init__() self.layer1 = self.track_layer(tf.layers.Dense(units=10)) self.layer2 = self.track_layer(tf.layers.Dense(units=10)) def call(self, input): \u0026#34;\u0026#34;\u0026#34;Actually runs the model.\u0026#34;\u0026#34;\u0026#34; result = self.layer1(input) result = self.layer2(result) return result 即使没有训练，我们也能够立即调用它并观察输出：\n# Let\u0026#39;s make up a blank input image model = MNISTModel() batch = tf.zeros([1, 1, 784]) print(batch.shape) # (1, 1, 784) result = model(batch) print(result) # tf.Tensor([[[ 0. 0., ...., 0.]]], shape=(1, 1, 10), dtype=float32) 这里并不需要使用 placeholders 或者 sessions。当我们第一输入时，模型的参数会被设定好。\n为了训练任何模型，我们都需要 loss function，calculate gradients 和 optimizer 去优化参数。 loss function\ndef loss_function(model, x, y): y_ = model(x) return tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=y_) calculate gradients \u0026amp; optimizer\noptimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001) for (x, y) in tfe.Iterator(dataset): grads = tfe.implicit_gradients(loss_function)(model, x, y) optimizer.apply_gradients(grads) 点击这里 launch binder ，在线运行这个例子\n其他特性 还有其他特性，如：\nget the second derivative derivative under control flow Custom Gradients 这里就不再介绍，感兴趣的可以参考官方文档或者本文的参考文档。\n参考文档 Google Research Blog \u0026gt; Eager Execution: An imperative, define-by-run interface to TensorFlow ","permalink":"https://blog.xiaoquankong.ai/zh/posts/introduce-to-eager-execution-of-tensorflow/","summary":"\u003c!-- TODO: https://github.com/hexojs/hexo/issues/2150 caused can not use GitHub badge --\u003e\n\u003cp\u003e\u003cstrong\u003eTL;DR\u003c/strong\u003e 在即将发布（本文章写于 2018-01-06）的 TenserFlow v1.5 中，TensorFlow将会引入一个重要的 User-friendly 特性：Eager Execution. 本文章将展示 Eager Execution 引入的一些新的特性。\u003c/p\u003e","title":"TenserFlow 新特性：Eager Execution"},{"content":"TL;DR 图形游戏（比如坦克大战）如果要实现智能Agent（AKA 电脑玩家）的话，目前最佳的方案就是Reinforcement Learning (简称 RL ;中文：增强学习)。 本文记录了我和Reinforcement Learning的第一次交手，将带你了解这位名扬四海却又神秘莫测的对手。:)\n背景 公司举办Hackthon（中国：黑客松 \u0026lt;- 来自维基百科的翻译），其中有一道题是实现坦克大战的玩家程序。在别人普遍使用人工策略的情况下，考虑到我们团队人少（精确的讲只有我一人会写程序，其他团队起码三个程序员），所以在人工策略这条路上肯定是非常的吃亏，所以不如另辟蹊径，拼搏一把（反正也基本出不了初赛了)。于是我们选择了让机器学习的方案。于是 就成了目标。在此之前，我也只是听过RL的大名，现在要使用RL，内心是既兴奋又紧张。\nReinforcement Learning 概念 为了更加容易理解，我们将引入坦克大战的例子来辅助讲解。总的来说RL模型将现实世界的问题抽象成两类对象的交互：Enviroment和Agent，对坦克大战而言，游戏就是Enviroment，而游戏玩家就是Agent。\nEnviroment提供observation：这是Agent对外界环境的观察，坦克大战中，游戏的图形界面就是Agent(游戏玩家)对Enviroment（游戏）的observation。Agent使用自己的逻辑，根据对observation的理解，给出一个action：这表示对外界环境的一个操作或者反馈，坦克大战中游戏玩家的操作就是action。action在每个step中会被发送给Enviroment,Environment则会返回新的observation和reward。reward表示的是当前情况下Environment对aciton的反馈：数值可能正数也可能负数也可能是零，坦克大战中坦克被击毁、击毁敌方坦克、获取装备或者旗帜等直接得分或者失分都算是reward。然后Agent根据新的observation给出新的action,如此循环往复。聪明的算法能在observation、action和reward中发现关系，使得每一次给出的action都能得到最大期望的reward。\n特点 与Supervised Learning不同的是，Reinforcement Learning要解决的问题是存在reward delay现象的，也就是说Reinforcement Learning会考虑全局最优，而不是当前这一步是最优的，避免“赢了战役，输了战争”这种现象。坦克大战中的例子就是高级玩家可能会选择一种策略：即使不停的被攻击，他的坦克不选择躲避炮弹而是选择承受炮火的同时持续不断的攻击你的基地的外墙。只看一步操作而言，这样的行动是失败的，因为己方的某一辆坦克被击毁，但从长远的角度来看，你能在最后一辆己方坦克被击毁前成功的击毁对方的基地。\nQ-learning 算法 我们将介绍Reinforcement Learning中比较容易理解的算法：Q-learning\nQ-learning 简介 TODO\n实验环境 我们将引入一个简单的环境：一个房子。这个房子由5个房间构成（编号：0 - 4），连上房子外的空间（编号：5），共六个状态。房间之间与房间和户外空间之间可能存在门，也就是相互联通。如下图所示：\n我们根据房间的联通状态，将上述的物理房间图抽象，将每个房间抽象成一个节点(Node)或者状态（state),房子之间存在联通关系的则用一个有向边表示（因为房门是双向联通，所以每个房门对应两个相向的有向边），如下图所示：\n在这个例子中，我们的目标是将 agent （抽象概念） 从房间里移动到户外空间。Q-learning 的目标是到达 reward 最高的状态，而在本例中，状态 5 就是我们想要的目标状态（也称最终状态），Q-learning 到达目标状态后就会永久留在目标状态，因此我们给状态 5 增加一个指向自己的有向边（如上图示）。这种目标或者状态称之为 absorbing goal 或者 absorbing state。\n为了让状态 5 成为目标状态，我们将所有指向状态 5 的有向边全部赋值 reward=100 ，除此之外的边全部赋值 reward=0 。如下图所示：\n假设本例子中的 agent 是一个笨笨的虚拟机器人，它会从以前的经验中学习知识，它能够从一个房间到另一个房间，但它不知道房间的情况也不知道从房间到外面空间的路径。\n假设本例子的目标是建立一个模型帮助 agent 从房子中的 任意一个房间 出发到达户外空间。现在我们假设 agent 在房间 2 ，我们想要让它学习如何到达户外空间。如下图所示：\n为了和 Reinforcement Learning 保持一支，我们特别将每个房间 Node 称之状态（state）或者叫做 observation ，而将 agent 的每一次移动称之为 action。observation 在这里有点难以理解，所以这里使用 状态（state）这个 Q-learning 术语。在本例子中，action 使用有向边来表示。如下图所示：\n从状态 2 agent 可以到达状态 3，这是因为状态 2 存在到状态 3 的有向边，也就是存在联通的门。状态 2 不能直接到达状态 1，这是因为状态 2 不存在到达状态 1 的有向边，也就是这两个房间之间不存在门。状态 3 可以到达状态 1 、状态 4 和回到状态 2。agent 在状态 1 和状态 0 的可能到达状态，读者可以自行观察，不再赘述。我们将上述所有可能状态、 action 和 reward 编制成一张表：得到 matrix R。如下图所示。\nNOTE 表中的 -1 表示无效值，也就是这个 action 不存在，比如不存在从状态 0 到状态 1 的 action 或者说门。\n我们将增加一个相似的矩阵 matrix Q ，用于表示 agent 从中学习的知识。matrix Q 的每一个行代表一个前一个状态，每一列表示下一个状态。刚开始时，agent 并没有学习到任何知识，所以 matrix Q 中的值初始化为 0. 在本例子中，我们已经知道所有的状态数为 6， 在现实例子中，这个数可能是未知的，所以初始化的时候可能只有一行一列，当发现新的状态时，Matrix Q 可以增加新的行和列。\nQ-learning 的状态转移规则如下：\n$$ Q(state, action) = R(state, action) + \\gamma * Max[Q(next state, all actions)] $$\n按照这个公式，赋值Q中一个元素的值等于 Matrix R 中的相应的值和 $\\gamma$ (学习参数) 乘以 下一个状态中所有的action的最大的 Q reward。\n我们的 agent 不需要老师就能从经验中学习，因此这个是 非监督学习。每一次 agent 从一个状态转到另一个状态，最终达到目标状态。这样的一次探索过程我们称之为 episode。每一个 episode 包含了 agent 从初始状态到目标状态的所有的 action。每当 agent 到达目标状态时，我们就开始下一个 episode。\nQ-Learning 算法大体运行如下：\nSet the gamma parameter, and environment rewards in matrix R. Initialize matrix Q to zero. For each episode { Select a random initial state. Do While the goal state hasn\u0026#39;t been reached. { Select one among all possible actions for the current state. Using this possible action, consider going to the next state. Get maximum Q value for this next state based on all possible actions. Compute: Q(state, action) = R(state, action) + Gamma * Max[Q(next state, all actions)] Set the next state as the current state. } } ","permalink":"https://blog.xiaoquankong.ai/zh/posts/introduce-to-reinforcement-leanring/","summary":"\u003cp\u003e\u003cstrong\u003eTL;DR\u003c/strong\u003e 图形游戏（比如坦克大战）如果要实现智能Agent（AKA 电脑玩家）的话，目前最佳的方案就是Reinforcement Learning (简称 RL ;中文：增强学习)。 本文记录了我和Reinforcement Learning的第一次交手，将带你了解这位名扬四海却又神秘莫测的对手。:)\u003c/p\u003e","title":"Reinforcement Learning: 初次交手，多多指教"},{"content":"TL;DR 本文将介绍Python bisect模块在某些场景下的妙用，可以高效和优雅的改善原有使用if-else才能解决问题。\n难题：查询整数所属的区间 应用开发过程中，经常出现一种情景，需要你查询一个整数落在哪一个范围内，比如根据消费金额确定优惠金额或者打折力度等。具体的例子有：消费满100元优惠10元，消费满200元优惠25元，等等。\n常规解决方案及其缺点 通常情况下，都是使用switch／if-elif来解决的，范围比较少的情况，代码还属于比较简洁的，当范围的数量增加，代码就变的相当的不简洁了，正如下面的代码：\ndiscount = None if value \u0026lt; 100: discount = 0 elif value \u0026lt; 200: discount = 10 elif values \u0026lt; 300: discount = 25 elif values \u0026lt; 400: discount = 42 elif values \u0026lt; 500: discount = 53 elif values \u0026lt; 600: discount = 64 elif values \u0026lt; 700: discount = 75 elif values \u0026lt; 800: discount = 86 elif values \u0026lt; 900: discount = 97 elif values \u0026lt; 1000: discount = 108 else: discount = 120 基于bisect的方案 bisect介绍 bisect是python的标准模块，是一个关于数组二分查找法的库，里面提供了在这里非常有用的三个函数bisect_left, bisect_right, bisect. 这三个参数都接受一个array和一个数字，返回将数字插入这个array后这个数字的位置（index），但并不真正执行插入操作。比如：\nIn[0]: import bisect In[1]: bisect.bisect([1, 3, 5], 2) Out[1]: 1 表示如果将2插入1 3 5中间，那么插进去之后的index则为返回值（本例，返回值为1），如果出现相同的值，bisect()函数选择将值插在后面也就是原有值的右侧：\nIn[0]: import bisect In[1]: bisect.bisect([1, 3, 5], 3) Out[1]: 2 bisect_left()函数选择将值插在前面也就是原有值的左侧：\nIn[0]: import bisect In[1]: bisect.bisect_left([1, 3, 5], 3) Out[1]: 1 另外bisect_right()函数是bisect()函数的别名，或者反之。\n利用bisect查找整数范围 bisect函数是二分查找，既可以用来插入，当然也可以用来检索信息，比如查找值所属的区段／区间。\n前面我们提到的那个函数就可以利用bisect做改写:\nmapping = { 0: 0, 1:\t10, 2: 25, 3: 42, 4: 53, 5: 64, 6: 75, 7: 86, 8: 97, 9: 108, 10: 120, } i = bisect(range(100, 1001, 100), value) discount = mapping[i] 这种方案在业务方案多变，查询范围特别多的情况下具备极大的可维护性和性能优势。\n","permalink":"https://blog.xiaoquankong.ai/zh/posts/introduce-to-python-bisect-module/","summary":"\u003cp\u003e\u003cstrong\u003eTL;DR\u003c/strong\u003e 本文将介绍Python bisect模块在某些场景下的妙用，可以高效和优雅的改善原有使用if-else才能解决问题。\u003c/p\u003e","title":"Python bisect模块的妙用"},{"content":"TL:DR 本文属于入门级课程101，用图文并茂的方式详细的介绍了LSTM的工作原理。\n神经网络 本文假设你已经了解最基本的神经网络的知识，为了更好的理解本文内容，本文先简单回顾一下神经网络的一些重要的术语和符号，这些符号将在后续的内容中持续使用。\n矩阵表示下的神经网络 假设：\n$x$是input layer的值 $W$是hidden layer的权重 $h$是hidden layer的输出值 $V$是hidden layer到output layer的权重 $y$是output layer的值 $\\phi$是激活函数，常见激活函数与特性请见神经网络里的激活函数, 这里使用$\\sigma$表示sigmoid函数 $[x,y]$表示两个列向量，在列的维度上concatenate 则有：\n$$ h = \\phi (Wx) $$\n$$ y = Vh $$\n下面是大意图：\nTODO: 替换本图\nRNN 应用场景 现实生活中很多事情都是序列的，后面的事情和前面是存在上下文关系的，单从一个片段是无法做出判断的。比如：你向上扔一个苹果，在任意时刻，你只能得到一个苹果的瞬时照片，单从照片你根本没法正确推测这个苹果的运动状态的，成功的推测苹果的运动状态，需要模型具备记忆能力能够记住之前苹果的位置信息。传统的神经网络只能判断一个瞬间状态的情况，不具备这种记忆的能力，因此在很多复杂的场景中无法适用。为此人们提出了RNN（Recurrent Neural Network)通过将上一个场景的信息引入下一个场景的方式来记住重要信息。\nRNN原理 既然上一个场景的hidden layer的值中包含了场景信息，那么理所当然的我们认为hidden layer中包含了有用的上下文知识。所以RNN就是在当前的预测中引入上一个场景的hidden layer值：\n$$ h_t = \\phi (W x_t + U h_{t-1}) $$\n$$ y_t = V h_t $$\n其中：\n$h_t$是$t$时刻的hidden layer的值 $h_{t-1}$是$t-1$时刻（亦即上一个时刻）的hidden layer的值 缺陷 尽管RNN成功记忆了部分上下文信息，但存在一个很大的缺陷，那就是它很难记住长期的记忆。还是上面抛苹果的例子，RNN能够记住短期的上下文，所以在后期它能够识别出苹果在加速下落，但是由于没有记住比较久远的苹果先是上升的这个信息，因此RNN只能识别出这个苹果是下降的。\nLSTM 为了解决这个问题，人们提出了LSTM（Long Short-Term Memory）网络，LSTM最大的特点就是能够记住长期记忆。是目前工业界和学术界最重要的RNN实现。\n工作原理 LSTM有两个重要的state或者memory：\n长期记忆 (long-term memory: $lsm$, 通常被称为cell state, 标识为$C$) 工作记忆 (working memory: $wm$, 通常被称为hidden state, 标识为$h$)。 一个学习迭代有一下几个部分：\n选择性遗忘部分长期记忆：将记忆中不需要的记忆移除 将现有的一些信息加入到长期记忆中 计算候选长期记忆 选择函数 从long-term memory中提取working memory 计算候选的working memory 选择函数 选择性遗忘部分长期记忆 这个部分也称之为forget gate layer\n遗忘函数/遗忘门(forget gate) 我们先决定哪些长期记忆需要被遗忘（或者保留）。我们使用一个单独的浅层神经网络来学习。在$t$时刻：\n$$ remember_t = \\sigma (W_r x_t + U_r wm_{t-1}) $$\n这里的$remember_t$是一个boolean序列，长度和$lsm_{t-1}$相同，通常被称为forget gate。值1表示保留$lsm_{t-1}$对应位置的数值，0则表示抛弃或者删除。\n上述公式还可以表示成：\n$$ f_t = \\sigma (W_f \\cdot [ h_{t-1} , x_t ] + b_f) $$\n保存下来的长期记忆 有了遗忘函数后，我们就能确定保存下来的长期记忆是什么了。在$t$时刻：\n$$ olsm_t = forget_t \\odot lsm_{t-1} $$\n增加新的长期记忆 除了有些某些老的记忆需要继续保留，我们需要把当前的一些重要信息添加到长期记忆中。\n计算候选的长期记忆 首先算出来全体候选记忆。在$t$时刻：\n$$ lsm\u0026rsquo;_t = \\phi (W_l x_t + U_l wm_{t-1}) $$\n这里的$lsm\u0026rsquo;_t$代表可能加入长期记忆的记忆序列，长度和$lsm_{t-1}$相同。这里的$\\phi$函数常常选择$tanh$函数。\n上述公式还可以表示成： $$ \\tilde{C}_t = \\phi (W_C \\cdot [ h_{t-1} , x_t ] + b_C) $$\n选择函数 有了候选记忆后，需要一个选择函数负责实际选择哪些记忆可以加入长期记忆。在$t$时刻：\n$$ save_t = \\phi (W_s x_t + U_s wm_{t-1}) $$\n上述公式还可以表示成： $$ i_t = \\sigma (W_i \\cdot [h_{t-1},x_t] + b_i) $$\n新的长期记忆 有了候选的长期记忆和选择函数后，我们就可以确定哪些记忆是要添加到长期记忆的。在$t$时刻：\n$$ nlsm_t = save_t \\odot lsm\u0026rsquo;_t $$\n更新长期记忆 既然有了遗忘和更新机制，那么最终的长期记忆也就可以确定了。在$t$时刻：\n$$ lsm_t = olsm_t + nlsm_t $$\n上述公式还可以表示成： $$ C_t = f_t \\odot C_{t-1} + i_t \\odot \\tilde{C}_t $$\n其中$\\odot$表示element-wise product, 如果使用*替代，则能得到： $$ C_t = f_t * C_{t-1} + i_t * \\tilde{C}_t $$\n应用长期记忆 长期记忆需要应用在当前的工作记忆中才有作用。\n选择函数 从上一个工作记忆和当前输入中确定选择函数。在$t$时刻：\n$$ focus_t = \\sigma (W_f x_t + U_f wm_{t-1}) $$\n上述公式还可以表示成： $$ o_t = \\sigma (W_o \\cdot [h_{t-1},x_t] + b_o) $$\n候选的工作记忆 工作记忆是从长期记忆转换来的。在$t$时刻：\n$$ wm\u0026rsquo;_t = \\phi (lsm_{t-1}) $$\n其中这里的$\\phi$常常选择$tanh$\n上述公式还可以表示成：\n$$ \\tilde{h}_t = \\phi (C_t) $$\n更新工作记忆 既然有了候选工作记忆和选择函数，那么最终的工作记忆也就确定了。在$t$时刻：\n$$ wm_t = focus_t \\odot wm\u0026rsquo;_t $$\n上述公式还可以表示成：\n$$ h_t = o_t * \\tilde{h}_t $$\n变种 LSTM诞生后，不断有人改进模型，至今LSTM已经有很多变种了(请参考[参考文献]部分)。本节将介绍其中最重要的两个变种：Peephole LSTM和Gated Recurrent Unit\nPeephole LSTM 普通的LSTM的所有的门的决策全部都是由输入$x$和$wm_{t-1}$决定，Peephole LSTM改进了门的实现，让$lsm_{t-1}$也参与门的决策。\nCoupled Input and Forget Gate (CIFG) 既然forget gate和input gate都是控制更新long-term memory（$C$）的，那么他们可以合并成为一个update gate:forget gate忘记的信息全部由input gate提供。\nGated Recurrent Unit (GRU) GRU不仅使用了update gate替代了forget gate和input gate,而且将long-term memory ($C$)和working memory（$h$）合并了，并做了一些细微的调整。由于简化了原有LSTM的结构，速度更快，目前流行度不断增加。\n参考文献 Understanding LSTM Networks Exploring LSTMs http://slazebni.cs.illinois.edu/spring17/lec03_rnn.pdf ","permalink":"https://blog.xiaoquankong.ai/zh/posts/quickly-understand-lstm/","summary":"\u003cp\u003e\u003cstrong\u003eTL:DR\u003c/strong\u003e 本文属于入门级课程101，用图文并茂的方式详细的介绍了LSTM的工作原理。\u003c/p\u003e","title":"理解LSTM的工作原理"},{"content":"TL;DR 本文章收集了一些机器学习相关的理论方面的入门级的资料，适合初学者作为入门课程101。\n数学基础 线性代数 Vector, Matrix, and Tensor Derivatives CS321n 介绍矩阵和向量导数的算法 Properties of the Trace and Matrix Derivatives Matrix Differentiation ( and some other stuff ) 排版非常好看 神经网络 Backpropagation Calculus on Computational Graphs: Backpropagation 推荐入门 从计算图的角度介绍BP算法 LSTM 博文 Exploring LSTMs 入门推荐 非常详细的用易于理解的方式介绍了LSTM中各种门，后面更大篇幅探索了一下LSTM在各种简单问题上的内部参数情况，但后面的部分，我暂时没看懂:( Understanding LSTM Networks Stanford CS224n 推荐阅读 非常细致的介绍了LSTM中的各种门，建议先阅读上一篇文章再看这个文章，在理解为什么需要各种门的基础上再学习各种门是怎么实现的会更加清晰来龙去脉。 MOOC TODO\n大学课程 Stanford CS224n: Natural Language Processing with Deep Learning 没啥好介绍的，来自Stanford NLP group的，质量什么的没得说。 ","permalink":"https://blog.xiaoquankong.ai/zh/posts/machine-learning-related-materials/","summary":"\u003cp\u003e\u003cstrong\u003eTL;DR\u003c/strong\u003e 本文章收集了一些机器学习相关的理论方面的入门级的资料，适合初学者作为入门课程101。\u003c/p\u003e","title":"机器学习相关的理论资料汇总"},{"content":"TL;DR 本文将介绍神经网络中常用的几种激活函数的特性和使用场合。\n神经网络(Neural Network)中的激活函数(Activation Function)选择是至关重要的，它直接影响着模型的performance。\n各种激活函数介绍 下面将对比较常见的几种激活函数做简单的介绍：\nSigmoid Sigmoid是最早被使用的激活函数之一，现在依旧经常出现在教科书和教学中，是最经典的激活函数之一。Sigmoid函数有时使用符号$\\sigma$来表示。\n数学表示 $$ a=\\frac{1}{1+e^{-z}} $$\n图像 Note: 为了图像更加紧凑，这幅图实际对应的函数是$a=\\frac{1}{1+e^{-5z}}$\n导数 $$ f\u0026rsquo;(z)=\\frac{\\mathrm d \\sigma}{\\mathrm d z}=\\frac{1}{1+e^{-z}}(1-\\frac{1}{1+e^{-z}})=f(z)(1-f(z))=\\sigma(1-\\sigma) $$\n特性 可以看到sigmoid函数的一个最大的特点就是：值域严格限制在(0, 1)开区间。这种特性使得sigmoid可以将实数范围的值表示成概率的形式，这是sigmoid最大的特点。\ntanh tanh函数的全称是\u0026quot;hyperbolic tangent\u0026quot;,属于\u0026quot;Hyperbolic function\u0026quot;(双曲线函数)，关于这个函数的更多详细的资料可以访问Hyperbolic function在Wikipedia的相关页面\n数学表示 $$ a=\\frac{e^{z}-e^{-z}}{e^{z}+e^{-z}} $$\n图像 导数 $$ f\u0026rsquo;(z)=\\frac{\\mathrm d \\tanh}{\\mathrm d z}=1-(\\frac{e^{z}-e^{-z}}{e^{z}+e^{-z}})^2=1-f(z)^2=1-\\tanh^2 $$\n特性 tanh函数和sigmoid函数非常相似，都是一个优雅的S型曲线，事实上$tanh=sigmoid(z)*2+1$。唯一不同的地方是tanh的值域严格限制在(-1, 1)开区间。\nReLU ReLU的全称是\u0026quot;Rectified Linear Unit\u0026quot;（中文名应该翻译成“线性整流函数”或“修正线性单元”，但中文名几乎无人使用）。是目前神经网络中最主流的激活函数。\n数学表示 $$ a=max(0, z) $$\n图像 导数 $$ f\u0026rsquo;(z) = \\begin{cases} 0 \u0026amp; \\quad \\text{if } z \u0026lt; 0\\\\ 1 \u0026amp; \\quad \\text{if } z \u0026gt; 0 \\end{cases} $$\n特性 ReLU可以被看作是sigmoid函数在$(-\\infty, 0)$定义域上的近似函数。如下图： 上图中，蓝色的线表示sigmoid函数，绿色的线表示ReLU函数。 Note：为了让图更加容易理解，实际使用的sigmoid函数是$a=\\frac{1}{1+e^{-5z}}$，ReLU函数是$a=max(0, z+0.5)$\n同时因为因为数学上特别简单，所以计算速度非常快。已经在很多领域替代sigmoid和tanh。\nLeaky ReLU 是对ReLU的微小改动，在某些情况下会有比较好的效果。\n数学表示 $$ a=max(\\alpha z, z) $$ 其中$\\alpha \u0026lt; 1$\n图像 Note: 上图中使用的函数为：$ a=max(0.05z, z) $\n导数 $$ f\u0026rsquo;(z) = \\begin{cases} \\alpha \u0026amp; \\quad \\text{if } z \u0026lt; 0\\\\ 1 \u0026amp; \\quad \\text{if } z \u0026gt; 0 \\end{cases} $$\n特性 Leaky ReLU是ReLU的改动版，在某些特定的情况下会有比较好的结果。\n列表对比 下面用列表的形式，对常见的几种激活函数的特性和使用场景做一个总结：\n函数名 函数公式 使用场景^[来自：Neural Networks and Deep Learning by Andrew Ng on Coursera] sigmoid $a=\\frac{1}{1+e^{-z}}$ 适合二分类问题的output layer tanh $a=\\frac{e^{z}-e^{-z}}{e^{z}+e^{-z}}$ 绝大多数情况下优于sigmoid ReLU $a=max(0, z)$ 优先使用这种方法（推荐） Leaky ReLU $a=max(\\alpha z, z)$ 某些情况下效果较好 Note: 在实际场景中，可以尝试不同的函数，寻找最优方案。\n","permalink":"https://blog.xiaoquankong.ai/zh/posts/activiation-functions-in-neural-networks/","summary":"\u003cp\u003e\u003cstrong\u003eTL;DR\u003c/strong\u003e 本文将介绍神经网络中常用的几种激活函数的特性和使用场合。\u003c/p\u003e","title":"神经网络里的激活函数"},{"content":"TL;DR requirements.txt中不要使用非ASCII编码的字符，否则会造成字符集错误，无法解析内容\n实验设计 两份requirements.txt文件，包含相同的依赖，但不同的地方是：一份是ASCII编码（或者说只包含英文字符）；另一份包含非ASCII字符（比如中文注释之类的） 分别安装两份requirements.txt文件，观察现象 实验材料 ASCII编码的requirements.txt 命名为：all_ascii_requirements.txt\n具体内容如下：\ndummy 非ASCII编码的requirements.txt 命名为：non_ascii_requirements.txt\n具体内容如下：\n# 这个是中文注释 dummy 实验过程 安装ASCII编码的requirements.txt 执行命令 pip install -r all_ascii_requirements.txt\n命令工作正常，输入内容如下：\nCollecting dummy (from -r all_ascii_requirements.txt (line 1)) Downloading dummy-0.1.0.zip Requirement already up-to-date: jinja2\u0026gt;=2.0.0 in /usr/local/lib/python2.7/dist-packages (from dummy-\u0026gt;-r all_ascii_requirements.txt (line 1)) Collecting numpy\u0026gt;=1.0.0 (from dummy-\u0026gt;-r all_ascii_requirements.txt (line 1)) Downloading numpy-1.13.1-cp27-cp27mu-manylinux1_x86_64.whl (16.6MB) 100% |################################| 16.6MB 66kB/s Requirement already up-to-date: mock\u0026gt;=1.0.0 in /usr/local/lib/python2.7/dist-packages (from dummy-\u0026gt;-r all_ascii_requirements.txt (line 1)) Requirement already up-to-date: MarkupSafe\u0026gt;=0.23 in /usr/local/lib/python2.7/dist-packages (from jinja2\u0026gt;=2.0.0-\u0026gt;dummy-\u0026gt;-r all_ascii_requirements.txt (line 1)) Requirement already up-to-date: six\u0026gt;=1.9 in /usr/local/lib/python2.7/dist-packages (from mock\u0026gt;=1.0.0-\u0026gt;dummy-\u0026gt;-r all_ascii_requirements.txt (line 1)) Requirement already up-to-date: funcsigs\u0026gt;=1; python_version \u0026lt; \u0026#34;3.3\u0026#34; in /usr/local/lib/python2.7/dist-packages (from mock\u0026gt;=1.0.0-\u0026gt;dummy-\u0026gt;-r all_ascii_requirements.txt (line 1)) Requirement already up-to-date: pbr\u0026gt;=0.11 in /usr/local/lib/python2.7/dist-packages (from mock\u0026gt;=1.0.0-\u0026gt;dummy-\u0026gt;-r all_ascii_requirements.txt (line 1)) Building wheels for collected packages: dummy Running setup.py bdist_wheel for dummy ... done Stored in directory: /root/.cache/pip/wheels/fb/72/de/c12e171be0c7bff52d4bcebf680bd3b012203c68b8372b02a5 Successfully built dummy Installing collected packages: numpy, dummy Found existing installation: numpy 1.11.0 Uninstalling numpy-1.11.0: Successfully uninstalled numpy-1.11.0 Successfully installed dummy-0.1.0 numpy-1.13.1 安装非ASCII编码的requirements.txt 执行命令 pip install -r non_ascii_requirements.txt\n命令工作出现异常，输出内容如下：\nException: Traceback (most recent call last): File \u0026#34;/usr/local/lib/python2.7/dist-packages/pip/basecommand.py\u0026#34;, line 215, in main status = self.run(options, args) File \u0026#34;/usr/local/lib/python2.7/dist-packages/pip/commands/install.py\u0026#34;, line 312, in run wheel_cache File \u0026#34;/usr/local/lib/python2.7/dist-packages/pip/basecommand.py\u0026#34;, line 295, in populate_requirement_set wheel_cache=wheel_cache): File \u0026#34;/usr/local/lib/python2.7/dist-packages/pip/req/req_file.py\u0026#34;, line 84, in parse_requirements filename, comes_from=comes_from, session=session File \u0026#34;/usr/local/lib/python2.7/dist-packages/pip/download.py\u0026#34;, line 422, in get_file_content content = auto_decode(f.read()) File \u0026#34;/usr/local/lib/python2.7/dist-packages/pip/utils/encoding.py\u0026#34;, line 31, in auto_decode return data.decode(locale.getpreferredencoding(False)) UnicodeDecodeError: \u0026#39;ascii\u0026#39; codec can\u0026#39;t decode byte 0xe8 in position 2: ordinal not in range(128) 实验结论 pip在解析requirements.txt，只能处理ASCII编码的文件，否则会出现Unicode错误。在编写requirements.txt时，切记使用ASCII编码，不要夹杂中文等非ASCII字符\n","permalink":"https://blog.xiaoquankong.ai/zh/posts/encoding-issue-in-python-requirements/","summary":"\u003cp\u003e\u003cstrong\u003eTL;DR\u003c/strong\u003e requirements.txt中不要使用非ASCII编码的字符，否则会造成字符集错误，无法解析内容\u003c/p\u003e","title":"python中requirements.txt的编码问题"},{"content":"TL;DR 本文使用开源框架chatterbot从零开始构建你自己的聊天机器人（还带有WEB界面奥～）。\n聊天机器人大体上分为三种：闲聊机器人、问答机器人和任务型机器人。闲聊机器人，顾名思义就是和你闲聊插科打诨的机器人，目前比较典型的代表是微软小冰，小黄鸡等。问答机器人有一个标准答案库，当用户来咨询时机器人负责理解用户的语意，给出符合语意的标准答案，目前比较典型的应用是各类咨询机器人，客服机器人等。最后一类：任务型机器人，通过和客户的沟通帮助用户完成特定任务比如定机票、定闹钟等，目前比较典型的应用是各种私人助理，苹果的siri系统也具备此类功能。\n我们这里介绍一个简单易用的闲聊机器人框架chatterbot website: http://chatterbot.readthedocs.io/\n先上一个成品图，这将是我们最后完成时的效果：\n安装 pip install chatterbot 快速入门（toy级别的方案） 下面的代码实现了一个toy聊天机器人\n# 导入所需的依赖 from chatterbot import ChatBot from chatterbot.trainers import ListTrainer chatbot = ChatBot(\u0026#34;SillyRobot\u0026#34;) # 这里创建了机器人实例，并设定了机器人的名字：SillyRobot # 定义训练数据集 conversation = [ \u0026#34;Hello\u0026#34;, \u0026#34;Hi there!\u0026#34;, \u0026#34;How are you doing?\u0026#34;, \u0026#34;I\u0026#39;m doing great.\u0026#34;, \u0026#34;That is good to hear\u0026#34;, \u0026#34;Thank you.\u0026#34;, \u0026#34;You\u0026#39;re welcome.\u0026#34; ] # 训练 chatbot.set_trainer(ListTrainer) chatbot.train(conversation) # 响应用户请求 response = chatbot.get_response(\u0026#34;Good morning!\u0026#34;) print(response) 上述代码会训练你给定的训练集，并把训练结果保存起来，没有指定的情况下，会使用存储模块chatterbot.storage.SQLStorageAdapter完成模型的存储。在完成训练后就可以将训练代码移除，这样机器人就不会每次都要从头训练了。\n比较正式的方案 上面的方案使用的语料库是硬编码在文档中的，这在正式项目中是不合适的。下面介绍一个比较正式的使用chatter的方案。\nimport os from chatterbot import ChatBot from chatterbot.trainers import ChatterBotCorpusTrainer current_dir = os.path.dirname(os.path.realpath(__file__)) chat_bot = ChatBot(\u0026#34;SillyRobot\u0026#34;) # 这里创建了机器人实例，并设定了机器人的名字：SillyRobot chat_bot.set_trainer(ChatterBotCorpusTrainer) # 使用中文语料库训练它 # chat_bot.train(\u0026#34;chatterbot.corpus.chinese\u0026#34;) # 语料库 # 开始对话 response = chat_bot.get_response(\u0026#34;我好么\u0026#34;) print(response) 官方自带的中文聊天数据集表现比较差，你需要自己实现一个trainer，具体怎么实现见官方文档 Creating a new training class.\nWeb集成 chatter自带了Django集成，所以很容易架设一个网站，提供HTTP接口、管理后台以及在线聊天页面等功能。具体代码可以拷贝官方的示例代码https://github.com/gunthercox/ChatterBot/tree/master/examples/django_app, 这里需要注意的是，你需要更改chatterbot的配置在settings.py里面的CHATTERBOT变量处，具体怎么修改，请参考文档1和文档2,但你仍然需要以下步骤：\n安装Django pip install django 同步数据库 python manage.py migrate 创建超级用户 python manage.py createsuperuser 训练chatterbot python manage.py train 运行server python manage.py runserver web服务器默认监听5000端口，访问http://127.0.0.0.1:5000就能访问页面了，页面效果如下图：\n","permalink":"https://blog.xiaoquankong.ai/zh/posts/building-your-own-chitbot-using-chatterbot/","summary":"\u003cp\u003e\u003cstrong\u003eTL;DR\u003c/strong\u003e 本文使用开源框架chatterbot从零开始构建你自己的聊天机器人（还带有WEB界面奥～）。\u003c/p\u003e","title":"使用chatterbot构建自己的中文chat(闲聊)机器人"},{"content":"TL;DR 弃坑wordpress，走向hexo\n一直以来，我都是使用wordpress来写博客，从Octopress时代就想尝试静态博客系统的，但由于惰性使然，一直也就用着wordpress。 最近两个事情让我终于下定决心要切换博客系统了：一是wordpress的markdown支持实在太烂，用了不少相关插件但是还是不好用；二是wordpress里面没有一个让我称心如意的主题,hexo的Next主题确实很适合码农，我在wordpress世界里找不到类似的主题。所以，还是选择追随潮流，很多技术人都选择了hexo，我想应该是有保障的,又开始折腾之路。\n","permalink":"https://blog.xiaoquankong.ai/zh/posts/bye-wordpress-and-hello-hexo/","summary":"\u003cp\u003e\u003cstrong\u003eTL;DR\u003c/strong\u003e 弃坑wordpress，走向hexo\u003c/p\u003e","title":"再见wordpress,你好hexo"},{"content":"TL;DR map函数及其类似函数在python2和python3下表现差异很大，py2下返回list，而py3下返回迭代器。解决办法是使用list函数显式求值。\n下面是关于map函数的代码示例，透过在python2和python3不同行为，为你展现不同：\n导入相关模块\nimport time 定义我们的task函数，通过打印输出让我们了解实际工作情况\ndef task(x): print(\u0026#34;round: {}, I am start to sleep\u0026#34;.format(x)) time.sleep(1) print(\u0026#34;round: {}, I am finished sleep\u0026#34;.format(x)) return pow(x, 2) 将可迭代对象map到task函数\nmap(task, range(4)) python2中的map函数 在python2中执行以上代码，可以得到如下输出：\nround: 0, I am start to sleep round: 0, I am finished sleep round: 1, I am start to sleep round: 1, I am finished sleep round: 2, I am start to sleep round: 2, I am finished sleep round: 3, I am start to sleep round: 3, I am finished sleep [0, 1, 4, 9] python3中的map函数 在python3中执行以上代码，可以得到如下输出：\n\u0026lt;map at 0x7f9df0559b00\u0026gt; 很不幸，你的task代码并没有执行，无论是打印输出还是返回值，都没有执行。 那是因为python2中的map是Apply function to every item of iterable and return a list of the results.而python3中的map是Return an iterator that applies function to every item of iterable, yielding the results.\n这个例子中，使用了交互式编程，用户可以直观的看到返回结果，但是在非交互式使用场景（比如作为模块运行）时，如果没有收集返回值也没有类似打印输出的情况下，一切看似正常，但实际没有运行的情况，将造成难以调试的bug。\n解决不同 如何才能让python3的map函数的行为和python2的一样呢？\n答案是使用list函数，将上述代码稍作改动：\nlist(map(task, range(4))) 那么你将得到输出：\nround: 0, I am start to sleep round: 0, I am finished sleep round: 1, I am start to sleep round: 1, I am finished sleep round: 2, I am start to sleep round: 2, I am finished sleep round: 3, I am start to sleep round: 3, I am finished sleep [0, 1, 4, 9] 这将和python2一模一样。\n更大范围的不同 很不幸的是python2和python3的差异不仅仅在一个map函数上，很多函数也存在类似的差异，如multiprocessing模块的Pool类的imap_unordered和map方法，实际上python2和python3差异还是比较大的，所以如果遇到兼容性问题，第一件事情就是立即查阅官方文档。\n","permalink":"https://blog.xiaoquankong.ai/zh/posts/the-difference-of-map-function-in-python-2-and-python-3/","summary":"\u003cp\u003e\u003cstrong\u003eTL;DR\u003c/strong\u003e map函数及其类似函数在python2和python3下表现差异很大，py2下返回list，而py3下返回迭代器。解决办法是使用list函数显式求值。\u003c/p\u003e","title":"map函数与相似函数在python2和python3中的不同"},{"content":"TL;DR 指定metadata_path时，使用相对路径会造成tensorboard找不到文件。解决办法：使用绝对路径\nTensorBoard的embedding载入时卡着不动 TensorBoard的Embedding功能，给图片和文字的三维直观展示提供了可能，这些东西通常情况下需要一个人类可读的meta信息：这个信息在TensorBoard中叫做metadata。\n你可以从https://www.tensorflow.org/versions/r1.3/programmers_guide/embedding获取更多官方文档\n原因分析 如下是官方代码示例\nfrom tensorflow.contrib.tensorboard.plugins import projector # Create randomly initialized embedding weights which will be trained. vocabulary_size = 10000 embedding_size = 200 embedding_var = tf.get_variable('word_embedding', [vocabulary_size, embedding_size]) # Format: tensorflow/tensorboard/plugins/projector/projector_config.proto config = projector.ProjectorConfig() # You can add multiple embeddings. Here we add only one. embedding = config.embeddings.add() embedding.tensor_name = embedding_var.name # Link this tensor to its metadata file (e.g. labels). embedding.metadata_path = os.path.join(LOG_DIR, 'metadata.tsv') # Use the same LOG_DIR where you stored your checkpoint. summary_writer = tf.summary.FileWriter(LOG_DIR) # The next line writes a projector_config.pbtxt in the LOG_DIR. TensorBoard will # read this file during startup. projector.visualize_embeddings(summary_writer, config) 上面的代码中，主要是通过projector对象将tensor变量embedding_var和metadata_path关联了起来，随后这个关联信息被summary_writer写入了配置文件。\n很多人的代码实现，在运行时没有任何问题，但是在使用tensorboard中进行embedding可视化的时候，却一直卡在加载metadata的过程中。具体原因是在指定embedding.metadata_path时，使用了相对路径来指定文件位置，但是这个文件位置是相对log目录的，tensorboard在启动的时候绝大多数情况下不是在log所在的目录中启动的，这时从tensorboard的角度来找这个相对文件，就无法找到，因此出现了载入卡住的情况。\nPS stackoverflow 也有人注意到了这个issue，并给出了相同的答案\n","permalink":"https://blog.xiaoquankong.ai/zh/posts/solution-for-tensorboard-embedding-blocked-when-loading-metadata/","summary":"\u003cp\u003e\u003cstrong\u003eTL;DR\u003c/strong\u003e 指定\u003ccode\u003emetadata_path\u003c/code\u003e时，使用相对路径会造成tensorboard找不到文件。解决办法：使用绝对路径\u003c/p\u003e","title":"TensorBoard的embedding卡在Loading metadata的解决方案"},{"content":"TL;DR Descriptor是Python实现streagy模式的一种变形，它将属性的访问／修改／删除委托给了Descriptor。\nDescriptor 示例代码如下：\nclass Descriptor(object): class_var = \u0026#34;cls\u0026#34; def __init__(self, *args, **kwargs): # Here the `super` is differ from python2: super(Descriptor, self) super().__init__(*args, **kwargs) def __get__(self, instance, owner): print(\u0026#34;==\u0026#34;) print(instance, owner) return None def __set__(self, instance, value): print(instance, value) def __delete__(self, instance): print(instance) class Host(object): desc = Descriptor() norm = 1 host = Host() host.desc host.norm == \u0026lt;__main__.Host object at 0x7ff7dbff5d68\u0026gt; \u0026lt;class \u0026#39;__main__.Host\u0026#39;\u0026gt; 1 host.desc = 2 host.norm = 2 \u0026lt;__main__.Host object at 0x7ff7dbff5d68\u0026gt; 2 desc = Descriptor() desc.__get__(None, None) == None None ","permalink":"https://blog.xiaoquankong.ai/zh/posts/introduce-to-python-descriptor/","summary":"\u003cp\u003e\u003cstrong\u003eTL;DR\u003c/strong\u003e Descriptor是Python实现streagy模式的一种变形，它将属性的访问／修改／删除委托给了Descriptor。\u003c/p\u003e","title":"Python描述器"},{"content":"TL;DR Python的默认参数实现机制很容易导致难以调试的bug。正确的使用方法是避免使用可变类型作为默认参数。\n默认可变参数带来的问题 函数的默认参数如果是可变类型的变量，可能会带来难以debug的bug。def定义的函数，在解释器定义为函数时，会计算默认参数的值，并将值存储在 func_defaults 属性中，并且该默认值只会初始化一次\ndef append_to_list(item, list_=[]): list_.append(item) print(list_) print(\u0026#34;.func_defaults:\u0026#34;, append_to_list.func_defaults) append_to_list(0) print(\u0026#34;.func_defaults:\u0026#34;, append_to_list.func_defaults) append_to_list(1) print(\u0026#34;.func_defaults:\u0026#34;, append_to_list.func_defaults) 运行以上代码，则会有如下输出:\n(\u0026#39;.func_defaults:\u0026#39;, ([],)) [0] (\u0026#39;.func_defaults:\u0026#39;, ([0],)) [0, 1] (\u0026#39;.func_defaults:\u0026#39;, ([0, 1],)) 解决方案 def append_to_list(item, list_=None): if list_ is None: list_ = [] list_.append(item) print(list_) print(\u0026#34;.func_defaults:\u0026#34;, append_to_list.func_defaults) append_to_list(0) print(\u0026#34;.func_defaults:\u0026#34;, append_to_list.func_defaults) append_to_list(1) print(\u0026#34;.func_defaults:\u0026#34;, append_to_list.func_defaults) 运行以上代码，则会有如下输出:\n(\u0026#39;.func_defaults:\u0026#39;, (None,)) [0] (\u0026#39;.func_defaults:\u0026#39;, (None,)) [1] (\u0026#39;.func_defaults:\u0026#39;, (None,)) 参考 编写高质量Python代码的91个建议 第32个建议 ","permalink":"https://blog.xiaoquankong.ai/zh/posts/default-arguments-in-python-function/","summary":"\u003cp\u003e\u003cstrong\u003eTL;DR\u003c/strong\u003e Python的默认参数实现机制很容易导致难以调试的bug。正确的使用方法是避免使用可变类型作为默认参数。\u003c/p\u003e","title":"Python函数默认参数的问题"},{"content":"TL;DR 上下文管理器通过控制代码块级别的上下文，可以实现的很多诸如自动关闭文件、捕获异常等功能\n什么是上下文管理器 上下文管理器 context manager 能够控制程序执行的上下文,比如控制文件的关闭,抑制异常,捕获异常,修改上下文变量等\n调用过程 简单例子 class Context(object): def __enter__(self): print(\u0026quot;__enter__ invoked\u0026quot;) def __exit__(self, exc_type, exc_val, exc_tb): print(\u0026quot;__exit__ invoked\u0026quot;) with Context(): print(\u0026quot;with block\u0026quot;) `\u0026lt;/pre\u0026gt; 运行以上代码，则会有如下输出: \u0026lt;pre\u0026gt;`__enter__ invoked with block __exit__ invoked `\u0026lt;/pre\u0026gt; ### 中级复杂的例子 \u0026lt;pre\u0026gt;`class ContextInstance(object): def __init__(self, msg): self.msg = msg def say(self): print(self.msg) def when_exit(self): print(\u0026quot;instance exited!\u0026quot;) class Context(object): def __init__(self, msg): self.msg = msg self.instance = None super(Context, self).__init__() def __enter__(self): print(\u0026quot;__enter__ invoked\u0026quot;) self.instance = ContextInstance(self.msg) return self.instance def __exit__(self, exc_type, exc_val, exc_tb): print(\u0026quot;__exit__ invoked\u0026quot;) self.instance.when_exit() with Context(\u0026quot;Message\u0026quot;) as ctx: ctx.say() print(\u0026quot;within block\u0026quot;) `\u0026lt;/pre\u0026gt; 运行以上代码，则会有如下输出: \u0026lt;pre\u0026gt;`__enter__ invoked Message within block __exit__ invoked instance exited! `\u0026lt;/pre\u0026gt; ### 复杂例子 \u0026lt;pre\u0026gt;`class ContextManager(object): def __init__(self, msg): self.msg = msg self.instance = None super(ContextManager, self).__init__() def __enter__(self): print(\u0026quot;__enter__ invoked\u0026quot;) self.instance = ContextInstance(self.msg) return self.instance def __exit__(self, exc_type, exc_val, exc_tb): print(\u0026quot;__exit__ invoked\u0026quot;) self.instance.when_exit() class ContextInstance(object): def __init__(self, msg): self.msg = msg super(ContextInstance, self).__init__() def say(self): print(self.msg) def when_exit(self): print(\u0026quot;instance existed!\u0026quot;) with ContextManager(\u0026quot;Message\u0026quot;) as ctx: ctx.say() print(\u0026quot;with block\u0026quot;) `\u0026lt;/pre\u0026gt; 运行以上代码，则会有如下输出: \u0026lt;pre\u0026gt;`__enter__ invoked Message with block __exit__ invoked instance existed! `\u0026lt;/pre\u0026gt; ## 使用场景 ### 控制文件关闭 \u0026lt;pre\u0026gt;`with open(\u0026quot;/tmp/context_manager.txt\u0026quot;, 'wt') as f: f.write(\u0026quot;contexts go here\u0026quot;) `\u0026lt;/pre\u0026gt; ### 抑制异常 \u0026lt;pre\u0026gt;`class Context(object): def __enter__(self): return self def __exit__(self, exc_type, exc_val, exc_tb): # supress all the exception return True with Context(): print(\u0026quot;start\u0026quot;) raise ValueError(\u0026quot;E!\u0026quot;) print(\u0026quot;end\u0026quot;) print(\u0026quot;next\u0026quot;) `\u0026lt;/pre\u0026gt; 运行以上代码，则会有如下输出: \u0026lt;pre\u0026gt;`start next `\u0026lt;/pre\u0026gt; ### 捕获异常 \u0026lt;pre\u0026gt;`# TODO： see unittest catch exceptioon `\u0026lt;/pre\u0026gt; ### 修改上下文 \u0026lt;pre\u0026gt;`env_context = [] class Context(object): def __enter__(self): env_context.append(1) def __exit__(self, exc_type, exc_val, exc_tb): env_context.pop() print(\u0026quot;before with\u0026quot;, len(env_context)) with Context(): print(\u0026quot;in with\u0026quot;, len(env_context)) print(\u0026quot;after with\u0026quot;, len(env_context)) `\u0026lt;/pre\u0026gt; 运行以上代码，则会有如下输出: \u0026lt;pre\u0026gt;`('before with', 0) ('in with', 1) ('after with', 0) `\u0026lt;/pre\u0026gt; ## contextlib 库 contextlib是Python官方包，使用contenxtlib可以很方便的构建上下文管理器 ### contextlib.contextmanager #### 简单用法 \u0026lt;pre\u0026gt;`import contextlib @contextlib.contextmanager def context(): print(\u0026quot;before yeild\u0026quot;) yield [] print(\u0026quot;after yeild\u0026quot;) with context() as value: print(\u0026quot;before value\u0026quot;) print(value) print(\u0026quot;after value\u0026quot;) `\u0026lt;/pre\u0026gt; 运行以上代码，则会有如下输出: \u0026lt;pre\u0026gt;`before yeild before value [] after value after yeild `\u0026lt;/pre\u0026gt; #### 捕获异常 \u0026lt;pre\u0026gt;`import contextlib @contextlib.contextmanager def context(): print(\u0026quot;before yeild\u0026quot;) try: yield except ValueError as e: print(e) print(\u0026quot;after yeild\u0026quot;) with context(): raise ValueError(\u0026quot;NO\u0026quot;) print(\u0026quot;after exception\u0026quot;) print(\u0026quot;after with\u0026quot;) `\u0026lt;/pre\u0026gt; 运行以上代码，则会有如下输出: \u0026lt;pre\u0026gt;`before yeild NO after yeild after with `\u0026lt;/pre\u0026gt; ### 嵌套的上下文管理器 (Nesting contexts) \u0026lt;pre\u0026gt;`import contextlib @contextlib.contextmanager def context(name): print(\u0026quot;entring %s\u0026quot; % name) yield name print(\u0026quot;exiting %s\u0026quot; % name) with contextlib.nested(context('a'), context('b'), context('c')) as (a, b, c): print(\u0026quot;inside with statement: %s\u0026quot; % ((a, b, c), )) `\u0026lt;/pre\u0026gt; 运行以上代码，则会有如下输出: \u0026lt;pre\u0026gt;`entring a entring b entring c inside with statement: ('a', 'b', 'c') exiting c exiting b exiting a /usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:9: DeprecationWarning: With-statements now directly support multiple context managers if __name__ == '__main__': `\u0026lt;/pre\u0026gt; 观察输出，你会发现，嵌套的上下文管理器工作起来像是先入后出的栈一样：最先进入的管理器最后退出，最后进入的管理器最先退出。 同时你应该观察到一个Warning：DeprecationWarning: With-statements now directly support multiple context managers。contextlib.nested函数将于后续Python版本被弃用，Python 2.7引入了嵌套上下文管理器新语法。 #### Python 2.7 版本的新语法 (py2.7 now support nested) \u0026lt;pre\u0026gt;`import contextlib @contextlib.contextmanager def context(name): print(\u0026quot;entring %s\u0026quot; % name) yield name print(\u0026quot;exiting %s\u0026quot; % name) with context('a') as a, context('b') as b, context('c') as c: print(\u0026quot;inside with statement: %s\u0026quot; % ((a, b, c), )) `\u0026lt;/pre\u0026gt; 运行以上代码，则会有如下输出: \u0026lt;pre\u0026gt;`entring a entring b entring c inside with statement: ('a', 'b', 'c') exiting c exiting b exiting a `\u0026lt;/pre\u0026gt; ### 自动关闭的上下文管理器 (closing context) `contextlib.closing`等价与如下代码: \u0026lt;pre\u0026gt;`import contextlib @contextlib.contextmanager def closing(thing): try: yield thing finally: thing.close() `\u0026lt;/pre\u0026gt; ### 简单例子 \u0026lt;pre\u0026gt;`import contextlib class Door(): def close(self): print(\u0026quot;Door closing\u0026quot;) def do_something(self): print(\u0026quot;doing something\u0026quot;) try: with contextlib.closing(Door()) as door: door.do_something() raise ValueError(\u0026quot;Exception raising\u0026quot;) except ValueError as e: print(e) `\u0026lt;/pre\u0026gt; 运行以上代码，则会有如下输出: \u0026lt;pre\u0026gt;`doing something Door closing Exception raising `\u0026lt;/pre\u0026gt; ### 替代的例子 \u0026lt;pre\u0026gt;`import contextlib @contextlib.contextmanager def closing(thing): try: yield thing finally: thing.close() class Door(): def close(self): print(\u0026quot;Door closing\u0026quot;) def do_something(self): print(\u0026quot;doing something\u0026quot;) try: with closing(Door()) as door: door.do_something() raise ValueError(\u0026quot;Exception raising\u0026quot;) except ValueError as e: print(e) `\u0026lt;/pre\u0026gt; 运行以上代码，则会有如下输出: \u0026lt;pre\u0026gt;`doing something Door closing Exception raising `\u0026lt;/pre\u0026gt; ### 实际的例子 \u0026lt;pre\u0026gt;`from contextlib import closing import urllib fair_license_url = 'http://www.samurajdata.se/opensource/mirror/licenses/fair.txt' with closing(urllib.urlopen(fair_license_url)) as page: for line in page: print(line) `\u0026lt;/pre\u0026gt; 运行以上代码，则会有如下输出: \u0026lt;pre\u0026gt;`Fair License \u0026amp;lt;Copyright Information\u0026amp;gt; Usage of the works is permitted provided that this instrument is retained with the works, so that any entity that uses the works is notified of this instrument. DISCLAIMER: THE WORKS ARE WITHOUT WARRANTY. [2004, Fair License: rhid.com/fair] 参考 Python官方contexlib库文档 ","permalink":"https://blog.xiaoquankong.ai/zh/posts/introduce-to-context-manager-in-python/","summary":"\u003cp\u003e\u003cstrong\u003eTL;DR\u003c/strong\u003e 上下文管理器通过控制代码块级别的上下文，可以实现的很多诸如自动关闭文件、捕获异常等功能\u003c/p\u003e","title":"Python context manager 上下文管理器"},{"content":"TL;DR Git Flow是一种得到广泛认可的模型，通过确定分支的实际用途（master/develop/feature等），达到团队共同认知认可全部开发过程效果。\n为什么需要Git开发规范（模型） 随着Git的普及，几乎所有的开发团队都在使用Git进行版本管理和团队协作。但是Git本身提供的功能很多，灵活性太大，每个人都可以有自己的使用方式和代码管理风格。\n混乱的开发流程 如果整个团队没有一个共同的Git使用共识，那么合作起来需要沟通和作出妥协，沟通不畅的情况下还可能导致的问题。\n轻则，分支名字千奇百怪：有的用日期，有的用功能名称，有的用Bug编号，有的用开发者自己的名字，甚至是混合体；本地分支和远程分支几十个，谁也不知道具体的分支是做什么的。\n严重的情况则比如：\nA向master提交了未经测试的下一个迭代或者版本才会上线的代码，并push到了服务器，这个时候线上代码出现问题需要紧急修复，B负责此事，他没有和A沟通，认为master上的代码就是线上代码，直接pull下来，修复了bug所在的代码片段，并将代码发布到了线上，这个时候A提交的未经测试的代码出现在了线上，同时由于未经过测试所以出现了很严重的Bug导致系统不可用。\n这种情况在团队人员水平参差不齐或者来自不同的小组，缺乏良好沟通和合作经验的情况下，很有可能会出现。\n常见的Git规范 针对这种情况，这个团队需要一个全员都认可并知晓的Git代码管理规范，这种规范最好是经过长时间工程实践考验的，通用的，容易学习和灵活的。目前工程领域里有两个开发规范或者说开发模型符合这些特征：一个是开源项目比较通用的同时也是Github官方推荐的Pull Request的方式，另一个是今天我们将要重点说明的Git Flow模型。\nGit Flow简介 Git Flow模型源自Vincent Driessen在2010年发布的一篇文章: A successful Git branching model, 这边文章得到了很多人的认同（比如阮一峰的Git分支管理策略）并且在工业界得到了广泛的应用。\n两个永久分支 Git Flow 模型中有两个永久存在的分支：master 和 develop\nmaster分支 master分支上的代码永远代表着当前生产环境的代码状况，并且master分支只接受代码合并，不接受commit提交\ndevelop分支 develop分支的代码代表这下一个迭代将要发布的代码情况，没有确定下来将要在下一个迭代发布的代码不能合并在这个分支。devlop分支的代码和master分支同步或者领先master分支，绝对不会出现落后master分支的情况。\n若干辅助分支 feature分支 业务的需求等都可以看作是产品的特性，feature分支就是正在开发的多个特性构成的集合，feature分支可以同时拥有多个，具备统一的命名规范（具体命名规范在下文中详述）。feature分支起源于develop分支，最终也将合并至develop分支，但也有例外：试验性的特性如果最终证明是失败的，将被删除而不合并入develop。每一个feature代表着一个特性，这些特性只在开发完毕准备下个迭代发布时合并入develop分支，不在下一个迭代发布的特性不能合并到develop分支。\nhotfix分支 线上代码出现严重Bug需要紧急修复时使用该分支。hotfix分支起源于master分支（也就是线上代码），开发完成后合并至master分支同时合并到develop分支以确保下一个版本的代码也包含这个hotfix补丁。\nrelease分支 当develop分支代码测试完毕，准备发布时，将从develop分支发起release分支，进入release阶段的代码将不允许新特性的添加（也就是不允许feature分支合并入develop分支或者release分支），又称特性冻结（feature freeze）。release分支将进行最终最严格的测试。release分支接受为了修复bug而产生的提交。release分支持续变动，直至开发工作验收通过，这时release分支将会打上版本标签（tag）合并至master分支同时合并至devlop分支以确保develop分支是master分支的直接后继。\nbugfix分支 bugfix分支和hotfix分支目的类似，但bug紧急程度不需要立即修复，而是留到下一个版本发布时修正。和hotfix不同的是bugfix起源于develop分支也最终合并于develop分支，这一点和feature分支非常相似。\nsupport分支 如果项目有老版本的代码因为各种原因需要维护时，会需要support分支来管理这些过时但是依旧需要维护的代码。\nspecial分支 对应于软件的各种特别版本：节日特别版（圣诞等），周年特别版等\nGit Flow AVH 介绍 除了这些标准规范，作者还提供了Git的插件帮助开发者更快和更好的处理具体的细节，我们现在介绍的是比较流行的Git Flow AVH版本。\n安装 这里使用Linux下安装为例子，更加详细和更多平台的安装指南请查阅官方维基git-flow AVH Edition\n安装开发版本:\nwget --no-check-certificate -q https://raw.githubusercontent.com/petervanderdoes/gitflow-avh/develop/contrib/gitflow-installer.sh \u0026amp;amp;\u0026amp;amp; sudo bash gitflow-installer.sh install develop; rm gitflow-installer.sh `\u0026lt;/pre\u0026gt; 安装稳定版本: \u0026lt;pre\u0026gt;`wget --no-check-certificate -q https://raw.githubusercontent.com/petervanderdoes/gitflow-avh/develop/contrib/gitflow-installer.sh \u0026amp;amp;\u0026amp;amp; sudo bash gitflow-installer.sh install stable; rm gitflow-installer.sh 初始化 Git flow 通过让你回答一些分支命名规则等信息来初始化。 大体问题如下：\n默认的生产发布分支（production releases）的名字是什么【默认是master】 默认的develop（\u0026ldquo;next release\u0026rdquo; development）分支的名字是什么【默认是develop】 默认的feature分支的前缀是什么【默认是feature/】 默认的bugfix分支的前缀是什么【默认是bugfix/】 默认的release分支的前缀是什么【默认是release/】 默认的hotfix分支的前缀是什么【默认是hotfix/】 默认是support分支的前缀是什么【默认是support/】 默认的版本号的前缀是什么【默认是是空】 Git flow的hooks和filters的路径 [默认是git项目中的.git/hooks目录] 通过这些问题，Git flow获取了相关的配置设定，初始化完成\n通用模式 Git flow的命令存在一个通用的模式，亦即 git flow MODE ACTION NAME [BASE], 下面将一一介绍：\nMODE MODE亦即模式，表示你想要使用的git flow的功能，功能负责映射命令到分支和相应的动作，常见模式有\nfeature bugfix hotfix release ACTION ACTION亦即动作，表示你想要在这个模式下执行的动作，常见的动作模式有\nstart finish NAME NAME亦即名字，表示你要完成的对象的名字，这个部分完全由你自己定义，有下面几个建议\nfeature / bugfix 模式下使用英文描述命名，因为这个可以方便的了解这些分支的用途 hotfix / release 模式使用版本号命名 [BASE] 按照传统惯例，[]内包裹的部分是可选的，这也不例外，base仅在start动作才有，finish动作并无此参数，base表示的是你想要操作的对象不是以默认的分支作为起点，而是以base参数指定的commit id作为起点\n使用范例 开始新的feature 假设你想要开始一个新的feature，这个feature对应产品需求的“增加回复功能“，你可以使用命令如：git flow feature start add_replay_function, 该命令将会执行如下动作：\n基于develop分支（或者你在初始化时指定的develop对应的分支）创建 \u0026lsquo;feature/add_replay_function\u0026rsquo;分支（如果你在初始化时改变了默认的feature/的分支前缀，这里也会发生相应的变动） 切换到刚刚新建的分支 在命令中feature是MODE，start是ACTION，add_replay_function是NAME\n完成新的feature 当你完成一个feature的开发时，你可以使用命令：git flow feature finish add_replay_function，该命令将会执行如下操作：\n将分支\u0026rsquo;feature/add_replay_function\u0026rsquo;（名字可能有所不同，见\u0026quot;开始新的feature\u0026quot;部分，下同）合并到develop分支 将本地的\u0026rsquo;feature/add_replay_function\u0026rsquo;分支删除 切换到develop分支 在命令中feature是MODE，finish是ACTION，add_replay_function是NAME\n开始release 当一切准备就绪，准备开始发布新版1.0的时候，你可以使用命令如：git flow release start 1.0, 该命令将会执行如下动作：\n基于 develop 分支创建 release/1.0 分支 切换至 release/1.0 分支 在命令中release是MODE，start是ACTION，1.0是NAME，注意这里的名字实际时版本号，这里推荐使用版本号\n完成release 当发布完成之后，使用命令：git flow release finish 1.0, 该命令将会执行如下动作：\n合并 release/1.0 到 master master分支被打上tag 合并 release/1.0 到 develop 删除本地的 release/1.0 分支 切换至 develop 分支 中间可能会要求你填写的信息：\n要求你填写 release/1.0 合并到 master 的 merge message，有默认值，可以直接使用默认值 要求你填写 tag 信息，这次提交将会被打上tag，没有默认值，建议填写本次发布的内容，改变等信息 可能会让你填写 release/1.0 合并到 develop 的 merge message，有默认值，可以直接使用默认值 通过以上的例子，你应该已经理解git flow的工作模式，剩下的命令你都可以举一反三，灵活应用，更多信息请查看官方文档。\n","permalink":"https://blog.xiaoquankong.ai/zh/posts/introduce-to-git-flow/","summary":"\u003cp\u003e\u003cstrong\u003eTL;DR\u003c/strong\u003e Git Flow是一种得到广泛认可的模型，通过确定分支的实际用途（master/develop/feature等），达到团队共同认知认可全部开发过程效果。\u003c/p\u003e","title":"Git Flow开发模型"},{"content":"TL;DR 本文从代码级别详细介绍了 Whisper 的实现和一些其中用到的编程技巧\n什么是Whisper 很多人熟悉著名的指标监控系统：Graphite\n这里有一些来自Graphite官方的介绍，翻译成中文后大致是：\nGraphite是一个运行在廉价硬件上的企业级的监控工具\nGraphite做两件事：\n存储数值化的时间序列数据 按需渲染数据图形 Whisper是Graphite核心组件之一。负责“存储数值化的时间序列数据”的两个部分：一个是负责接收网络数据的Carbon组件，另一个就是负责存储到磁盘的Whisper组件。\n正式的说，Whisper是为Graphite项目定制的时间序列数据库（或者软件库），其本身也是可以单独作为或者集成为通用的时间数据库。\n重要:\n以下代码／结构分析基于whisper==0.9.10，不同的版本可能会存在变动。\n数据库结构总揽 综述 一个Whisper数据库，由单个文件构成。这个文件可以分成三个部分：Header，Archives，data。 每个部分都是C兼容的数据结构构成，在实现上whisper使用struct库来实现pack和unpack。 每一个archive都对应着一个precision不同的存储区域。\nHeader Header由四个字段构成：aggregationType, maxRetention, xff, archiveCount\naggregationType 数据类型：Long int (aka \u0026lsquo;L\u0026rsquo; in struct format), 用来控制高精度向低精度聚合时采用的策略（算法） 具体策略如下：\naggregationTypeToMethod = dict({ 1: \u0026#39;average\u0026#39;, 2: \u0026#39;sum\u0026#39;, 3: \u0026#39;last\u0026#39;, 4: \u0026#39;max\u0026#39;, 5: \u0026#39;min\u0026#39;, 6: \u0026#39;avg_zero\u0026#39; }) maxRetention 数据类型：Long int, 该数据库能够存储的最大时间长度（单位秒）\nxff 全称: xFilesFactor, 数据类型：Float (aka \u0026lsquo;f\u0026rsquo; in struct format), 当higher precision向lower precision聚合时，如果有效数据低于这个threshold，那么聚合后的结果将设置成None。\narchiveCount 数据类型：Long int, 描述archive的数量\nArchives 综述 whisper在创建数据库文件时，关于Archives做了如下检查：\n至少有个archive Archives的精度在顺序上必须严格递减，不能精度相同 Arhcives的精度上必须是整数关系，高精度必须是低精度的整数倍 Archives的retention必须严格递增，不能相同 高精度的archive必须有足够的点保证至少完成一次Consolidation 举例：\nHigher: 1s/20 Lower: 60s/1 满足前四点，但不满足最后一个条件\n具体检查代码如下：\nif not archiveList: raise InvalidConfiguration(\u0026#34;You must specify at least one archive configuration!\u0026#34;) archiveList.sort(key=lambda a: a[0]) # Sort by precision (secondsPerPoint) for i, archive in enumerate(archiveList): if i == len(archiveList) - 1: break nextArchive = archiveList[i + 1] if not archive[0] \u0026lt; nextArchive[0]: raise InvalidConfiguration(\u0026#34;A Whisper database may not be configured having \u0026#34; \u0026#34;two archives with the same precision (archive%d: %s, archive%d: %s)\u0026#34; % (i, archive, i + 1, nextArchive)) if nextArchive[0] % archive[0] != 0: raise InvalidConfiguration(\u0026#34;Higher precision archives\u0026#39; precision \u0026#34; \u0026#34;must evenly divide all lower precision archives\u0026#39; precision \u0026#34; \u0026#34;(archive%d: %s, archive%d: %s)\u0026#34; % (i, archive[0], i + 1, nextArchive[0])) retention = archive[0] * archive[1] nextRetention = nextArchive[0] * nextArchive[1] if not nextRetention \u0026gt; retention: raise InvalidConfiguration(\u0026#34;Lower precision archives must cover \u0026#34; \u0026#34;larger time intervals than higher precision archives \u0026#34; \u0026#34;(archive%d: %s seconds, archive%d: %s seconds)\u0026#34; % (i, retention, i + 1, nextRetention)) archivePoints = archive[1] pointsPerConsolidation = nextArchive[0] // archive[0] if not archivePoints \u0026gt;= pointsPerConsolidation: raise InvalidConfiguration(\u0026#34;Each archive must have at least enough points \u0026#34; \u0026#34;to consolidate to the next archive (archive%d consolidates %d of \u0026#34; \u0026#34;archive%d\u0026#39;s points but it has only %d total points)\u0026#34; % (i + 1, pointsPerConsolidation, i, archivePoints)) 结构 Archives对应着不同精度的存储实现，其有三个部分组成：offset, secondsPerPoint, points\noffset 数据类型：Long int，offset指示相应的data区域在这个文件中的offset\nsecondsPerPoint 数据类型：Long int, 表示每个点所代表的采样时长，e.g. archive的精度／precision，显然最高精度只能是1秒一次\npoints 数据类型：Long int, 表示数据点的数量\n衍生指标 这些指标本身不存在于文件中，由其他指标计算得到\n####### retention\n\u0026#39;retention\u0026#39;: secondsPerPoint * points 表示这个archive的保存时长，单位秒\n####### size\n\u0026#39;size\u0026#39;: points * pointSize 表示data部分所占据的字节长度\nData 这个部分表示具体的数据点，数据点线性排列在文件中，每个数据点有两个部分构成：Interval, data\nInterval 数据类型：Long int，表示时间戳（从UNIX纪元 (aka 1970-01-01 00:00 UTC) 开始的秒数）\ndata 数据类型：double (aka \u0026rsquo;d\u0026rsquo; in struct format)，表示具体的metric数值\n总结 ASCII art 图表 +-------------------------------------------------------------------------+ |AT|MR|xff|AC|offset|SPP|points| ... |Interval|data| ... | +-------------------------------------------------------------------------+ | | Archive One | ... | Point One | ... | +-------------------------------------------------------------------------+ | Header | Archives | Data | +-------------------------------------------------------------------------+ | Whisper file | +-------------------------------------------------------------------------+ AT: aggregationType MR: maxRetention AC: archiveCount SPP: secondsPerPoint 创建数据库 参数 关键参数：\npath 数据库文件的路径 archiveList xFilesFactor=None aggregationMethod=None archiveList # Validate archive configurations... validateArchiveList(archiveList) 检查条件参见 inline page\n写入Header aggregationType = struct.pack(longFormat, aggregationMethodToType.get(aggregationMethod, 1)) oldest = max([secondsPerPoint * points for secondsPerPoint, points in archiveList]) maxRetention = struct.pack(longFormat, oldest) xFilesFactor = struct.pack(floatFormat, float(xFilesFactor)) archiveCount = struct.pack(longFormat, len(archiveList)) packedMetadata = aggregationType + maxRetention + xFilesFactor + archiveCount fh.write(packedMetadata) 写入ArchiveList 其中比较重要的是offset的计算\nheaderSize = metadataSize + (archiveInfoSize * len(archiveList)) archiveOffsetPointer = headerSize for secondsPerPoint, points in archiveList: archiveInfo = struct.pack(archiveInfoFormat, archiveOffsetPointer, secondsPerPoint, points) fh.write(archiveInfo) archiveOffsetPointer += (points * pointSize) Data区域填充 \\x00 if CAN_FALLOCATE and useFallocate: remaining = archiveOffsetPointer - headerSize fallocate(fh, headerSize, remaining) elif sparse: fh.seek(archiveOffsetPointer - 1) fh.write(b\u0026#39;\\x00\u0026#39;) else: remaining = archiveOffsetPointer - headerSize chunksize = 16384 zeroes = b\u0026#39;\\x00\u0026#39; * chunksize while remaining \u0026gt; chunksize: fh.write(zeroes) remaining -= chunksize fh.write(zeroes[:remaining]) 这里的存在几种优化的 IO方法 fallocate 这个一个Linux独有的系统调用，函数原型是int fallocate(int fd, int mode, off_t offset, off_t len);。该函数允许调用者直接分配文件中范围在offset到len的区段的磁盘空间，速度比写入文件分配的更快。\n更多信息，请参见 fallocate手册\nsparse file 文件并没有真正分配在磁盘上，而是记录该文件有这个尺寸，等到真正写入时，才会分配磁盘空间，因此这个文件时稀疏的。优点是创建时非常快，但真正写入时存在磁盘碎片的可能，导致写入和读取比普通方式更慢。\n更多信息，请参见 稀疏文件的维基百科\nwrite by chunk 系统底层的磁盘时按照扇区工作的，如果写入数据和扇区大小一致，那么就会减少不必要的调整时间。现代磁盘的扇区大小通常为4K，因此按照4K或者4K的整数倍写入都可以获得性能提升。\n更多信息，请参见 磁盘扇区的维基百科\n实现方面的陷阱 代码的实现部分有一个陷阱：\nvalidateArchiveList(archiveList)\n在函数内对 archiveList 做了排序 archiveList.sort(key=lambda a: a[0]) , 如果不阅读内部代码，容易让人丢失重要的细节\n查询数据库 根据最大Retention,修正查询时间范围 if now is None: now = int(time.time()) if untilTime is None: untilTime = now fromTime = int(fromTime) untilTime = int(untilTime) # Here we try and be flexible and return as much data as we can. # If the range of data is from too far in the past or fully in the future, we # return nothing if fromTime \u0026gt; untilTime: raise InvalidTimeInterval(\u0026#34;Invalid time interval: from time \u0026#39;%s\u0026#39; is after until time \u0026#39;%s\u0026#39;\u0026#34; % (fromTime, untilTime)) oldestTime = now - header[\u0026#39;maxRetention\u0026#39;] # Range is in the future if fromTime \u0026gt; now: return None # Range is beyond retention if untilTime \u0026lt; oldestTime: return None # Range requested is partially beyond retention, adjust if fromTime \u0026lt; oldestTime: fromTime = oldestTime # Range is partially in the future, adjust if untilTime \u0026gt; now: untilTime = now 查找能够覆盖查询范围的最高精度的archive diff = now - fromTime for archive in header[\u0026#39;archives\u0026#39;]: if archive[\u0026#39;retention\u0026#39;] \u0026gt;= diff: break 时间范围对齐到Interval fromInterval = int(fromTime - (fromTime % archive[\u0026#39;secondsPerPoint\u0026#39;])) + archive[\u0026#39;secondsPerPoint\u0026#39;] untilInterval = int(untilTime - (untilTime % archive[\u0026#39;secondsPerPoint\u0026#39;])) + archive[\u0026#39;secondsPerPoint\u0026#39;] 概括说来，总是寻找和时间点最接近的下一个Interval\n特别的，相同时间的开始和结束的查询范围被调整成为总是包含下一个step：\nif fromInterval == untilInterval: # Zero-length time range: always include the next point untilInterval += archive[\u0026#39;secondsPerPoint\u0026#39;] 计算offset # Determine fromOffset timeDistance = fromInterval - baseInterval pointDistance = timeDistance // archive[\u0026#39;secondsPerPoint\u0026#39;] byteDistance = pointDistance * pointSize fromOffset = archive[\u0026#39;offset\u0026#39;] + (byteDistance % archive[\u0026#39;size\u0026#39;]) # Determine untilOffset timeDistance = untilInterval - baseInterval pointDistance = timeDistance // archive[\u0026#39;secondsPerPoint\u0026#39;] byteDistance = pointDistance * pointSize untilOffset = archive[\u0026#39;offset\u0026#39;] + (byteDistance % archive[\u0026#39;size\u0026#39;]) python % 运算 的一个trick byteDistance % archive[\u0026#39;size\u0026#39;] 能够达到wrap的效果，等价于\nif byteDistance \u0026gt;= 0: return byteDistance else: return archive[\u0026#39;size\u0026#39;] + byteDistance 原因是python的求余运算 % 的特性\nprint(-3 % 5) # 输出 2 print(3 % 5) # 输出 3 读取数据 # Now we unpack the series data we just read (anything faster than unpack?) byteOrder, pointTypes = pointFormat[0], pointFormat[1:] points = len(seriesString) // pointSize seriesFormat = byteOrder + (pointTypes * points) unpackedSeries = struct.unpack(seriesFormat, seriesString) # And finally we construct a list of values (optimize this!) valueList = [None] * points # Pre-allocate entire list for speed currentInterval = fromInterval step = archive[\u0026#39;secondsPerPoint\u0026#39;] for i in xrange(0, len(unpackedSeries), 2): pointTime = unpackedSeries[i] if pointTime == currentInterval: pointValue = unpackedSeries[i + 1] valueList[i // 2] = pointValue # In-place reassignment is faster than append() currentInterval += step timeInfo = (fromInterval, untilInterval, step) return (timeInfo, valueList) 其中比较重要的信息是，读取数据时会判断时间戳（Interval）是否等于期望的时间戳，如果不相同就认为没有值，设为None, 这是一种重要的行为，这样写的时候就可以离散写，不用连续写数据，结果的正确性得到保障。\n数据库更新 Whisper数据库本身虽然支持单个数据点更新API update()也支持多个数据点更新API update_many()，但是在carbon的使用中，是使用多数据点更新的API,两者实现上类似，只是后者批量更新，IO效率更高，本文将讨论 update_many()\n参数 path 代表文件路径 points is a list of (timestamp,value) points 排序 将列表中的点按照时间戳从大到小降序排列，完成后较新的数据点在前面\npoints = [(int(t), float(v)) for (t, v) in points] points.sort(key=lambda p: p[0], reverse=True) # Order points by timestamp, newest first 读取文件头 读取文件头返回数据结构\ninfo = { \u0026#39;aggregationMethod\u0026#39;: aggregationTypeToMethod.get(aggregationType, \u0026#39;average\u0026#39;), \u0026#39;maxRetention\u0026#39;: maxRetention, \u0026#39;xFilesFactor\u0026#39;: xff, \u0026#39;archives\u0026#39;: archives, } 将数据点按照Archive\u0026rsquo;s Retention分组进行写入 for point in points: age = now - point[0] while currentArchive[\u0026#39;retention\u0026#39;] \u0026lt; age: # We can\u0026#39;t fit any more points in this archive if currentPoints: # Commit all the points we\u0026#39;ve found that it can fit currentPoints.reverse() # Put points in chronological order __archive_update_many(fh, header, currentArchive, currentPoints) currentPoints = [] try: currentArchive = next(archives) except StopIteration: currentArchive = None break if not currentArchive: break # Drop remaining points that don\u0026#39;t fit in the database currentPoints.append(point) if currentArchive and currentPoints: # Don\u0026#39;t forget to commit after we\u0026#39;ve checked all the archives currentPoints.reverse() __archive_update_many(fh, header, currentArchive, currentPoints) 值得注意的是数据在传入具体函数写入时，Points都做了order reverse，变成了oldest数据在最前面\nPS：这块代码不容易理解\n分组写入操作 具体实现函数 __archive_update_many(fh, header, archive, points)\n数据点对齐 step = archive[\u0026#39;secondsPerPoint\u0026#39;] alignedPoints = [(timestamp - (timestamp % step), value) for (timestamp, value) in points] 数据点按照连续性分组 # Create a packed string for each contiguous sequence of points packedStrings = [] previousInterval = None currentString = b\u0026#34;\u0026#34; lenAlignedPoints = len(alignedPoints) for i in xrange(0, lenAlignedPoints): # Take last point in run of points with duplicate intervals if i + 1 \u0026lt; lenAlignedPoints and alignedPoints[i][0] == alignedPoints[i + 1][0]: continue (interval, value) = alignedPoints[i] # 如果是开头或者时间点是连续的 if (not previousInterval) or (interval == previousInterval + step): currentString += struct.pack(pointFormat, interval, value) previousInterval = interval else: # 如果时间点断开了 numberOfPoints = len(currentString) // pointSize startInterval = previousInterval - (step * (numberOfPoints - 1)) packedStrings.append((startInterval, currentString)) currentString = struct.pack(pointFormat, interval, value) previousInterval = interval if currentString: numberOfPoints = len(currentString) // pointSize startInterval = previousInterval - (step * (numberOfPoints - 1)) packedStrings.append((startInterval, currentString)) 其中需要注意的点：\n对齐后的时间点做了去重复操作，只保留时间上最后一个点，这是很重要的特性\n数据写入 # Read base point and determine where our writes will start fh.seek(archive[\u0026#39;offset\u0026#39;]) packedBasePoint = fh.read(pointSize) (baseInterval, baseValue) = struct.unpack(pointFormat, packedBasePoint) if baseInterval == 0: # This file\u0026#39;s first update baseInterval = packedStrings[0][0] # Use our first string as the base, so we start at the start # Write all of our packed strings in locations determined by the baseInterval for (interval, packedString) in packedStrings: timeDistance = interval - baseInterval pointDistance = timeDistance // step byteDistance = pointDistance * pointSize myOffset = archive[\u0026#39;offset\u0026#39;] + (byteDistance % archive[\u0026#39;size\u0026#39;]) fh.seek(myOffset) archiveEnd = archive[\u0026#39;offset\u0026#39;] + archive[\u0026#39;size\u0026#39;] bytesBeyond = (myOffset + len(packedString)) - archiveEnd if bytesBeyond \u0026gt; 0: fh.write(packedString[:-bytesBeyond]) assert fh.tell() == archiveEnd, \u0026#34;archiveEnd=%d fh.tell=%d bytesBeyond=%d len(packedString)=%d\u0026#34; % ( archiveEnd, fh.tell(), bytesBeyond, len(packedString)) fh.seek(archive[\u0026#39;offset\u0026#39;]) fh.write( packedString[-bytesBeyond:]) # Safe because it can\u0026#39;t exceed the archive (retention checking logic above) else: fh.write(packedString) 注意：这里的文件写入有warp的现象\n聚合到下一级Archive # Now we propagate the updates to lower-precision archives higher = archive lowerArchives = [arc for arc in header[\u0026#39;archives\u0026#39;] if arc[\u0026#39;secondsPerPoint\u0026#39;] \u0026gt; archive[\u0026#39;secondsPerPoint\u0026#39;]] for lower in lowerArchives: fit = lambda i: i - (i % lower[\u0026#39;secondsPerPoint\u0026#39;]) lowerIntervals = [fit(p[0]) for p in alignedPoints] uniqueLowerIntervals = set(lowerIntervals) propagateFurther = False for interval in uniqueLowerIntervals: if __propagate(fh, header, interval, higher, lower): propagateFurther = True if not propagateFurther: break higher = lower 单点聚合 def __propagate(fh, header, timestamp, higher, lower): aggregationMethod = header[\u0026#39;aggregationMethod\u0026#39;] xff = header[\u0026#39;xFilesFactor\u0026#39;] lowerIntervalStart = timestamp - (timestamp % lower[\u0026#39;secondsPerPoint\u0026#39;]) lowerIntervalEnd = lowerIntervalStart + lower[\u0026#39;secondsPerPoint\u0026#39;] fh.seek(higher[\u0026#39;offset\u0026#39;]) packedPoint = fh.read(pointSize) (higherBaseInterval, higherBaseValue) = struct.unpack(pointFormat, packedPoint) if higherBaseInterval == 0: higherFirstOffset = higher[\u0026#39;offset\u0026#39;] else: timeDistance = lowerIntervalStart - higherBaseInterval pointDistance = timeDistance // higher[\u0026#39;secondsPerPoint\u0026#39;] byteDistance = pointDistance * pointSize higherFirstOffset = higher[\u0026#39;offset\u0026#39;] + (byteDistance % higher[\u0026#39;size\u0026#39;]) higherPoints = lower[\u0026#39;secondsPerPoint\u0026#39;] // higher[\u0026#39;secondsPerPoint\u0026#39;] higherSize = higherPoints * pointSize relativeFirstOffset = higherFirstOffset - higher[\u0026#39;offset\u0026#39;] relativeLastOffset = (relativeFirstOffset + higherSize) % higher[\u0026#39;size\u0026#39;] higherLastOffset = relativeLastOffset + higher[\u0026#39;offset\u0026#39;] fh.seek(higherFirstOffset) if higherFirstOffset \u0026lt; higherLastOffset: # We don\u0026#39;t wrap the archive seriesString = fh.read(higherLastOffset - higherFirstOffset) else: # We do wrap the archive higherEnd = higher[\u0026#39;offset\u0026#39;] + higher[\u0026#39;size\u0026#39;] seriesString = fh.read(higherEnd - higherFirstOffset) fh.seek(higher[\u0026#39;offset\u0026#39;]) seriesString += fh.read(higherLastOffset - higher[\u0026#39;offset\u0026#39;]) # Now we unpack the series data we just read byteOrder, pointTypes = pointFormat[0], pointFormat[1:] points = len(seriesString) // pointSize seriesFormat = byteOrder + (pointTypes * points) unpackedSeries = struct.unpack(seriesFormat, seriesString) # And finally we construct a list of values neighborValues = [None] * points currentInterval = lowerIntervalStart step = higher[\u0026#39;secondsPerPoint\u0026#39;] for i in xrange(0, len(unpackedSeries), 2): pointTime = unpackedSeries[i] if pointTime == currentInterval: neighborValues[i // 2] = unpackedSeries[i + 1] currentInterval += step # Propagate aggregateValue to propagate from neighborValues if we have enough known points knownValues = [v for v in neighborValues if v is not None] if not knownValues: return False knownPercent = float(len(knownValues)) / float(len(neighborValues)) if knownPercent \u0026gt;= xff: # We have enough data to propagate a value! aggregateValue = aggregate(aggregationMethod, knownValues, neighborValues) myPackedPoint = struct.pack(pointFormat, lowerIntervalStart, aggregateValue) fh.seek(lower[\u0026#39;offset\u0026#39;]) packedPoint = fh.read(pointSize) (lowerBaseInterval, lowerBaseValue) = struct.unpack(pointFormat, packedPoint) if lowerBaseInterval == 0: # First propagated update to this lower archive fh.seek(lower[\u0026#39;offset\u0026#39;]) fh.write(myPackedPoint) else: # Not our first propagated update to this lower archive timeDistance = lowerIntervalStart - lowerBaseInterval pointDistance = timeDistance // lower[\u0026#39;secondsPerPoint\u0026#39;] byteDistance = pointDistance * pointSize lowerOffset = lower[\u0026#39;offset\u0026#39;] + (byteDistance % lower[\u0026#39;size\u0026#39;]) fh.seek(lowerOffset) fh.write(myPackedPoint) return True else: return False 如果上一个精度的aggrigation的xff过低导致聚合失败，那么后续级别的aggrigation就会取消\nTrick 在打开文件时，Whisper 使用了 Linux 上的 fadvise 来建议操作系统对文件访问进行某个策略的优化。\nif CAN_FADVISE and FADVISE_RANDOM: posix_fadvise(fh.fileno(), 0, 0, POSIX_FADV_RANDOM) fadvise 在其手册中的介绍翻译成中文大意是：\n允许应用程序告知操作系统它会如何使用文件描述符，这样操作系统就能选用最合适的读取和缓存策略来访问相应的文件\nfadvise 有多个选项：\nFADV_NORMAL ：不需要特殊对待 FADV_RANDOM : 期望页面以随机访问进行 FADV_SEQUENTIAL : 期望页面访问以顺序访问进行 FADV_WILLNEED : 期望在近期再次访问 FADV_DONTNEED : 不期望在近期再次访问 FADV_NOREUSE : 只会访问数据一次 关于fadvise的更多信息，请参见 man fadvise\n","permalink":"https://blog.xiaoquankong.ai/zh/posts/introduce-to-the-implement-of-whisper/","summary":"\u003cp\u003e\u003cstrong\u003eTL;DR\u003c/strong\u003e 本文从代码级别详细介绍了 Whisper 的实现和一些其中用到的编程技巧\u003c/p\u003e","title":"时间数据库Whisper的实现简介"},{"content":"在使用 Python 的 iterator 时，遇到一个很愚蠢的错误，浪费的很多时间才找到原因。特此记录一下，提醒自己，提示他人。 Bug 复现 原来的问题比较复杂，导致出错难以调试，经过将问题不断简化，最后简化后的代码如下：\n打印 iterator 的函数 我们定义一个用于打印 iterator 的函数\ndef print_iterator(iterator): while True: try: element = next(iterator) except StopIteration: break else: print(element) 测试一下\nprint_iterator(iter([1, 2, 3])) 输出：\n1 2 3 代码工作正常\n定义一个计算 iterable 长度的函数 def counter_iterable(iterable): iterator = iter(iterable) iterator_length = sum(1 for _ in iterator) print(iterator_length) 测试一下\ncounter_iterable([1, 2, 3]) 输出：\n3 代码工作正常\n整合在一起 将 放在 中调用：\ndef counter_iterable_and_print(iterable): iterator = iter(iterable) iterator_length = sum(1 for _ in iterator) print(iterator_length) print_iterator(iterator) 测试一下\ncounter_iterable_and_print([1, 2, 3]) 期望的输出应该有长度和打印内容两个部分构成：\n3 1 2 3 但实际上输出是：\n3 代码工作不正常\nbug 原因 这个 bug 的\u0008产生和我对两个概念的理解和记忆错误有关： 第一个是没有充分理解和记忆 iterator 的工作机制。\niterator 是什么 根据 Iterator on Python wiki :\nAn iterator is an object that implements next (in python3, it is __next__) method, which is expected to return the next element of the iterable object that returned it, and raise a StopIteration exception when no more elements are available.\nIterator will typically need to maintain some kind of position state information (like the index of the last element returned or the like). If the iterable maintained that state itself, it would become inherently non-reentrant (meaning you could use it only one loop at a time).\n上述 bug 产生的原因：上面的代码中共享了一个 iterator 对象，由于 iterator 具有记忆内部状态的能力，所以当\niterator_length = sum(1 for _ in iterator) 执行完毕后，实际这个 iterator 对象已经完成了全部元素的迭代。后续再次调用这个对象的 __next__() 方法时，直接抛出 StopIteration \u0008异常。因此这就是为什么后续的\nprint_iterator(iterator) 并没有任何输出的原因，因为函数收到的参数已经是一个走到最后的\u0008 iterator 对象了。\niterator 内有有状态信息，具有不可重入（non-reentrant）的特性，这个和 list, tuple, dict 等容器不一样，容器通过 __getitem__ 来迭代。\n另外一个是没有搞清楚 iterable 和 iterator 的区别。正确的理解是：iterable 是一个工厂函数，通过显式的调用 iter 函数或者调用其 __iter__() 方法或者使用\u0008 for 循环来获得这个工厂的产品：一个 iterator 对象。\n这里需要简单说明的是：无论是使用 iter 函数还是在 for 循环中使用，都是间接的调用 iterable 对象的 __iter__ 方法。\n\u0008iterator 被要求需要支持 iterable 协议的。通常情况下，对 iterator 调用 __iter__ 方法，返回的 iterator 就是它自己。 这样 iterator 对象就能够在 for-loop 中使用了。\n关于 iterator 的 PEP 在 PEP 234 \u0026ndash; Iterators\n","permalink":"https://blog.xiaoquankong.ai/zh/posts/a-pitfall-of-python-iterator/","summary":"\u003cp\u003e在使用 Python 的 iterator 时，遇到一个很愚蠢的错误，浪费的很多时间才找到原因。特此记录一下，提醒自己，提示他人。\n\u003ca href=\"https://mybinder.org/v2/gh/howl-anderson/howl-anderson.github.io/master?filepath=python-iterator-%E9%81%87%E5%88%B0%E7%9A%84%E4%B8%80%E4%B8%AA%E5%9D%91%2Fproof-of-concept.ipynb\"\u003e\u003cimg loading=\"lazy\" src=\"https://mybinder.org/badge.svg\" alt=\"Binder\"  /\u003e\n\u003c/a\u003e\u003c/p\u003e","title":"python iterator 遇到的一个坑"},{"content":"修改一个项目的 Unit Test 代码时，遇到一个关于 mock 的问题点，花了我很久时间才，找到解决方案。特此记录一下，提醒自己，提示他人。 我给一个开源软件贡献 PR 时，遇到一个大的问题是我总是要给我的代码写单元测试，会出现一些奇怪的问题（我以为的，实际最后往往证明是我蠢 :( ，本例就是)\n经过简化，最小可复现问题代码集如下：\n文件 dependency.py def some_funny_func(): return \u0026#39;funny\u0026#39; 文件 module.py from dependency import some_funny_func def call_func(): return some_funny_func() 文件 tester.py from unittest.mock import patch from module import call_func def test_call_func(): def mocked_funny_func(): return \u0026#34;not funny at all\u0026#34; with patch(\u0026#34;dependency.some_funny_func\u0026#34;, mocked_funny_func): return_value = call_func() assert return_value == \u0026#34;not funny at all\u0026#34; 一切看似合情合理（有些高手，可能已经发现问题了，但我当时没有看出来问题），但是就是通过不了测试。\n这里的错误是，没有深入理解 patch 的工作原理，patch 通过修改 module 属性的方式工作。这里 from module import call_func 执行的时候已经经 dependency.some_funny_func 导入了 module， 换言之：module.some_funny_func 已经指向了 dependency.some_funny_func, 此时通过 patch(\u0026quot;dependency.some_funny_func\u0026quot;, mocked_funny_func) 只是修改了 dependency.some_funny_func 至新的 mocked_funny_func. 但不能修改 module.some_funny_func, 因为这个是修改前赋值的，它现在依旧指向原来的函数。\n将上述改写成 python 代码，原理大概如下：\n# 模拟 dependency.some_funny_func dependency = {} dependency[\u0026#34;some_funny_func\u0026#34;] = \u0026#34;some_value\u0026#34; # 模拟 module.some_funny_func module = {} module[\u0026#34;some_funny_func\u0026#34;] = dependency[\u0026#34;some_funny_func\u0026#34;] # 模拟 mock dependency[\u0026#34;some_funny_func\u0026#34;] = \u0026#34;some_other_value\u0026#34; # 查看结果 print(dependency[\u0026#34;some_funny_func\u0026#34;]) print(module[\u0026#34;some_funny_func\u0026#34;]) 在线演示 在线演示地址\n","permalink":"https://blog.xiaoquankong.ai/zh/posts/a-pitfall-of-python-mock-module/","summary":"\u003cp\u003e修改一个项目的 Unit Test 代码时，遇到一个关于 \u003ccode\u003emock\u003c/code\u003e 的问题点，花了我很久时间才，找到解决方案。特此记录一下，提醒自己，提示他人。\u003ca href=\"https://mybinder.org/v2/gh/howl-anderson/howl-anderson.github.io/master?filepath=python-mock-%25E9%2581%2587%25E5%2588%25B0%25E7%259A%2584%25E4%25B8%2580%25E4%25B8%25AA%25E5%259D%2591%2Fmain.py.ipynb\"\u003e\u003cimg loading=\"lazy\" src=\"https://mybinder.org/badge.svg\" alt=\"Binder\"  /\u003e\n\u003c/a\u003e\u003c/p\u003e","title":"python mock 遇到的一个坑"},{"content":"最大匹配每次寻找和确定最佳分词的时候按照最长（最大）匹配作为依据，从字符串的右边到左边（反向）依次寻找最大匹配。\n解释 从解释的角度，可以\u0008理解为：\u0008判断待分词字符串的后 N 个字符构成的字符串是否在字典中，如果在，则匹配结束。如果没有匹配成功，则依次缩减 N 直到匹配成功。\n示例 以 中国的首都是北京 为例, 假设 N 初始化为 4：\n\u0008匹配 都是北京， 字典中无此词语，匹配失败，缩减 N 为 3 匹配 是北京， 字典中无此词语，匹配失败，缩减 N 为 2 匹配 北京， 字典中有此词语，匹配成功，结束匹配 继续匹配剩余未分词字符串 中国的首都是，直至字符串全部被分词。\n局限 因为只考虑了能否构成词汇，完全没有考虑上下文，所以部分情况下会出现问题。 例如 中国的首都是北京 则会被分成 中国 / 的 / 首 / 都是 / 北京\n优化方案 本算法\u0008耗时最大的部分在于扫描词典，因此可以通过特定的数据结构优化加速字典查找过程：\n按照长度分别构建多个字典，能够一定程度的加速字典扫描的速度 使用\u0008 Trie-Tree 能够\u0008极大的加速查找过程 \u0008关联算法 和 最大正向匹配法 思想完全一样，\u0008用于解决 最大正向匹配法 的某些问题。比如：我们在野生动物园玩 在 最大正向匹配法 中无法正确分词，但 反向最大匹配法 能够正确分词。\n参考文献 中文分词基础原则及正向最大匹配法、逆向最大匹配法、双向最大匹配法的分析 ","permalink":"https://blog.xiaoquankong.ai/zh/posts/creating-a-chinese-tokenizer-using-the-maximum-reverse-matching-method/","summary":"\u003cp\u003e最大匹配每次寻找和确定最佳分词的时候按照最长（最大）匹配作为依据，从字符串的右边到左边（反向）依次寻找最大匹配。\u003c/p\u003e","title":"构建中文分词器 - 反向最大匹配法"},{"content":"将所有可能的分词结果按照词语构建成一个有向无环图，寻找其中联合概率最大的路径。\n解释 所需的语料是一个包含词语频率的字典。查找所有可能的分词结果可以通过查找字典中的词是否在一个字符串的开始位置来完成。\n构建成有向无环图后，我们找出所有路径中联合概率（朴素观点为：各个\u0008词语概率相乘）最大的路径。但一般情况下图理论和图相关的库都是用来求解最短路径（所有路径中权重之和最小的路径），因此这里做数学上的变换，能够按照最小路径求解的方式，找到联合概率最大的路径。\u0008具体变动如下:\n使用 log 函数将求解相乘问题转换成相加问题 log(a * b) = log(a) + log(b) 使用倒数函数将求解最大问题变成求解最小问题 a \u0026gt; b then 1/a \u0026lt; 1/b 示例 以 我们在野生动物园玩 为例，假设我们的词典里只包含如下词汇\n词汇 频数 概率 概率的倒数 log(概率的倒数) 我们 30 0.30 3.3 1.19 在 40 0.40 2.5 3.69 在野 2 0.02 50 3.91 野生动物园 8 0.08 12.5 2.53 物 1 0.01 100 4.61 园 1 0.01 100 4.61 玩 18 0.18 5.6 1.72 NOTE : log 函数在这里是以自然对数 e 为低的\u0008，等同于函数 ln\n从 我们在野生动物园玩 开始，扫描词汇表，找到匹配的前缀词汇。\n第一轮找到词汇 我们，剩余未分词\u0008\u0008字符串为 在野生动物园玩 第二轮找到词汇 在 和 在野，剩余未分词字符串为 野生动物园玩 和 生动物园玩 第三轮则对上面两个未分词字符串，应用相同的规则分词，得到词汇 野生动物园 和 生动 如此重复直到所有的未分词字符串为空。 经过上述步骤，我们得到两种分词可能\n我们 / 在 / 野生动物园 / 玩 我们 / 在野 / 生动 / 物 / 园 / 玩 添加开始节点 \u0026lt;start\u0026gt; 和 结束节点 \u0026lt;end\u0026gt; 后，我们可以构建一个 有向无环图, 节点之间的边的权重为上一个节点对应的 log(概率的倒数)。\n则得到类似如下的有向无环图：\n通过求解最小路径的方法可以得到最短路径为 我们 / 在 / 野生动物园 / 玩\n局限 因为基于词典，因此不具备新词发现的能力，同时也很难处理歧义问题。\n","permalink":"https://blog.xiaoquankong.ai/zh/posts/implementing-a-dag-based-chinese-tokenizer/","summary":"\u003cp\u003e将所有可能的分词结果按照词语构建成一个有向无环图，寻找其中联合概率最大的路径。\u003c/p\u003e","title":"构建中文分词器 - 有向无环图法"},{"content":"最大匹配每次寻找和确定最佳分词的时候按照最长（最大）匹配作为依据，从字符串的左边到右边（正向）依次寻找最大匹配。\n解释 从解释的角度，可以\u0008理解为：\u0008判断待分词字符串的前 N 个字符构成的字符串是否在字典中，如果在，则匹配结束。如果没有匹配成功，则依次缩减 N 直到匹配成功。\n示例 以 我们在野生动物园玩 为例, 假设 N 初始化为 4：\n\u0008匹配 我们在野， 字典中无此词语，匹配失败，缩减 N 为 3 匹配 我们在， 字典中无此词语，匹配失败，缩减 N 为 2 匹配 我们， 字典中有此词语，匹配成功，结束匹配 继续匹配剩余未分词字符串 在野生动物园玩，直至字符串全部被分词。\n局限 因为只考虑了能否构成词汇，完全没有考虑上下文，所以部分情况下会出现问题。 例如 我们在野生动物园玩 则会被分成 我们 / 在野 / 生动 / 物 / 园 / 玩\n优化方案 本算法\u0008耗时最大的部分在于扫描词典，因此可以通过特定的数据结构优化加速字典查找过程：\n按照长度分别构建多个字典，能够一定程度的加速字典扫描的速度 使用\u0008 Trie-Tree 能够\u0008极大的加速查找过程 \u0008关联算法 反向最大匹配法 和 最大正向匹配法 思想完全一样，\u0008用于解决 最大正向匹配法 的某些问题。比如：我们在野生动物园玩 在 最大正向匹配法 中无法正确分词，但 反向最大匹配法 能够正确分词。\n参考文献 中文分词基础原则及正向最大匹配法、逆向最大匹配法、双向最大匹配法的分析 ","permalink":"https://blog.xiaoquankong.ai/zh/posts/creating-a-chinese-tokenizer-using-the-maximum-forward-matching-method/","summary":"\u003cp\u003e最大匹配每次寻找和确定最佳分词的时候按照最长（最大）匹配作为依据，从字符串的左边到右边（正向）依次寻找最大匹配。\u003c/p\u003e","title":"构建中文分词器 - 正向最大匹配法"},{"content":" TL;DR 从零开始实现 Q-learning 算法，在 OpenAI Gym 的环境中演示：如何一步步实现增强学习。\n前面的博文里已经介绍过 Q-learning 的一些基本情况了，如果你没见过前面的博文或者已经忘记的差不多了，那么可以使用这个 Reinforcement Learning: 初次交手，多多指教 访问。\n但是总的来说，如果没有实际代码跑一番，估计你对这个算法的正确性还是有疑虑的。本文将从头构建一个 Q-learning 算法，来解决一个 toy 级别的强化学习场景的学习工作。希望能加深你对 Q-learning 的理解和对强化学习的认知。\n源代码 比较精美的，但是做了一定扩展的实现在 q_learning_demo 和本文代码相对应的，稍有改动的 Jupyter Notebook 在 proof-of-concept 场景 我们要用 Q-learning 解决什么问题呢？我们使用 OpenAI Gym 里提供的一个环境：FrozenLake-v0.\nFrozenLake-v0 环境的中文描述大概是这样的：\n冬天的时候，你和你的朋友们在公园扔飞盘。 你不小心把飞盘扔到了公园的湖中间。 湖面已经结冰，但是有些地方的没有结冰，形成一个冰洞，有人踩上去会掉下去。 这个飞盘对你来说非常宝贵，你觉得非常有必要把飞盘拿回来。 但是冰面很滑，你不能总是想去什么方向就去什么方向，滑滑的冰面可能会带你走向别的方向。 冰面用如下的字符块表示： SFFF FHFH FFFH HFFG S : Safe，开始点，安全 F : frozen surface, 冻结的表面，安全 H : hole, 掉下去就死定了 G : goal, 飞盘所在的地方 每个轮回，以你拿回飞盘或者掉进洞里而结束。 只有当你拿到飞盘才能获得1个奖励，其他情况都为0 OpenAI Gym OpenAI 是 Elon Musk 创建的一家致力于非盈利的通用人工智能的公司。 其开源产品 Gym 是提供了一种增强学习的实现框架，主要用于提供一些模拟器供研究使用。\n之前的博客提到过，增强学习是 Agent 和 Environment 直接的交互构成的。Gym 提供了很多常见的 Environment 对象。利用这些 Environment，研究者可以很快构建增强学习的应用。\nGym 运行模式 # 导入gym import gym # 构建环境 env = gym.make(\u0026#34;Taxi-v1\u0026#34;) # 获取第一次的观察结果 observation = env.reset() # 开始探索环境 for _ in range(1000): env.render() # 渲染观察结果 # 你的 Agent 应该会根据观察结果，选择最合适的动作，但这里我们使用随机选择的动作 action = env.action_space.sample() # your agent here (this takes random actions) # 将动作发送给环境，获取新的观察结果、奖励和是否结束的标志等 observation, reward, done, info = env.step(action) if done: # 游戏结束 break 通过上面的示例，你应该了解OpenAI gym的工作模式。\n训练流程 导入依赖\nimport gym import numpy as np from collections import defaultdict import functools 定义两个主要组件\n# 构建 Environment env = gym.make(\u0026#39;FrozenLake-v0\u0026#39;) env.seed(0) # 确保结果具有可重现性 # 构建 Agent tabular_q_agent = TabularQAgent(env.observation_space, env.action_space) # 开始训练 train(tabular_q_agent, env) tabular_q_agent.test(env) 训练循环\ndef train(tabular_q_agent, env): for episode in range(100000): # 训练 100000 次 all_reward, step_count = tabular_q_agent.learn(env) TabularQAgent 的实现\nclass TabularQAgent(object): def __init__(self, observation_space, action_space): self.observation_space = observation_space self.action_space = action_space self.action_n = action_space.n self.config = { \u0026#34;learning_rate\u0026#34;: 0.5, \u0026#34;eps\u0026#34;: 0.05, # Epsilon in epsilon greedy policies \u0026#34;discount\u0026#34;: 0.99, \u0026#34;n_iter\u0026#34;: 10000} # Number of iterations self.q = defaultdict(functools.partial(generate_zeros, n=self.action_n)) def act(self, observation, eps=None): if eps is None: eps = self.config[\u0026#34;eps\u0026#34;] # epsilon greedy. action = np.argmax(self.q[observation]) if np.random.random() \u0026gt; eps else self.action_space.sample() return action def learn(self, env): obs = env.reset() rAll = 0 step_count = 0 for t in range(self.config[\u0026#34;n_iter\u0026#34;]): action = self.act(obs) obs2, reward, done, _ = env.step(action) future = 0.0 if not done: future = np.max(self.q[obs2]) self.q[obs][action] = (1 - self.config[\u0026#34;learning_rate\u0026#34;]) * self.q[obs][action] + self.config[\u0026#34;learning_rate\u0026#34;] * (reward + self.config[\u0026#34;discount\u0026#34;] * future) obs = obs2 rAll += reward step_count += 1 if done: break return rAll, step_count def test(self, env): obs = env.reset() env.render(mode=\u0026#39;human\u0026#39;) for t in range(self.config[\u0026#34;n_iter\u0026#34;]): env.render(mode=\u0026#39;human\u0026#39;) action = self.act(obs, eps=0) obs2, reward, done, _ = env.step(action) env.render(mode=\u0026#39;human\u0026#39;) if done: break obs = obs2 核心代码 我们重点关注核心代码，Q-learning 是如何学习的，相关代码简化后得到：\n如何更新 Q table # 获取第一次观察结果 obs = env.reset() while True: # 一直循环，直到游戏结束 action = self.act(obs) # 根据策略，选择 action obs2, reward, done, _ = env.step(action) future = 0.0 if not done: future = np.max(self.q[obs2]) # 获取后一步期望的最大奖励 # 更新 Q 表格，保留部分当前值 加上 部分当前奖励和未来一步的最大奖励 self.q[obs][action] = (1 - self.config[\u0026#34;learning_rate\u0026#34;]) * self.q[obs][action] + self.config[\u0026#34;learning_rate\u0026#34;] * (reward + self.config[\u0026#34;discount\u0026#34;] * future) # 更新 obs = obs2 # 游戏结束，退出循环 if done: break explore / exploit 问题 上面的代码我只提到了 self.act 会根据策略选择 action，那么该如何选择呢？这里就涉及到了 explore exploit tradeoff 的问题了。我们理想中的 action 选择策略是既能充分利用现有学习到的知识，每次都去最大化的最终的reward，这就是 exploit。但是同时，我们也希望我们的选择策略能适当的去探索一下其他路径，不能固定在已经知道的最优选择，避免局部最优解，适当时候也去探索其他路径，可能能发现更加优秀的路径，也就是全局最优解，这就是 explore 问题。\n我们采取了一个概率方案，有一定概率去通过随机选择的方式，探索新路径。\n# eps 数值在 [0, 1] ，控制探索的力度，越大探索的越多 if eps is None: eps = self.config[\u0026#34;eps\u0026#34;] # epsilon greedy. action = np.argmax(self.q[observation]) if np.random.random() \u0026gt; eps else self.action_space.sample() return action 其他没有交代的点 由于本篇是科普性质，所以没有cover很多其他的问题点，比如学习和探索的因子可以是decay的，刚开始训练的时候学习和探索强度比较大，后续慢慢缩小，这样模型就会慢慢收敛。\n","permalink":"https://blog.xiaoquankong.ai/zh/posts/demo-of-q-learning-in-openai-gym/","summary":"\u003cp\u003e\u003ca href=\"https://mybinder.org/v2/gh/howl-anderson/q_learning_demo/master?filepath=jupyter_notebooks%2Fproof-of-concept.ipynb\"\u003e\u003cimg loading=\"lazy\" src=\"https://mybinder.org/badge.svg\" alt=\"Binder\"  /\u003e\n\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eTL;DR\u003c/strong\u003e 从零开始实现 Q-learning 算法，在 OpenAI Gym 的环境中演示：如何一步步实现增强学习。\u003c/p\u003e","title":"基于 OpenAI Gym 的 Q-Learning 算法演示"}]